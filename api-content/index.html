{"posts":[{"title":"容器基础-namespace的实现原理","content":"[Toc] namespace的主要目的是为了实现资源的隔离和管理，以避免不同进程之间的资源竞争和冲突。同时，它也可以提供更高的安全性，因为每个进程只能访问其namespace中的资源，而无法访问其他进程的资源。因此，namespace机制对于需要隔离和管理资源的系统来说是非常有用的。 这种隔离机制使得不同进程可以运行在各自独立的系统中，不会互相干扰。例如，一个进程的改变不会影响到其他进程，其他进程也无法访问这个进程所拥有的资源。这种特性可以用来实现轻量级虚拟化（容器）服务，让容器中的进程产生错觉，认为它们置身于一个独立的系统中。 目前支持以下6种Namespace： IPC：用于进程间通信，包括SystemV IPC和Posix消息队列等。 Network：用于隔离Linux系统的设备、IP地址、端口、路由表、防火墙规则等网络资源。 PID：用于隔离进程。 UTS：用于隔离主机名和域名。 Mount：用于隔离文件系统挂载点。 User：用于隔离用户和权限。 pid_namespace 实现pid 隔离原理 struct task_struct { ... /* namespaces */ struct nsproxy *nsproxy; ... struct pid *thread_pid; struct hlist_node pid_links[PIDTYPE_MAX]; struct pid_link pids[PIDTYPE_MAX]; ... } enum pid_type { PIDTYPE_PID, //进程ID PIDTYPE_PGID, //进程组ID PIDTYPE_SID, //进程会话（session）ID PIDTYPE_MAX //最大值 }; struct nsproxy { atomic_t count; struct uts_namespace *uts_ns; struct ipc_namespace *ipc_ns; struct mnt_namespace *mnt_ns; struct pid_namespace *pid_ns_for_children; struct net *net_ns; struct time_namespace *time_ns; struct time_namespace *time_ns_for_children; struct cgroup_namespace *cgroup_ns; }; struct pid { refcount_t count; unsigned int level; spinlock_t lock; /* lists of tasks that use this pid */ struct hlist_head tasks[PIDTYPE_MAX]; struct hlist_head inodes; /* wait queue for pidfd notifications */ wait_queue_head_t wait_pidfd; struct rcu_head rcu; struct upid numbers[]; }; //user process id struct upid { int nr; struct pid_namespace *ns; }; 通过task_struct结构我们可以看到，隔离的实现被代理到nsproxy pid_links字段是用于表示进程之间的链接关系的哈希表。 进程组和会话是Linux内核中的重要概念。进程组是一组具有相同进程的集合，而会话则是一组进程组的集合。每个进程都被分配一个全局进程号（PID），用于唯一标识该进程。 pid_links字段存储了进程号（PID）与进程组标识符（PGID）和会话标识符（SID）之间的链接关系。通过该字段，可以确定哪些进程属于同一个进程组或同一会话。 当创建一个新的进程时，内核会根据PID和PID namespace来确定该进程的进程组和会话信息，并将这些信息存储在pid_links字段中。这样，当需要查找或操作某个进程时，可以通过pid_links字段快速定位到该进程的进程组和会话信息，以便进行相应的操作。 struct pid_namespace { struct idr idr; //存储pid struct rcu_head rcu; unsigned int pid_allocated; struct task_struct *child_reaper; struct kmem_cache *pid_cachep; unsigned int level; struct pid_namespace *parent; struct user_namespace *user_ns; struct ucounts *ucounts; int reboot; /* group exit code if this pidns was rebooted */ struct ns_common ns; } 从pid_namespace可以看出，pid_namespace是按照树的结构进行组织的，在下面的pid分配中可以看到(参考pid 分配)，分配nr是会同样在低层级的namespace 中 分配nr,这样，低层级的namespace 就可以访问到高层级的nr(参考下面的pid 查找过程) pid 分配 可以看到主要是对pid层级结构的初始化，从每一层namespace的idr结构中申请nr ，然后初始化到每一个层级的upid, struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, size_t set_tid_size) { struct pid *pid; enum pid_type type; int i, nr; struct pid_namespace *tmp; struct upid *upid; int retval = -ENOMEM; if (set_tid_size &gt; ns-&gt;level + 1) return ERR_PTR(-EINVAL); pid = kmem_cache_alloc(ns-&gt;pid_cachep, GFP_KERNEL); if (!pid) return ERR_PTR(retval); tmp = ns; pid-&gt;level = ns-&gt;level; for (i = ns-&gt;level; i &gt;= 0; i--) { int tid = 0; if (set_tid_size) { tid = set_tid[ns-&gt;level - i]; retval = -EINVAL; if (tid &lt; 1 || tid &gt;= pid_max) goto out_free; if (tid != 1 &amp;&amp; !tmp-&gt;child_reaper) goto out_free; retval = -EPERM; if (!checkpoint_restore_ns_capable(tmp-&gt;user_ns)) goto out_free; set_tid_size--; } idr_preload(GFP_KERNEL); spin_lock_irq(&amp;pidmap_lock); // 在每一层的-dir中分配nr(也就是进程号) if (tid) { nr = idr_alloc(&amp;tmp-&gt;idr, NULL, tid, tid + 1, GFP_ATOMIC); if (nr == -ENOSPC) nr = -EEXIST; } else { int pid_min = 1; if (idr_get_cursor(&amp;tmp-&gt;idr) &gt; RESERVED_PIDS) pid_min = RESERVED_PIDS; nr = idr_alloc_cyclic(&amp;tmp-&gt;idr, NULL, pid_min, pid_max, GFP_ATOMIC); } spin_unlock_irq(&amp;pidmap_lock); idr_preload_end(); if (nr &lt; 0) { retval = (nr == -ENOSPC) ? -EAGAIN : nr; goto out_free; } //初始化每一层的数据 pid-&gt;numbers[i].nr = nr; pid-&gt;numbers[i].ns = tmp; tmp = tmp-&gt;parent; } retval = -ENOMEM; get_pid_ns(ns); refcount_set(&amp;pid-&gt;count, 1); spin_lock_init(&amp;pid-&gt;lock); for (type = 0; type &lt; PIDTYPE_MAX; ++type) INIT_HLIST_HEAD(&amp;pid-&gt;tasks[type]); init_waitqueue_head(&amp;pid-&gt;wait_pidfd); INIT_HLIST_HEAD(&amp;pid-&gt;inodes); upid = pid-&gt;numbers + ns-&gt;level; spin_lock_irq(&amp;pidmap_lock); if (!(ns-&gt;pid_allocated &amp; PIDNS_ADDING)) goto out_unlock; for ( ; upid &gt;= pid-&gt;numbers; --upid) { /* Make the PID visible to find_pid_ns. */ idr_replace(&amp;upid-&gt;ns-&gt;idr, pid, upid-&gt;nr); upid-&gt;ns-&gt;pid_allocated++; } spin_unlock_irq(&amp;pidmap_lock); return pid; out_unlock: spin_unlock_irq(&amp;pidmap_lock); put_pid_ns(ns); out_free: spin_lock_irq(&amp;pidmap_lock); while (++i &lt;= ns-&gt;level) { upid = pid-&gt;numbers + i; idr_remove(&amp;upid-&gt;ns-&gt;idr, upid-&gt;nr); } /* On failure to allocate the first pid, reset the state */ if (ns-&gt;pid_allocated == PIDNS_ADDING) idr_set_cursor(&amp;ns-&gt;idr, 0); spin_unlock_irq(&amp;pidmap_lock); kmem_cache_free(ns-&gt;pid_cachep, pid); return ERR_PTR(retval); } pid隔离 进程查找： struct pid *find_get_pid(pid_t nr) { struct pid *pid; rcu_read_lock(); pid = get_pid(find_vpid(nr)); rcu_read_unlock(); return pid; } static inline struct pid *get_pid(struct pid *pid) { if (pid) refcount_inc(&amp;pid-&gt;count); return pid; } struct pid *find_vpid(int nr) { return find_pid_ns(nr, task_active_pid_ns(current)); } struct pid_namespace *task_active_pid_ns(struct task_struct *tsk) { return ns_of_pid(task_pid(tsk)); } static inline struct pid *task_pid(struct task_struct *task) { return task-&gt;thread_pid; } static inline struct pid_namespace *ns_of_pid(struct pid *pid) { struct pid_namespace *ns = NULL; if (pid) ns = pid-&gt;numbers[pid-&gt;level].ns; return ns; } struct pid *find_pid_ns(int nr, struct pid_namespace *ns) { return idr_find(&amp;ns-&gt;idr, nr); } 我们看到查找是根据current上下文，获取到current-&gt; pid-&gt;numbers[pid-&gt;level].ns,在这个namespace的idr中进行查找，实现了pid的隔离。 ","link":"https://nomagic.cc/post/rong-qi-ji-chu-namespace-de-shi-xian-yuan-li/"},{"title":"Istio 基本原理与使用实践","content":" 备注：实例代码参考：https://github.com/inclee/istio-demo.git 什么是服务网格，它的技术背景是什么 随着业务发展，服务量持续增长，系统需要转向微服务架构以应对快速迭代和大规模并发的需求。在微服务系统中，服务与服务之间会形成复杂的调用链路，微服务间的网络请求协议非常复杂，这里面包括了各种诸如服务发现，安全加密、流量控制、故障处理，以及请求路由，调用链追踪。。太多太多的服务治理需求，随着服务规模的增大，这些问题的复杂性将显著增加。 服务治理的三种形态 在应用程序中包含治理逻辑 在微服务化的过程中，将服务拆分后会发现一堆麻烦事儿，连基本的业务连通都成了问题。 在处理一些治理逻辑，比如怎么找到对端的服务实例，怎么选择一个对端实例发出请求等时，都需要自己写代码来实现。这种方式简单，对外部依赖少，但会导致存在大量的重复代码。所以，微服务越多，重复的代码越多，维护越难;而且，业务代码和治理逻辑耦合，不管是对治理逻辑的全局升级，还是对业务的升级，都要改同一段代码，这种方式导致了治理逻辑和业务逻辑的严重耦合，代码复杂度提高，并且维护起来成本太高，甚至不可维护。 独立的SDK包 相比如上面提到的方案，一种常见的解决方法是将公共的治理逻辑抽象化，并将其包含在一个通用库中，也就是我们熟知的软件开发包。这样，所有的微服务都可以使用这个通用库。当这些治理能力嵌入开发框架中后，使用该开发框架进行编程的所有代码便具有这种能力。这就是SDK（Software Development Kit）模式。 虽然SDK模式在编码层面将业务逻辑与治理逻辑解耦，但业务代码与SDK依然需要一同编译，并运行在同一进程中。这导致了以下几个问题：首先，业务代码必须与SDK使用相同的编程语言，也就是说，编程语言被限制了，在更新治理逻辑时，需要全面升级自身的服务，即使业务逻辑并未发生变化，这无疑给用户带来了不便。虽然将治理逻辑独立出来了，但是对业务代码本身还是有一定的侵入性。 独立的治理进程 把治理逻辑彻底从用户的业务代码中剥离出来，作为一个独立的进程存在，作为网络的代理存在，在这种形态下，用户的业务代码和治理逻辑都以独立的进程存在，两者的代码和运行都无耦合，这样可以做到与开发语言无关，升级也相互独立。在对已存在的系统进行微服务治理时，只需搭配 Sidecar 即可，对原服务无须做任何修改，并且可以对老系统渐进式升级改造，先对部分服务进行微服务化。 “软件的复杂性没有不是可以通过分层和抽象解决不了的，如果一层不够那么多分一层”。 同样独立进程治理的方式也符合这种思维，可以类比把应用所需要的网络功能，分层抽象到了网络层。同样独立进程的方式把服务治理的逻辑也抽象到了更下一层，应用层只需无感知的依赖就好了。当然抽象和分层的代价还是有的，就是会增加效率的损耗，大多数情况下通过增加一点损耗，来解决软件的复杂性是值得的。 而这第三种方式就是我们需要介绍的服务网格，为什么叫这个名字，我想没有比这个图更为直观的。 服务网格所需要满足的特性 现在，让我们了解服务网格可以为我们提供的一些功能。请注意，实际功能列表取决于服务网格的实现。但是，总的来说，我们应该在所有实现中都期望其中大多数功能。 我们可以将这些功能大致分为三类：流量管理，安全性和可观察性。 流量管理 服务网格的基本特征之一是流量管理。这包括动态服务发现和路由。尤其影子流量和流量拆分功能，这些对于实现金丝雀发布和A/B测试非常有用。 由于所有服务之间的通信都是由服务网格处理的，因此它还启用了一些可靠性功能。例如，服务网格可以提供重试，超时，速率限制和断路器。这些现成的故障恢复功能使通信更加可靠。 安全性 服务网格通常还处理服务到服务通信的安全性方面。这包括通过双向TLS（mTLS）强制进行流量加密，通过证书验证提供身份验证以及通过访问策略确保授权。 服务网格中还可能存在一些有趣的安全用例。例如，我们可以实现网络分段，从而允许某些服务进行通信而禁止其他服务。而且，服务网格可以为审核需求提供精确的历史信息。 可观察性 强大的可观察性是处理分布式系统复杂性的基本要求。由于服务网格可以处理所有通信，因此正确放置了它可以提供可观察性的功能。例如，它可以提供有关分布式追踪的信息。 服务网格可以生成许多指标，例如延迟，流量，错误和饱和度。此外，服务网格还可以生成访问日志，为每个请求提供完整记录。这些对于理解单个服务以及整个系统的行为非常有用。 —————————————————————————————————————————— Istio Istio 简介 Istio是最初由IBM，Google和Lyft开发的服务网格的开源实现。它可以透明地分层到分布式应用程序上，并提供服务网格的所有优点，例如流量管理，安全性和可观察性。 它旨在与各种部署配合使用，例如本地部署，云托管，Kubernetes容器以及虚拟机上运行的服务程序。尽管Istio与平台无关，但它经常与Kubernetes平台上部署的微服务一起使用。 从根本上讲，Istio的工作原理是以Sidcar的形式将Envoy的扩展版本作为代理布署到每个微服务中，该代理网络构成了Istio架构的数据平面。这些代理的配置和管理是从控制平面完成的。 控制平面基本上是服务网格的大脑。它为数据平面中的Envoy代理提供发现，配置和证书管理。 当然，只有在拥有大量相互通信的微服务时，我们才能体现Istio的优势。在这里，sidecar代理在专用的基础架构层中形成一个复杂的服务网格： 使用Istio之前 使用Istio之后 Istio在与外部库和平台集成方面非常灵活。例如，我们可以将Istio与外部日志记录平台，遥测或策略系统集成。 Istio的核心组件 Istio 架构图 Istio由一些核心组件构成，这些组件共同提供了广泛的服务网格功能： 1.Envoy(Proxy): 环绕服务网格中的每个服务，Istio使用了一个被称为“边车”的代理模式，这个代理就是由Envoy实现的。Envoy负责与服务网格中所有服务的所有通信的拦截和路由。 2.Pilot： Pilot负责管理和配置所有Envoy代理实例。它提供路由规则，以便在服务间进行流量分发。 3.Mixer: Mixer提供了一种模块化的方式来实现访问控制和使用策略。它还负责收集遥测数据。 4.Citadel: Citadel负责提供服务间通信的安全认证和授权。 5.Galley: Galley负责验证、摄取、解析和派发服务网格的配置。 这些组件协同工作，构成了Istio服务网格，提供了负载均衡，服务到服务的认证，监控等等一系列功能。此外，他们还可以通过 Kubernetes 等容器编排平台进行扩展和管理。 Istio的工作原理 Istio的工作机制和架构，分为控制面和数据面两部分。可以看到，控制面主要包括 Pilot、Mixer、Citadel等服务组件;数据面由伴随每个应用程序部署的代理程序Envoy组成，执行针对应用 程序的治理逻辑。 自动注入: 指在创建应用程序时自动注入 Sidecar代理。在 Kubernetes场景下创建 Pod时，Kube-apiserver调用管理面组件的 Sidecar-Injector服务，自动修改应用程序的描述信息并注入Sidecar。在真正创建Pod时，在创建业务容器的同时在Pod中创建Sidecar容器。 在Istio中，Envoy代理的注入到每个Pod中去主要依赖于Kubernetes的Webhook自动注入机制。这个过程主要是通过Istio的Sidecar-Injector组件完成的。下面是具体的步骤： 当你创建一个名为&quot;foo&quot;的命名空间，并为它打上istio-injection=enabled标签时，其实你是在告诉Kubernetes，你希望在这个命名空间下创建的每个Pod都会触发Istio的自动注入机制。 例如，你可以通过以下命令来实现这一步： kubectl create namespace foo kubectl label namespace/foo istio-injection=enabled 当我们在foo命名空间下创建一个新的Deployment（比如运行一个简单的httpbin服务）时，Kubernetes的API Server会将这个Deployment的PodSpec发送给注册的Injector Webhook（在这个例子中就是Istio的Sidecar-Injector）。 在收到PodSpec后，Istio Sidecar-Injector会查看是否有标签istio-injection=enabled。如果有，Sidecar-Injector就会在PodSpec中添加一个新的容器（就是Envoy代理）和一些必要的环境配置，然后再将修改过的PodSpec返回给API Server。 API Server收到修改后的PodSpec，接下来就会按照这个新的PodSpec创建Pod, 这样每个Pod就会自动运行一个Envoy代理。 这样，Istio就可以自动地将Envoy Sidecar代理注入到每一个Pod中，而无需我们在每个服务或者每个Pod的配置中手动添加 流量拦截: 当我们在Kubernetes集群中部署Istio并对某个namespace启用自动注入后，Sidecar Injector将会在每个Pod的初始化过程中注入一个Init Container。 这个Init Container会运行一个脚本，这个脚本会设置iptables规则来将所有的入站和出站流量都转发到Sidecar（即Envoy Proxy）中。这样就可以确保所有的网络流量都会经过Envoy处理，这包括服务间通信和服务与外部世界的通信。 服务发现: Istio服务发现的功能主要由其组件Pilot实现。Pilot负责收集和连接到服务网格各服务的信息，并生成适合的Envoy配置信息。下面是Pilot服务发现的基本工作原理： Pilot首先从Kubernetes API服务器收集集群的服务信息。这些信息包括服务名、运行的Pod、端口等等。注意Pilot不仅仅支持Kubernetes，它还可以与其他类型的服务注册方式集成，比如Consul或者Eureka。在Kubernetes这里是利用的Kubernetes本地配置，或者Informer系统。 收集到这些服务信息后，Pilot便可以理解服务网格中有哪些服务、这些服务运行在哪里以及如何访问它们。 当新的服务添加进来或者现有服务有所改变时，Pilot将更新这些信息并生成新的Envoy配置。 Pilot将这些更新的配置通过xDS API推送给网格中的每个Envoy实例。这样，每个Envoy都能理解哪个后端服务的请求应该发往哪台主机。 因此，当服务的消费者尝试访问服务时，其相关的Envoy代理便已经知道如何路由请求到正确的目的地服务。 这样，Istio通过Pilot实现了服务发现以及为Envoy提供动态配置的功能，使服务网格内的微服务可以彼此发现并与每个其它进行通信 负载均衡: Istio的负载均衡是通过Envoy代理来实现的，从上面所说的服务发现可知： 当服务A需要调用服务B时，它会首先发送请求到它的sidecar Envoy。这个Envoy知道所有服务B的实例以及它们的负载情况，Envoy使用这些信息，根据预配置的负载均衡策略（比如轮询、随机、最少请求等），选择一个服务B的实例，然后将请求转发到那里。 Istio通过使用Pilot提供的服务发现信息和动态路由信息，结合Envoy自己的智能负载均衡能力，从而实现了强大的流量管理功能。这些功能包括请求路由、故障注入、重试、断路器等等，这都有助于增加服务网格中服务间通信的健壮性和灵活性。 流量治理: 所有请求流量首先会被服务实例的sidecar Envoy代理拦截。 根据Istio配置的规则，Envoy将决定如何处理这些请求。比如 负载均衡 会话保持 故障注入 超时 重试 HTTP重定向 HTTP重写 熔断 限流 服务隔离 灰度发布 等治理规则 访问安全: Istio 通过一下几个方式保证访问的安全性： 1、双向TLS: 在Istio服务网格中，所有服务间的通信都会被自动升级为双向TLS连接，这意味着通信不仅被加密，同时服务间的身份也被验证。Istio使用Kubernetes的Service Account作为服务的身份，当服务的Envoy代理建立连接时，它会自动将Service Account的标志作为认证证书的一部分，从而实现服务的身份验证。 2、认证鉴权: Istio使用的是RBAC（Role-Based Access Control，基于角色的访问控制）模型进行认证鉴权。Istio的RBAC系统基于声明式政策，其中服务将被组织为一个或多个服务角色集。然后，可通过与这些服务角色相关的策略，允许或者拒绝这些服务进行特定的操作。 3、终端用户验证：Istio还支持端到端用户验证。这通常通过使用JSON Web Token (JWT)在服务之间身份传递和验证来完成。 4、审计日志: Istio还可以提供一套丰富的审计日志系统，记录谁访问了哪些服务、何时访问以及访问内容等详细信息，这对于安全审计非常有帮助。 服务遥测: Istio的服务遥测原理主要是通过Envoy代理收集数据，然后再通过Istio的Mixer组件进行处理和分析，最后通过各类工具进行跟踪和可视化。 1、数据收集: Istio服务网格中的每一个服务实例都有一个Envoy代理（Sidecar模式），这个Envoy代理负责拦截和转发所有进出服务实例的网络入口和出口。在这个过程中，Envoy会自动收集关于请求和响应的详细数据，例如请求的大小、持续时间、响应的状态码等等。 2、处理和导出数据: 这些收集到的数据会被打包成统一格式的报告，然后由Envoy发送给Istio的遥测组件（Mixer）进行处理。Mixer可以对这些报告进行聚合和分析，还可以以插件的方式集成不同的后端系统，用于存储和可视化这些数据。 3、跟踪和可视化: Istio支持分布式追踪，可以集成OpenTracing API兼容的追踪系统，如Jaeger、Zipkin等。对于度量指标，Istio还可以集成Prometheus和其他指标系统，从而允许开发者和运维人员通过Grafana等可视化工具对服务性能进行详细的跟踪和监控。 策略执行: Istio的策略执行主要依赖于Mixer组件来完成。以下是其实现原理的步骤： 1、收集信息 : 当服务间发生通信时，Envoy sidecar代理会拦截这些请求，从中收集如服务公理、源和目标服务、请求路径等信息。 2、检查策略 ：Envoy代理将这些信息发送给Mixer组件。Mixer组件会基于这些信息，检查对应的策略（Policies）。这些策略可以包含各种规则，如速率限制、访问控制、配额管理等。 3、策略决策：Mixer在检查策略后，根据检查的结果做出决策。如果请求符合策略，它会给Envoy一个成功的返回值，允许请求通过。如果请求不符合策略，它会返回错误，告诉Envoy拒绝请求。 4、执行决策：在收到Mixer的决策后，Envoy代理执行对应的操作，如放行或拒绝请求。 此外，Mixer还可以使用各种适配器（Adapters）与外部服务进行集成，这些外部服务可以提供权限认证、监控、日志等功能。通过灵活配置适配器和策略，Istio的Mixer组件能够支持为服务网格提供丰富、灵活的策略执行能力。 外部访问: Istio支持微服务架构的外部访问主要通过Ingress Gateway和Egress Gateway两种方式实现。 Ingress Gateway: Ingress Gateway处理进入服务网格的流量。Istio通过配置Ingress Gateway来控制和路由进入集群的流量。这是通过Istio的路由规则来实现的，可以基于URI、headers等请求信息进行复杂的路由和重定向。这个功能为我们提供了一个集中化的流量入口点，同时能帮助我们更好地处理如TLS终止、跨域策略等复杂的入口流量需求。 Egress Gateway: Egress Gateway处理出站流量，即服务网格内部的服务向外部网络发送的请求。Istio允许通过定义ServiceEntry资源来开启对指定外部服务的访问， 我们还可以配置Egress Gateway来作为所有出站流量的集中出口点，并通过配置流量策略来控制和观察出站流量。 基于上面这两种Gateway，Istio提供了强大的流量管理能力，使得我们能够对服务网格中的流量入口和出口进行精细的控制和观察，进而满足各种复杂的流量策略需求和安全需求。 ————————————————————————————————————————————— 这里总结在以上过程中涉及的动作和动作主体，可以将其中的每个过程都抽象成一句话:服务调用双方的Envoy代理拦截流量，并根据管理面的相关配置执行相应的治理动作，这也是Istio的数据面和控制面的配合方式。 Istio 应用实例 环境搭建 保证在k8s环境已经安装好了Istio,如果未安装请参考Istio setup 进行安装 1 创建命名空间 kubectl create ns istio-demo 2 命名空间打标签，允许Istio进行Pod注入 kubectl label namespace default istio-injection=enabled 3 部署实验demo kubectl apply -f manifest/foreend_all.yaml kubectl apply -f manifest/backend_all.yaml 确保所部署Pod/Service/DestinationRule/VirtualService/Gateway 运行成功 kubectl get all -n istio-demo &gt; NAME READY STATUS RESTARTS AGE pod/backend-v1-65fd86dcdd-ltcjj 2/2 Running 0 3m43s pod/backend-v1-65fd86dcdd-nq648 2/2 Running 0 3m55s pod/backend-v2-59c77fb946-m8sm5 2/2 Running 0 3m47s pod/foreend-857d6fddcc-vs6cx 2/2 Running 0 8m35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/backend-service ClusterIP 10.100.178.64 &lt;none&gt; 80/TCP 15m service/foreend-service ClusterIP 10.102.94.224 &lt;none&gt; 80/TCP 15m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/backend-v1 2/2 2 2 15m deployment.apps/backend-v2 1/1 1 1 15m deployment.apps/foreend 1/1 1 1 15m NAME DESIRED CURRENT READY AGE replicaset.apps/backend-v1-65fd86dcdd 2 2 2 3m55s replicaset.apps/backend-v1-8458cff47 0 0 0 15m replicaset.apps/backend-v2-59c77fb946 1 1 1 3m47s replicaset.apps/backend-v2-69d465559f 0 0 0 15m replicaset.apps/foreend-6cc87765f9 0 0 0 15m replicaset.apps/foreend-857d6fddcc 1 1 1 8m35s replicaset.apps/foreend-9bb648dcf 0 0 0 9m47s 4 开启外网访问 部署gateway资源 kubectl apply -f manifest/gateway.yaml 查看gateway 端口映射 kubectl get svc -n istio-system &gt;&gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.103.120.4 &lt;none&gt; 3000/TCP 5d18h istio-egressgateway ClusterIP 10.107.71.254 &lt;none&gt; 80/TCP,443/TCP 5d18h istio-ingressgateway LoadBalancer 10.106.234.50 &lt;pending&gt; 15021:31299/TCP,80:31510/TCP,443:30119/TCP,31400:30718/TCP,15443:31501/TCP 5d18h istiod ClusterIP 10.96.76.135 &lt;none&gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 5d18h jaeger-collector ClusterIP 10.104.113.121 &lt;none&gt; 14268/TCP,14250/TCP,9411/TCP,4317/TCP,4318/TCP 5d18h kiali ClusterIP 10.103.75.77 &lt;none&gt; 20001/TCP,9090/TCP 5d18h loki-headless ClusterIP None &lt;none&gt; 3100/TCP 5d18h prometheus ClusterIP 10.109.227.217 &lt;none&gt; 9090/TCP 5d18h tracing ClusterIP 10.96.28.55 &lt;none&gt; 80/TCP,16685/TCP 5d18h zipkin ClusterIP 10.111.47.170 &lt;none&gt; 9411/TCP 5d18h root@incleelinux:~/eshop# 我们看到istio-ingressgateway的80端口nodeport 被映射到宿主机31510端口 配置Nginx代理到集群内部 upstream gateway-router{ server 192.168.1.5:31510; } server { listen 80; server_name istio-demo.com; location /{ proxy_set_header Host $host; proxy_set_header User-Agent $http_user_agent; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://gateway-router; } } 配置完hosts 就可以访问到k8s部署的istio-demo服务了 curl -v http:istio-demo &gt;&gt; {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-8768b489-kjp5d&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} 实例 负载均衡 参考backend-all.ymal的DestinationRule配置： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: backend namespace: istio-demo spec: host: backend-service trafficPolicy: loadBalancer: simple: ROUND_ROBIN # 轮训负载均衡 # simple: RANDOM # 随机负载均衡 subsets: - name: v1 labels: version: v1 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v2 labels: version: v2 connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: interval: 1s #每隔1秒发现一次异常 consecutive5xxErrors: 2 #连续两次出错就会被认为是异常 baseEjectionTime: 5m #发现为异常的实例将在30s内被排除 maxEjectionPercent: 50 #安全策略，保证即使某些实例被识别为异常，仍有50%的实例可用，避免全部实例都被熔断 --- 在sepc.trafficPolicy.loadBalancer中更改配置算法，然后请求看对应的响应效果。 subset 内部配置的trafficPolicy 会覆盖掉DestinationRule.spec中的trafficPolicy 灰度发布 基于比例的灰度 参考backend-all.yaml的VirtualService配置： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: backend namespace: istio-demo spec: hosts: - backend-service http: - route: - destination: host: backend-service subset: v1 weight: 20 - destination: host: backend-service subset: v2 weight: 80 更改对应的流量比例（weight），观察流量分发结果。 基于内容的灰度 #根据匹配内容继续流量分发 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: backend namespace: istio-demo spec: hosts: - backend-service http: - match: - headers: User-Agent: regex: .*(Chrome/([\\d.]+)).* route: - destination: host: backend-service subset: v1 - route: - destination: host: backend-service subset: v2 执行的结果为： ~ $ for i in `seq 100`; do curl -H 'User-Agent: Chrome/12.2' http://backend-service/backend; echo ''; sleep 1; done; {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} {&quot;Pod\\u540d\\u5b57&quot;:&quot;backend-v1-6457f5d6b4-h4z26&quot;,&quot;\\u670d\\u52a1\\u7248\\u672c&quot;:&quot;v1&quot;} 灰度发布和负载均衡之间会相互影响吗？ 在 Istio 的流量管理体系中，灰度发布的比例设置和负载均衡算法是相互独立的。他们各自独立控制不同层次的流量控制和路由。 灰度发布的比例控制是通过 VirtualService 配置实现的，它主要决定了流量应该被发送到哪些服务的哪些版本(subset)。例如，你可以设置80%的流量发送到v1版本，20%的流量发送到v2。 而关于如何在具体的版本(subset)之间，也就是在同一个版本的多个 Pod 之间分配流量，这是由 DestinationRule 中的loadBalancer 字段控制的。你可以在这里指定具体的负载均衡策略，例如 ROUND ROBIN（轮询）、LEAST CONN（最小连接）或者 RANDOM(随机）等。 所以，在一个典型的 Istio 配置中，VirtualService 会决定流量向哪个 Service 的哪个版本（subset）流动，而 DestinationRule 则决定这个流量如何在这个版本的多个 Pod 之间进行分配。这两部分配置可以独立进行，灰度发布与负载均衡算法相互不影响。 超时 我们在backend的VirtualService为subset:v2 路由增加5ms的测试 #根据匹配内容继续流量分发 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: backend namespace: istio-demo spec: hosts: - backend-service http: - match: - headers: User-Agent: regex: .*(Chrome/([\\d.]+)).* route: - destination: host: backend-service subset: v1 - route: - destination: host: backend-service subset: v2 timeout: 5ms 开启测试结果： for i in `seq 100`; do curl http://backend-service/backend; echo ''; sleep 1; done; upstream request timeout upstream request timeout upstream request timeout upstream request timeout HTTP 重定向 参考foreend的VirtualService apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: foreend namespace: istio-demo spec: hosts: - &quot;istio-demo.com&quot; gateways: - istio-system/istio-demo-gateway http: - match: # 重定向 - uri: prefix: /notexist redirect: uri: /default authority: istio-demo.com - match: # 重写 - uri: prefix: /rewrite rewrite: uri: /?panic=1 retries: # 重试 attempts: 3 perTryTimeout: 2s retryOn: 5xx route: - destination: host: foreend-service subset: v1 - match: # 故障注入超时 curl -m 1 -v http://istio-demo.com/delay - uri: prefix: /delay fault: delay: fixedDelay: 3s percent: 100 route: - destination: host: foreend-service subset: v1 - match: # 故障注入中断 curl -m 1 -v http://istio-demo.com/abort - uri: prefix: /abort fault: abort: httpStatus: 500 percentage: value: 100 route: - destination: host: foreend-service subset: v1 # 默认路由 - route: - destination: host: foreend-service subset: v1 #timeout: 500ms 我们把/notexist从定向到/default HTTP 重写 参考foreend的VirtualService，我们把/rewrite重写到同一个服务的/?panic=1 路径 关于HTTP重定向和重写 HTTP重定向和HTTP重写是网络中常见的两种技术，它们有相同的目标——改变用户请求的URL，但它们的工作原理和用途是有所不同的。 HTTP重定向（Redirect）的工作原理是，当服务器收到用户请求后，会返回一个特别的响应，响应的状态码通常是301（永久重定向）或302（临时重定向），并在响应头中包含一个新的位置（Location），通知客户端重新发送请求到新的位置。重定向通常被用在网站迁移、合并网页、更改URL结构等场景。 HTTP重写（Rewrite）则是在服务器内部完成的，当服务器收到用户请求时，会根据预设的规则改变请求的URL，然后再对改变后的URL进行处理。用户一般是看不到这个过程的，URL在地址栏上显示的还是原来的URL。重写通常被用在美化URL、隐藏真实的文件路径、使URL更符合SEO等场景。 重试 根据上面的重写，我们把我们把/rewrite重写到同一个服务的/?panic=1 路径，这个路径的请求会返回500错误，这是Envoy代理会开启重试 我们可以看到foreend 服务端处理日志： 2024-02-05T21:48:24.190656694+08:00 127.0.0.6 - - [05/Feb/2024 13:48:24] &quot;GET /?panic=1 HTTP/1.1&quot; 500 - 2024-02-05T21:48:24.237931694+08:00 127.0.0.6 - - [05/Feb/2024 13:48:24] &quot;GET /?panic=1 HTTP/1.1&quot; 500 - 2024-02-05T21:48:24.288567648+08:00 127.0.0.6 - - [05/Feb/2024 13:48:24] &quot;GET /?panic=1 HTTP/1.1&quot; 500 - 2024-02-05T21:48:24.315263829+08:00 127.0.0.6 - - [05/Feb/2024 13:48:24] &quot;GET /?panic=1 HTTP/1.1&quot; 500 - 第一条是从写返回的500，后面重试了3次，对应的配置： retries: # 重试 attempts: 3 perTryTimeout: 2s # retryOn: 5xx 熔断 参考配置： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: backend namespace: istio-demo spec: host: backend-service trafficPolicy: loadBalancer: simple: ROUND_ROBIN # 轮训负载均衡 # simple: RANDOM # 随机负载均衡 connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: interval: 1s #每隔1秒检测一次异常情况 consecutive5xxErrors: 2 #连续两次出错就会被驱逐 baseEjectionTime: 5m #实例被重新加入可服务endpoint的间隔 maxEjectionPercent: 100 #驱逐百分比，100表示所有pod都可驱逐 subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- #根据比例进行流量分发 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: backend namespace: istio-demo spec: hosts: - backend-service http: - route: - destination: host: backend-service subset: v1 weight: 50 - destination: host: backend-service subset: v2 weight: 50 测试请求： for i in `seq 100`; do curl http://backend-service/circlebreak; echo ''; sleep 1; done; &lt;!doctype html&gt; &lt;html lang=en&gt; &lt;title&gt;500 Internal Server Error&lt;/title&gt; &lt;h1&gt;Internal Server Error&lt;/h1&gt; &lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;/p&gt; {&quot;message&quot;:&quot;ok&quot;} {&quot;message&quot;:&quot;ok&quot;} {&quot;message&quot;:&quot;ok&quot;} &lt;!doctype html&gt; &lt;html lang=en&gt; &lt;title&gt;500 Internal Server Error&lt;/title&gt; &lt;h1&gt;Internal Server Error&lt;/h1&gt; &lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;/p&gt; no healthy upstream {&quot;message&quot;:&quot;ok&quot;} no healthy upstream no healthy upstream v2的circlebreak接口会返回500错误，我们看到返回两次后，v2pod会被认为异常，我们100%把v2的pod驱逐以后，后面到v2的请求返回了no healthy upstream 限流 ","link":"https://nomagic.cc/post/istio-jie-shao-yu-ji-ben-shi-yong/"},{"title":"kubelet源码解析","content":"kubelet kubelet 主要作用 同步pod状态 从数据源(本地文件，url接口，apiserver)获取Pod的期望状态，同步所管理的pod至期望状态 Pod 状态同步至ApiServer 容器的健康监测 监测容器的健康状态，根据Pod设置的重启策略进行重启, 节点资源监控 监控节点资源使用状态，定期向Master节点进行报告 主要逻辑体现在Kubelet的Run 方法 // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) { 。。。 // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) 。。。 // 同步Pod 状态到ApiServer kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // Pod生命周期事件 kl.pleg.Start() ///Pod 状态同步循环 kl.syncLoop(updates, kl) } pleg PLEG（Pod Lifecycle Event Generator）确保pod状态和podspec保持一致。 PLEG会一直调用container runtime获取本节点containers/sandboxes的信息，并与自身维护的pods cache信息进行对比，生成对应的PodLifecycleEvent，然后输出到eventChannel中，通过eventChannel发送到kubelet syncLoop进行消费。 简单来说，PLEG的作用包括： 周期性地从容器运行时获取最新的容器状态。 生成Pod生命周期事件。 通过事件通道将这些事件发送给kubelet的同步循环进行处理 Pod 状态同步实现 状态同步是指从数据源获取pod变更时间，同步到本地缓存同步到POD期望状态，并通过cri同步到container runtime 整体同步流程 变更事件的获取 PodConfig PodConfig创建和管理 Kubernetes Pod 配置的对象。它提供了一种整合多个配置源的方式，将这些配置源规范化为一个流，进而更新为 Pod 配置，也就是说PodConfig是一个多路复用器，聚合来自本地文件，url请求和api server的事件 type PodConfig struct { pods *podStorage //接收mux传递过来的PodUpdate，merge成pods缓存和有序的变更写入到updates channel mux *config.Mux //多路复用器 // the channel of denormalized changes passed to listeners updates chan kubetypes.PodUpdate // contains the list of all configured sources sourcesLock sync.Mutex sources sets.String } func NewPodConfig(mode PodConfigNotificationMode, recorder record.EventRecorder) *PodConfig { updates := make(chan kubetypes.PodUpdate, 50) storage := newPodStorage(updates, mode, recorder) podConfig := &amp;PodConfig{ pods: storage, mux: config.NewMux(storage), // mux 把变更传给 storage,storage 把变更merge到updates channel updates: updates, sources: sets.String{}, } return podConfig } config.Mux type Mux struct { merger Merger //这里对应的实现是podStorage sourceLock sync.RWMutex // Maps source names to channels sources map[string]chan interface{} } func NewMux(merger Merger) *Mux { mux := &amp;Mux{ sources: make(map[string]chan interface{}), merger: merger, } return mux } // 返回生成的channel，内部监听channel的可读时间，然后回调到merge func (m *Mux) ChannelWithContext(ctx context.Context, source string) chan interface{} { if len(source) == 0 { panic(&quot;Channel given an empty name&quot;) } m.sourceLock.Lock() defer m.sourceLock.Unlock() channel, exists := m.sources[source] if exists { return channel } newChannel := make(chan interface{}) m.sources[source] = newChannel go wait.Until(func() { m.listen(source, newChannel) }, 0, ctx.Done()) return newChannel } func (m *Mux) listen(source string, listenChannel &lt;-chan interface{}) { for update := range listenChannel { m.merger.Merge(source, update) } } 监听注册 在创建Kubelet的含函数中调用makePodSourceConfig方法创建PodConfig实例，并且注册了对本地文件，URL和API server的监听，通过监听channel写入podStorage.Merge func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, nodeHasSynced func() bool) (*config.PodConfig, error) { // source of all configuration cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder) ctx := context.TODO() if kubeCfg.StaticPodPath != &quot;&quot; { config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.FileSource)) } // define url config source if kubeCfg.StaticPodURL != &quot;&quot; { config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.HTTPSource)) } if kubeDeps.KubeClient != nil { config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(ctx, kubetypes.ApiserverSource)) } return cfg, nil } podStorage Merge 把多个数据源里的合并成一组有序的updates变更时间，写入到updates管道，并且管理缓存所维护的pods（这个在merge方法里实现） func (s *podStorage) Merge(source string, change interface{}) error { ... adds, updates, deletes, removes, reconciles := s.merge(source, change) switch s.mode { case PodConfigNotificationIncremental: s.updates &lt;- *removes s.updates &lt;- *adds s.updates &lt;- *updates s.updates &lt;- *deletes s.updates &lt;- *adds s.updates &lt;- *reconciles ... return nil } Pod事件分发 syncLoop 可以看到Kubelet.Run 开启了， 调用syncLoop同步loop syncLoop中处理了 PodUpdate pod变更事件 pleg 生命呢周期事件 syncCh 定时同步 liveness 事件 readiness 事件 func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) { ... // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil { go kl.cloudResourceSyncManager.Run(wait.NeverStop) } ... if err := kl.initializeModules(); err != nil { kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, &quot;Failed to initialize internal modules&quot;) os.Exit(1) } // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) ... go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) ... // 同步POD状态到APIServer kl.statusManager.Start() if kl.runtimeClassManager != nil { kl.runtimeClassManager.Start(wait.NeverStop) } // POD 生命事件处理 kl.pleg.Start() kl.syncLoop(updates, kl) } func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) { ... for { ... if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } } } func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool { select { case u, open := &lt;-configCh: switch u.Op { case kubetypes.ADD: handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: handler.HandlePodUpdates(u.Pods) case kubetypes.SET: default: klog.ErrorS(nil, &quot;Invalid operation type received&quot;, &quot;operation&quot;, u.Op) } case e := &lt;-plegCh: if e.Type == pleg.ContainerStarted { kl.lastContainerStartedTime.Add(e.ID, time.Now()) } if isSyncPodWorthy(e) { // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok { klog.V(2).InfoS(&quot;SyncLoop (PLEG): event for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;event&quot;, e) handler.HandlePodSyncs([]*v1.Pod{pod}) } else { // If the pod no longer exists, ignore the event. klog.V(4).InfoS(&quot;SyncLoop (PLEG): pod does not exist, ignore irrelevant event&quot;, &quot;event&quot;, e) } } if e.Type == pleg.ContainerDied { if containerID, ok := e.Data.(string); ok { kl.cleanUpContainersInPod(e.ID, containerID) } } case &lt;-syncCh: podsToSync := kl.getPodsToSync() handler.HandlePodSyncs(podsToSync) case update := &lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { handleProbeSync(kl, update, handler, &quot;liveness&quot;, &quot;unhealthy&quot;) } case update := &lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) status := &quot;&quot; if ready { status = &quot;ready&quot; } handleProbeSync(kl, update, handler, &quot;readiness&quot;, status) case update := &lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) status := &quot;unhealthy&quot; if started { status = &quot;started&quot; } handleProbeSync(kl, update, handler, &quot;startup&quot;, status) case &lt;-housekeepingCh: if !kl.sourcesReady.AllReady() { // If the sources aren't ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. klog.V(4).InfoS(&quot;SyncLoop (housekeeping, skipped): sources aren't ready yet&quot;) } else { start := time.Now() klog.V(4).InfoS(&quot;SyncLoop (housekeeping)&quot;) if err := handler.HandlePodCleanups(); err != nil { klog.ErrorS(err, &quot;Failed cleaning pods&quot;) } duration := time.Since(start) if duration &gt; housekeepingWarningDuration { klog.ErrorS(fmt.Errorf(&quot;housekeeping took too long&quot;), &quot;Housekeeping took longer than 15s&quot;, &quot;seconds&quot;, duration.Seconds()) } klog.V(4).InfoS(&quot;SyncLoop (housekeeping) end&quot;) } } return true } pod变更事件同步 处理流程 podupdate 事件统一通过 SyncHandler 的接口的handlerPod*方法投递到podWorkers.UpdatePod中，可以看到podWorkers 为每一个Pod 创建了一个单独的channel 和 managePodLoop 协程来处理这个Pod的变更事件，在managePodLoop协程中将处理Pod的TerminatedPodWork、TerminatingPodWork,其余update时间分别代理到kubelet 的syncPod/syncTerminatingPod/syncTerminatedPod 方法，并把PodId 写入到了podWokers.workQueue 队列 syncPod/syncTerminatingPod/syncTerminatedPod 分别调用ContainerRuntime(调用CRI接口)实现容器的创建和删除、状态上报ApiServer、挂载/卸载容器、创建/删除CGroup 和各种Manner的注册与清理操作 syncCh 定时同步 参考Kubelet的syncLoop中处理syncCh的定时逻辑，这里的逻辑就是定期的从podWokers.workQueue中取出pod进行同步 pleg 生命周期事件同步 PLEG，全称Pod Lifecycle Event Generator，通过周期性的获取Pod状态和本地缓存POD老状态进行对比，从而生成生命周期事件，写入到事件channel kubelet 在synLoop循环syncLoopIteration中读取Pod生命周期事件派发到dispatchwork进行状态同步 处理流程 Probe 事件处理（liveness/readiness） 处理流程 Kubetlet在创建是创建了 klet.livenessManager = proberesults.NewManager() klet.readinessManager = proberesults.NewManager() klet.startupManager = proberesults.NewManager() 并且交于ProbeManager进行管理。 在POD进行synd的过程中（syncPod方法）会通过ProbeManager.Add方法接入probe的管理，按照probe类别，分别生成创建对应.prober.Worker 跑在不同的协程中。每个worker中有一个定时循环定期的调用doProbe进行探测，把探测结果通过对应probeManner.Set写回updates管道。 kubelet 会在syncLoopIteration循环中监听probe的updates更新，然后通过HandlePodSyncs提交同步任务到syncWorker,后面就是pod的sync流程了。 ","link":"https://nomagic.cc/post/kubelet-yuan-ma-jie-xi/"},{"title":"微服务架构下的数据一致性方案","content":"我们知道微服务开发模式下，每个微服的存储也是独立隔离开的，每个服务都有自己单独的存储，这就引发了一个问题如何确保微服务架构下的数据一致性呢？更具体的是说如何实现原子性和隔离性的呢？ 原子性和隔离性在分布式下分别对应原子提交和并发控制，原子提交也就是说事务需要有回滚的能力，并发控制也就是通过加锁或者多版本来实现访问相同数据的并发事务实现隔离。 首先需要明确的是微服务模式下有哪些数据一致性的挑战： 分布式事务：多个服务协调工作来保证数据的一致性 同步和复制：数据同步复制的一致性 并发访问：多个服务并发访问资源 分布式事务管理器（Distributed Transaction Manager，简称DTM）两阶段/三阶段提交 基本原理 分布式事务管理器的工作原理可以简单概括为以下几个步骤： 事务的开始：分布式事务管理器接收到一个事务请求，并为该事务生成一个全局唯一的事务ID。 事务的协调：分布式事务管理器将该事务的操作分发给不同的节点进行执行，并记录每个节点的执行结果。 事务的提交或回滚：分布式事务管理器根据节点的执行结果，决定是提交还是回滚该事务。如果所有节点的操作都成功执行，则提交该事务；如果有任何一个节点的操作失败，则回滚该事务。 组成 事务管理器（TM）：事务管理器是分布式事务管理器的核心组件，负责协调分布式事务的执行。 资源管理器（RM）：资源管理器负责管理具体的资源，如数据库、消息服务器等。 参与者：参与者是指执行分布式事务的应用程序。 实现方案 分布式事务管理器的实现方式有很多种，常见的实现方式包括： 基于XA协议的实现：XA协议是一种工业标准的二阶段提交协议，可用于实现分布式事务。 基于消息队列的实现：消息队列可以用于实现分布式事务的最终一致性。 基于补偿机制的实现：补偿机制是一种可靠性保障机制，可用于实现分布式事务的回滚。 XA 协议 XA 协议是由 X/Open 公司于 1991 年发布的一套标准协议。XA 是 eXtended Architecture 的缩写，因此该协议旨在解决如何在异构系统中保证全局事务的原子性。 基于XA协议的实现分布式事务管理器，需要以下几个组件： 事务管理器（TM)：负责协调分布式事务的执行。 资源管理器（RM）：负责管理具体的资源，如数据库、消息服务器等。 参与者（应用程序：AP）：参与者是指执行分布式事务的应用程序。 XA协议的执行流程如下： 参与者（应用程序）向事务管理器（TM）发起事务请求。 TM向各个RM发起准备（prepare）请求。 RM响应TM的准备请求，如果准备成功，则返回PREPARED；如果准备失败，则返回NOT_READY。 TM收到所有RM的准备响应后，如果所有响应都是PREPARED，则向各个RM发起提交（commit）请求；如果有一个响应不是PREPARED，则向各个RM发起回滚（rollback）请求。 RM响应TM的提交或回滚请求。 TM的实现 TM的实现需要提供以下功能： 创建事务：TM需要提供一个接口，用于创建事务。 加入事务：TM需要提供一个接口，用于加入事务。 提交事务：TM需要提供一个接口，用于提交事务。 回滚事务：TM需要提供一个接口，用于回滚事务。 RM的实现 RM的实现需要提供以下功能： 支持XA协议：RM需要实现XA协议的接口，用于响应TM的准备、提交和回滚请求。 执行业务操作：RM需要根据业务需求，执行具体的业务操作。 实现代码参考 type XAResource interface { // 准备事务 Prepare(xid string) error // 提交事务 Commit(xid string) error // 回滚事务 Rollback(xid string) error } type MyXAResource struct { // 资源 ID resourceId string } func (m *MyXAResource) Prepare(xid string) error { // 执行准备逻辑 return nil } func (m *MyXAResource) Commit(xid string) error { // 执行提交逻辑 return nil } func (m *MyXAResource) Rollback(xid string) error { // 执行回滚逻辑 return nil } type XATransactionManager struct { // 资源管理器列表 resourceManagers []XAResource } func (m *XATransactionManager) Begin() (xid string, err error) { // 生成事务 ID xid = uuid.NewString() // 向所有资源管理器发起准备请求 for _, resourceManager := range m.resourceManagers { err = resourceManager.Prepare(xid) if err != nil { return &quot;&quot;, err } } return xid, nil } func (m *XATransactionManager) Commit(xid string) error { // 向所有资源管理器发起提交请求 for _, resourceManager := range m.resourceManagers { err := resourceManager.Commit(xid) if err != nil { return err } } return nil } func (m *XATransactionManager) Rollback(xid string) error { // 向所有资源管理器发起回滚请求 for _, resourceManager := range m.resourceManagers { err := resourceManager.Rollback(xid) if err != nil { return err } } return nil } 使用例子： // 创建 XA 事务管理器 transactionManager := &amp;XATransactionManager{ resourceManagers: []XAResource{ &amp;MyXAResource{resourceId: &quot;resource1&quot;}, &amp;MyXAResource{resourceId: &quot;resource2&quot;}, }, } // 开始事务 xid, err := transactionManager.Begin() if err != nil { // 处理错误 } // 执行业务操作 // ... // 提交事务 err = transactionManager.Commit(xid) if err != nil { // 处理错误 } 和两阶段提交的关系（2PC） XA协议和两阶段提交（Two-Phase Commit，简称2PC）是分布式系统中实现事务一致性的两个关键概念。 XA协议：XA协议是一种由X/Open组织定义的分布式事务处理的标准协议。它定义了一组接口和规范，用于在分布式环境下协调事务的提交和回滚。XA协议的核心是将事务分为全局事务（Global Transaction）和本地事务（Local Transaction），并通过协调者（Coordinator）和参与者（Participant）之间的通信来实现事务的一致性。 两阶段提交（Two-Phase Commit）：两阶段提交是一种基于XA协议的分布式事务协调协议。它通过两个阶段的操作来保证所有参与者要么都提交事务，要么都回滚事务，从而实现分布式事务的一致性。首先，协调者向所有参与者发送事务准备请求，并等待参与者的响应。然后，在所有参与者都准备好提交事务的情况下，协调者向所有参与者发送提交请求；否则，协调者向所有参与者发送回滚请求。 因此，XA协议是一种通用的事务处理标准协议，而两阶段提交是基于XA协议的具体实现机制。XA协议定义了事务的分布式处理模型，而两阶段提交则是在此模型基础上进行具体操作的协议 三阶段提交 两阶段提交存在的问题，如果在事务准备阶段过后，协调者出了状况，那么所有的参与者不知道接下来要怎么办，整个事务就卡住了无法进行。 三阶段提交增加了一个预提交阶段。 CanCommit PreCommit DoCommit 在第一个阶段后，协调者出现问题的时候，因为并没有真的锁定资源，所以这时候系统是没有被堵塞的。加入在第二个阶段协后调者出现问题，参与者超时后会自动提交事务，因为第一步的检查会使PreCommit执行成功的概率增大，也就是需要回滚的概率降低。但是由于网络原因某个参与者没有收到协调者的回滚信息，进行了超时提交的概率还是存在的。这种网络或者意外的原因无法很好的解决，比如在两阶段提交中，提交的时候，有一个参与者宕机了，又没能进行很快的恢复，这时其他事务就有可能读取到这个事务的不完整状态。 ##分布式一致性算法（例如，Paxos和Raft） 算法通过选举和消息传递机制来实现分布式数据一致性。通过协商和达成共识来确保数据的一致性，并处理节点故障和网络分区等问题。这些算法提供了更强的容错性和可扩展性，但实现和理解上更为复杂，这里不展开叙述。 数据最终一致性 通过上面的叙述和CAP理论我们可以知道，在分布式的环境下，服务保持可用的情况下，做不到强一致性。 而当业务发生在多个服务中，我们追求最终一致性。那么怎么解决上面提到的问题，做到跨服务的最终一致性呢？ 避免同时跨服务的写操作 这是个业务问题，在微服务的架构下，每个服务都是独立的，如果有一个业务功能需要同时修改两个服务的数据，往往这个业务可以拆分成两个步骤，比如场景一种提到的订单和库存的例子，如果我们可以先锁定库存，然后再确认订单看上去这个问题就迎刃而解了。 因此在业务中发现一个功能需要同时修改两个服务的数据，我们首先可以来讨论这个业务设计是否合理；如果业务上很多场景都要求两个服务的数据保持强一致，那可能我们需要看看微服务的划分是否合理。 最大努力通知 + 最大努力处理 为了解决场景二和场景三的不一致性问题，需要上游服务和下游服务的共同努力： 上游服务需要尽可能将事件发送出去，比如：先同步发送，如果失败改为异步重试，重试多次仍然失败可以先持久化，通过定时任务来重发或者人工干预重发。 下游服务也要尽可能的把事件处理掉，收到事件后可以考虑先将事件持久化，消费成功后标记事件，如果消费失败可以通过定时任务重试消费。 保证幂等性 当我们提到重试，就不得不考虑幂等性的问题，这里的幂等性包括以下两个场景： 上游服务接口的幂等性，保证下游系统的重试逻辑可以得到正确响应 下游服务消费事件保证幂等性，避免因上游多发事件或事件已消费成功后再次重试产生的问题 核心业务数据补偿机制 在分布式系统的执行链路上，每个节点都有可能失败，加上业务的复杂度，即便我们做了很多我们认为万全的准备，数据不一致的情景也很难彻底解决，而对于那些小概率发生但技术解决起来成本昂贵的问题，我们可以尝试通过对业务的深刻理解设计一些后台的维护功能，保证在核心业务数据异常时，可以在一定的规则内进行修复，从而保证业务的顺利进行。 ","link":"https://nomagic.cc/post/wei-fu-wu-jia-gou-xia-de-shu-ju-yi-zhi-xing-fang-an/"},{"title":"函数式编程","content":"函数式编程 理论基础： 真身：不是编程语言里的函数，而是数学里的函数概念。 λ演算 一套用于研究函数定义、应用和递归的形式系统 λ演算被称为最小的通用编程语言。可以用来清晰地定义什么是一个可计算函数。它包括一条变换规则（变量替换）和一条函数定义方式 语法规则 任何λ表达式都可以通过下述三条BNF范式描述： **范式1、**＜expr＞ ::= ＜identifier＞ f(x) = x =&gt; λ x. **范式2、**＜expr＞ ::= (λ ＜identifier＞ . ＜expr＞) f (x) = x + 2 =&gt;λ x. x +2; f(x,y)=x+y =&gt;（λ x. λ y. x + y） **范式3、**＜expr＞ ::= (＜expr＞ ＜expr＞) f(y)=y+2; f(x)=1-x =&gt;（λ x.x+2)(λ x.1-x) ==&gt; f(1-x) = 1-x+2 无状态和无副作用： 关于状态： 一段程序里如果有一个变量需要维护，程序的确定性会比较高，如果有10个变量，emmm想要维护确定性，或者想要知道在某个时刻程序处于什么状态是有难度的。 那么怎么做？面向对象是把状态的变更封装起来。 与其建立种种机制来控制可变的状态，不如尽可能消灭可变的状态 函数式编程如何消灭状态？ 还是对应于数学中的函数的概念：定义域←→映射←→值域，函数除了进行映射之外，其他啥也不干。 无状态实现 每次计算都基于输入产生输出，本身无状态，也不依赖除参数外的任何状态 threshold = 5 def check(score,threshold): if score &gt; threshold: return &quot;Good&quot; else: return &quot;Bad&quot; def checkUnSure(score): if score &gt; threshold: return &quot;Good&quot; else: return &quot;Bad&quot; assert(check(6,5) == &quot;Good&quot;) assert(check(6,5) == &quot;Good&quot;) threshold = 7 assert(checkUnSure(6) == &quot;Good&quot;) // 不确定checkUnSure(6)到底什么结果 threshold = &quot;a string&quot; // 或者其他某个地方悄悄把threshold改掉。 assert(checkUnSure(6) == &quot;Good&quot;) 无副作用 除了输出参数外，不会对其他任何东西产生影响 def checkUnSure(score): if score &gt; threshold: threshold = &quot;&quot; return &quot;Good&quot; else: return &quot;Bad&quot; 纯函数：没有副作用，仅依赖于输入参数的函数。 不可变性：将数据视为不可更改的，避免直接修改原始数据。 还是映射数学中函数的概念：函数没办法修改参数，参数只是求值时的一个定义域元素的拷贝 核心工具与数据结构： 用高阶抽象取代基本的控制结构,更接近于自然语言，更容易理解 总得逻辑 做什么而不是怎么做，按照做什么大概分为3类： 筛选 映射 规约 实现的工具 filter,map,reduce,every,some,sort,step,concat,slice,split,join,if else than --→ 如果按照是什么，而不是怎么做 那么这种工具会构造出更多 例子 //两端代码比较 animal = [] dogs = filter(animal,a.type == &quot;dog&quot;) dogs = [] for a in animal{ if a.type == &quot;dog&quot;{ dogs.append(a) } } 分页 普通实现 分页 size = 1000 pages = [] list = [...] idx = 0 page = [] for _,item := range list{ if idx == size{ pages.append(page) page = [] } page = append(item) size ++ } if !page.Empty(){ pages.append(page) } Step实现 //step --&gt;是每多少个元素为一不，而不是如何把元素按个数分割 func step(count,size,fn func(_s,_e int)){ s,e = 0 for e &lt; count{ fn(s,min(count,e+size)) e+=size } } //用step 实现 step(len(list),size,func(s,e){ pages.append(list[s:e])}) //如果想要方面理解，可以在多一层抽象 typedef split step split(len(list),size,func(s,e){ pages.append(list[s:e])}) //按个数分割 go 错误处理 普通实现 mtx, err := lock.Mutex(cc, lock.Key(&quot;lock:&quot;+unique), lock.TTL(ttl), retry) if err != nil { return err } if err = mtx.Lock(); err != nil { return err } defer func(){ err = erros.append(mtx.Unlock) }() if err = dowork();err != nil{ return err } 。。。。 if err = fun1();err != nil { return err } // 无止尽的错误处理代码 if-else-then type IfThen struct { err error } func If(err error) *IfThen { return &amp;IfThen{err: err} } func (it *IfThen) If(err error) { it.err = err } func (it *IfThen) Else(fn func()) *IfThen { if it.err != nil { fn() } return it } func (it *IfThen) Than(fn func() error) *IfThen { if it.err == nil { it.err = fn() } return it } func (it *IfThen) Final(fn func()) *IfThen { it.err = errors.append(fn()) return it } func (it IfThen) Error() error { return it.err } var mtx lock.Locker err = If(func() error { mtx, err = lock.Mutex(lx.ctx, lx.opts...); return err }()). Than(func() error { return mtx.Lock() }). Than(func() error { return dowork()}). Final(func() error { return mtx.Unlock() }). Error() // 如果Go支持λ表达式---&gt;可惜它不支持，随它去吧。 var mtx lock.Locker err = If({mtx, err = lock.Mutex(lx.ctx, lx.opts...); return err }). Than({mtx.Lock}). Than({dowork}). Final({mtx.UnLock}). Error() 函数组合：（代码组合和抽象方式） 通过将小型纯粹功能组合形成更大型复杂功能来构建程序逻辑。 还是对应的数学中函数的概念-→复合函数 高阶函数： 接受一个或多个其他函数作为参数，并/或返回一个新的以其他功能组合而成的功能。 f(g(c(x))) f()(gx) 柯里化 减少元的过程 def logplain(level,model,msg): print(level,model,msg) def log(level): def mlog(model): def output(msg): print(level,model,msg) return output return mlog info = log(&quot;info::&quot;)(&quot;db=&gt;&quot;) info(&quot;select id from t_user where name = 'aaa'&quot;) info(&quot;很方便&quot;) logplain(&quot;info&quot;,&quot;db=&gt;&quot;,&quot;msg&quot;) logplain(&quot;info&quot;,&quot;db=&gt;&quot;,&quot;msg&quot;) logplain(&quot;info&quot;,&quot;db=&gt;&quot;,&quot;msg&quot;) &quot;info&quot;,&quot;db=&gt;&quot; //很多重复输入 惰性求值： 只在需要时才进行计算，延迟执行操作可以提高效率并节省资源消耗。 并行化与异步编程： 函数式编程对并行化和异步编程提供了天然的支持，因为纯函数不依赖于共享状态。 class Student: def __init__(self): self.examd = False self.score = 0 def setScore(self,score): self.examd = True self.score = score def dispatch(student): ret = &quot;等待成绩&quot; if student.examd: if student.score &lt; 60: ret = &quot;去打螺丝&quot; else: ret = &quot;继续深造&quot; ret if __name__ == &quot;__main__&quot;: a = Student() b = Student() a.setScore(60) b.setScore(100) ### 虽然考了一百分，不加锁 有可能会跑去打螺丝 thread1 = Thread(target=dispatch, args=(copy(a),)) thread2 = Thread(target=dispatch, args=(b,)) 通过复制来减少共享状态，保证了逻辑一致性。 更容易写出安全代码 func = aysnc (){} funb = aysnc (){} result = await funa() * funb() #funa,funb 是两个异步独立执行的程序，如果之间相互依赖或者依赖于外部环境，那么这个结果就是无法确定的。 数据并行处理 MapReduce 的大数据处理中的应用 通过用户定义的map函数将输入分割成key/value对，然后处理该数据，最终通过Reduce函数将处理完成的记过合并。 模块化和可测试性： 函数式程序易于模块化，代码更容易被拆分成独立、可重用的组件。 纯函数易于测试，可以通过输入输出验证其正确性。 总结 用高阶抽象取代基本的控制结构，提高语法的表现能力 减少不确定性因素 一个操作的行为应该是确定的 用机制而不是用变量去控制 要做什么，而不是怎么去做。 各种编程范式之间不是绝对隔离的，而是相互融合，没有极端的状态 ","link":"https://nomagic.cc/post/han-shu-shi-bian-cheng/"},{"title":"boltdb 架构解读","content":"boltdb 是受LMDB项目的启发，使用的go语言实现的一个本地kv存储引擎。可嵌入到自己的项目中实现kv数据的本地存储，知名的项目etcd,consul,openvpn等使用了boltdb进行数据存储 逻辑结构 使用单个文件来存储kv数据，通过mmap文件内存映射把文件的磁盘地址映射成逻辑地址。 在数据组织上，boltdb是一颗大的btree,按照bucket分组数据,相当于表的概念，每一个bucket是一个子b-tree，数据以innode结构存储在b-tree的叶子节点。Cursor 作为b-tree的遍历器。进行节点的查找。 物理结构 存储上，通过mmap文件内存映射把文件映射到进程的逻辑地址，通过指针读取的方式读取到文件的内容。文件的开头存储DB的元数据。后面按照页面划分成存储块。每一个b-tree.Node对应一个页面的pid，将整个b-tree的树形结构映射成内存地址的线性结构。 数据组织 boltdb 把文件映射到虚拟地址空间，db.data指针指向这个地址，在物理空间上映射的地址空间按照Page进行管理，比如读取一个Page type page struct { id pgid flags uint16 count uint16 overflow uint32 ptr uintptr } func (db *DB) page(id pgid) *page { pos := id * pgid(db.pageSize) return (*page)(unsafe.Pointer(&amp;db.data[pos])) } 逻辑上，整个boltdb的数据组织成一颗b-tree，b-tree的一个节点是一个Page，根据节点类型的不同，Page中存储的数据不同，分别存储分支节点数据或叶子节点数据，树的遍历过程中根据BranchPage存储递归到对应的Page type branchPageElement struct { pos uint32 ksize uint32 pgid pgid } type leafPageElement struct { flags uint32 //标识是普通数据，还是嵌套bucket，如果是bucket,value 存储bucket信息 pos uint32 ksize uint32 vsize uint32 } pos 是数据距离element数组结束处的距离。这样处理方便根据element对象读取数据，参考下面的leafPageElement的key/value读取操作 页面数据的物理组织如下图 page.ptr的地址就是page页面开始存储数据地方，写入的前一部分存储元素（element的元数据），后面是需要存储的kv数据，根据pos+ksize/vsize 对kv进行读写 //写入 func (n *node) write(p *page) { if n.isLeaf { p.flags |= leafPageFlag } else { p.flags |= branchPageFlag } //b刚开始指向elements数组结束的地址，也就是第一个数据开始写入的地方。随着数据的写入，b往后移动写入数据的大小。 b := (*[maxAllocSize]byte)(unsafe.Pointer(&amp;p.ptr))[n.pageElementSize()*len(n.inodes):] for i, item := range n.inodes { if n.isLeaf { elem := p.leafPageElement(uint16(i)) elem.pos = uint32(uintptr(unsafe.Pointer(&amp;b[0])) - uintptr(unsafe.Pointer(elem))) elem.flags = item.flags elem.ksize = uint32(len(item.key)) elem.vsize = uint32(len(item.value)) } else { elem := p.branchPageElement(uint16(i)) elem.pos = uint32(uintptr(unsafe.Pointer(&amp;b[0])) - uintptr(unsafe.Pointer(elem))) elem.ksize = uint32(len(item.key)) elem.pgid = item.pgid _assert(elem.pgid != p.id, &quot;write: circular dependency occurred&quot;) } klen, vlen := len(item.key), len(item.value) if len(b) &lt; klen+vlen { b = (*[maxAllocSize]byte)(unsafe.Pointer(&amp;b[0]))[:] } copy(b[0:], item.key) b = b[klen:] copy(b[0:], item.value) b = b[vlen:] } } //读取 func (n *leafPageElement) key() []byte { buf := (*[maxAllocSize]byte)(unsafe.Pointer(n)) return (*[maxAllocSize]byte)(unsafe.Pointer(&amp;buf[n.pos]))[:n.ksize:n.ksize] } // value returns a byte slice of the node value. func (n *leafPageElement) value() []byte { buf := (*[maxAllocSize]byte)(unsafe.Pointer(n)) return (*[maxAllocSize]byte)(unsafe.Pointer(&amp;buf[n.pos+n.ksize]))[:n.vsize:n.vsize] } 整体btree 存储结构如下： 节点Page数据的读取： func (p *page) leafPageElement(index uint16) *leafPageElement { n := &amp;((*[0x7FFFFFF]leafPageElement)(unsafe.Pointer(&amp;p.ptr)))[index] return n } // leafPageElements retrieves a list of leaf nodes. func (p *page) leafPageElements() []leafPageElement { if p.count == 0 { return nil } return ((*[0x7FFFFFF]leafPageElement)(unsafe.Pointer(&amp;p.ptr)))[:] } // branchPageElement retrieves the branch node by index func (p *page) branchPageElement(index uint16) *branchPageElement { return &amp;((*[0x7FFFFFF]branchPageElement)(unsafe.Pointer(&amp;p.ptr)))[index] } // branchPageElements retrieves a list of branch nodes. func (p *page) branchPageElements() []branchPageElement { if p.count == 0 { return nil } return ((*[0x7FFFFFF]branchPageElement)(unsafe.Pointer(&amp;p.ptr)))[:] } 0x7FFFFFF: magic-number amd64的使用的是4-bytes的相对地址。所以一个程序不能申请超过2GB(0x7FFFFFFF)的静态数据,又因为sizeOf(branchPageElement) =16 ,这里的大小就是0x7FFFFFF，当然也可以使用其他的值，只是这里需要一个常量，把地址转换成数组指针，然后再转换成slice（slice内部使用是这个数组地址） 通过index索引就可以直接拿到对应的branchPageElement或者leafPageElement对象。 数据查询是b-tree搜索的过程，使用Cursor结构作为b-tree的遍历器， 数据更改是使用cursor.seek搜索到对应的节点，cursor.node中调用bucket.node方法，才把Page加载成为node结构。这样在事务执行的过程中，只有node的数据有变更，需要回写磁盘。 把Page读为node func (n *node) read(p *page) { n.pgid = p.id n.isLeaf = ((p.flags &amp; leafPageFlag) != 0) n.inodes = make(inodes, int(p.count)) for i := 0; i &lt; int(p.count); i++ { inode := &amp;n.inodes[i] if n.isLeaf { elem := p.leafPageElement(uint16(i)) inode.flags = elem.flags inode.key = elem.key() inode.value = elem.value() } else { elem := p.branchPageElement(uint16(i)) inode.pgid = elem.pgid inode.key = elem.key() } _assert(len(inode.key) &gt; 0, &quot;read: zero-length inode key&quot;) } // Save first key so we can find the node in the parent when we spill. if len(n.inodes) &gt; 0 { n.key = n.inodes[0].key _assert(len(n.key) &gt; 0, &quot;read: zero-length node key&quot;) } else { n.key = nil } } 在逻辑上node和page是一一对应关系，只是在事务提交写盘的时候，需要把node映射成新的页面（不覆盖原有页面，用来实现mvcc）,这些新的页面也就是代码中所说的dirty page。 freelist type freelist struct { ids []pgid // all free and available free page ids. pending map[txid][]pgid // mapping of soon-to-be free page ids by tx. cache map[pgid]bool // fast lookup of all free and pending page ids. } 在写事物执行的过程中，数据的更改需要申请新的页面，把原来旧的页面缓存到pending,用于更小txid的读取以实现mvcc。 ids 所有已经不在使用的空闲页面 cache 为了快速校验一个page是否在freelist 需要注意的是freelist进行alloc或者事务提交写入page进行pids合并时算法复杂度为n,freelist比较大时有可能会有影响， 事务的实现 事务分为读写事务和只读事务,在b-tree的查找过程中会建立一个查找路径上节点的内存映射。在事务提交时，把对应的内存会写到磁盘，事务回滚时直接丢弃修改。 原子性实现 事务在提交行会把事物执行期间所有变更的node，刷新到新申请的页面（dirty page)，然后write到文件，依赖于meta的保存状态，meta保存只有两种状态：成功或者失败，映射到了事务的原子性概念。 一致性实现 事务的变更数据不会覆盖旧的页面。整个DB的内存映像依赖meta进行构建，原子性的实现依赖于meta信息的正确保存，当前meta数据正确落盘以后，整个事务才算提交成功。如果当前写入meta失败了，由于有两个meta存在，没有在操作meta的映像可以确保在正确的状态，使得整个数据库处于一致性状态。 隔离实现 隔离的实现同样是依赖于MMVC策略， 事务在建立的过程中，使用磁盘内容建立的各自的内存快照，每个事务内存快照相互独立，在进行修改是也只是修改的自己内存状态，是事务提交时不会覆盖旧的页面，所以其他事物可以完整的读取到自己建立时的内存状态。（快照读的概念） 持久性实现 参照上面的磁盘刷盘 事务的提交 func (tx *Tx) Commit() error { .... ///b-tree的再平衡，b-tree node的节点过少，就把node 和兄弟节点合并。 var startTime = time.Now() tx.root.rebalance() if tx.stats.Rebalance &gt; 0 { tx.stats.RebalanceTime += time.Since(startTime) } ///把对应的node 写入page 把原来node的page放入free-list,申请新的页面，进行写入。 if err := tx.root.spill(); err != nil { tx.rollback() return err } tx.stats.SpillTime += time.Since(startTime) tx.meta.root.root = tx.root.root //freelist 重写 tx.db.freelist.free(tx.meta.txid, tx.db.page(tx.meta.freelist)) p, err := tx.allocate((tx.db.freelist.size() / tx.db.pageSize) + 1) if err != nil { tx.rollback() return err } if err := tx.db.freelist.write(p); err != nil { tx.rollback() return err } tx.meta.freelist = p.id //文件大小增长 if tx.meta.pgid &gt; opgid { if err := tx.db.grow(int(tx.meta.pgid+1) * tx.db.pageSize); err != nil { tx.rollback() return err } } //把tx的脏页面，写入db文件。 if err := tx.write(); err != nil { tx.rollback() return err } //写元数据信息 if err := tx.writeMeta(); err != nil { tx.rollback() return err } tx.stats.WriteTime += time.Since(startTime) // Finalize the transaction. tx.close() // Execute commit handlers now that the locks have been removed. for _, fn := range tx.commitHandlers { fn() } return nil } 事务的回滚 func (tx *Tx) rollback() { if tx.db == nil { return } if tx.writable { tx.db.freelist.rollback(tx.meta.txid) tx.db.freelist.reload(tx.db.page(tx.db.meta().freelist)) } tx.close() } 如果是写事务，freelist.rollback回滚掉freelist.pending freelist.reload 重新加载旧的freelist页面。 并发 因为data的文件加了文件锁，同时只会有一个进程读写db文件，同一个进程内，只允许有一个写事务在执行，这就保证了事务提交时不会有并发的情况。 所以boltDB 更适用于读多写少的场景，同样需要注意的是，长的事务没有提交前会使freelist中的页面不会释放，导致db文件的快速增长。 ","link":"https://nomagic.cc/post/boltdb-jia-gou-jie-du/"},{"title":"kube-proxy设计及实现","content":"不确定的 pod ip 我们知道在k8s中，应用是以pod模式部署运行的，deployment通过replicset控制pod满足应用的预期状态，在整个协调的过程中，pod是会动态的销毁或者重建，在整个过程中代表每个POD的网路地址及pod ip 是会不断在变动，这就带来了一个问题：如果某组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”） 集合提供功能，前端要如何发现并跟踪要连接的 IP 地址，以便其使用负载的后端组件呢？于是k8s引用了service抽象来解决这个问题。 service api 对象 service api 对象是如何解决上面的问题的呢？ endpoints kube-proxy 代理模式 ipvs ","link":"https://nomagic.cc/post/kube-proxy/"},{"title":"Containerd 源码解析","content":"containerd 介绍 （图片来源：https://blog.csdn.net/chengyinwu/article/details/121842772） Containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。在宿主机中以daemon的方式运行，管理当前主机上完整的容器生命周期，包括容器镜像的传输和存储、容器的执行和管理、存储和网络等。具体来说，Containerd 负责以下任务： 管理容器的生命周期（从创建容器到销毁容器）。 拉取/推送容器镜像。 存储管理（管理镜像及容器数据的存储）。 调用 runC 运行容器（与 runC 等容器运行时交互）。 管理容器网络接口及网络。 Containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用。 containerd 发展历史 参考：https://www.cnblogs.com/tencent-cloud-native/p/14134164.html?spm=wolai.workspace.0.0.f21d6a18xgG3Ac Containerd 架构图 CRI（容器运行时接口） https://github.com/kubernetes/cri-api 在早期 rkt 和 docker 争霸时，kubelet 中需要维护两坨代码分别来适配 docker 和 rkt ，这使得 kubelet 每次发布新功能都需要考虑对运行时组件的适配问题，严重拖慢了新版本发布速度。另外虚拟化已经是一个普遍的需求，如果出现了类型的运行时，SIG-Node 小组可能还需要把和新运行时适配的代码添加到 kubelet 中。这种做法并不是长久之计，于是在 2016 年，SIG-Node提出了容器操作接口 CRI（Container Runtime Interface）。 CRI 是对容器操作的一组抽象，只要每种容器运行时都实现这组接口，kubelet 就能通过这组接口来适配所有的运行时。 可以说CRI是接入k8s的容器运行时所需要时实现的接口，接口目前定义了两部分RuntimeService、ImageManagerService分别负责容器的管理和镜像的管理 可参考CRI接口定义 从上面的架构图和描述可以看到containerd本身包含蛮多内容，咱们这边文章主要的侧重点是kubelet如何和containerd的cri插件交互，然后创建和管理pod和container的。 源码分析 既然kubelet 通过containerd CRI 插件来管理容器，那我们把重点放在containerd的cri plugin上 对应的源码目录是pkg/cri,入口文件是cri.go cri grpc服务的启动。 我们知道kubelet 是通过grpc协议的方式和cri 插件通信的，那么cri grpc service 是如何启动的呢？ func init() { config := criconfig.DefaultConfig() plugin.Register(&amp;plugin.Registration{ Type: plugin.GRPCPlugin, ID: &quot;cri&quot;, Config: &amp;config, Requires: []plugin.Type{ plugin.EventPlugin, plugin.ServicePlugin, plugin.NRIApiPlugin, }, InitFn: initCRIService, }) } func initCRIService(ic *plugin.InitContext) (interface{}, error) { ready := ic.RegisterReadiness() ic.Meta.Platforms = []imagespec.Platform{platforms.DefaultSpec()} ic.Meta.Exports = map[string]string{&quot;CRIVersion&quot;: constants.CRIVersion} ctx := ic.Context pluginConfig := ic.Config.(*criconfig.PluginConfig) if err := criconfig.ValidatePluginConfig(ctx, pluginConfig); err != nil { return nil, fmt.Errorf(&quot;invalid plugin config: %w&quot;, err) } c := criconfig.Config{ PluginConfig: *pluginConfig, ContainerdRootDir: filepath.Dir(ic.Root), ContainerdEndpoint: ic.Address, RootDir: ic.Root, StateDir: ic.State, } log.G(ctx).Infof(&quot;Start cri plugin with config %+v&quot;, c) if err := setGLogLevel(); err != nil { return nil, fmt.Errorf(&quot;failed to set glog level: %w&quot;, err) } log.G(ctx).Info(&quot;Connect containerd service&quot;) client, err := containerd.New( &quot;&quot;, containerd.WithDefaultNamespace(constants.K8sContainerdNamespace), containerd.WithDefaultPlatform(platforms.Default()), containerd.WithInMemoryServices(ic), ) if err != nil { return nil, fmt.Errorf(&quot;failed to create containerd client: %w&quot;, err) } var s server.CRIService if os.Getenv(&quot;DISABLE_CRI_SANDBOXES&quot;) == &quot;&quot; { log.G(ctx).Info(&quot;using CRI Sandbox server - use DISABLE_CRI_SANDBOXES=1 to fallback to legacy CRI&quot;) s, err = sbserver.NewCRIService(c, client, getNRIAPI(ic)) } else { log.G(ctx).Info(&quot;using legacy CRI server&quot;) s, err = server.NewCRIService(c, client, getNRIAPI(ic)) } if err != nil { return nil, fmt.Errorf(&quot;failed to create CRI service: %w&quot;, err) } go func() { if err := s.Run(ready); err != nil { log.G(ctx).WithError(err).Fatal(&quot;Failed to run CRI service&quot;) } // TODO(random-liu): Whether and how we can stop containerd. }() return s, nil } .... 我们看到包的init方法，把cri插件注册到插件系统，初始化方法是initCRIService。 在应用入口函数command/main.go.App()的方法中创建了Server实例，Server的构造方法中会加载注册了插件，并调用对应的初始化函数，对插件进行初始化。 github.com/containerd/containerd/pkg/cri.initCRIService (/home/parallels/Documents/src/containerd/pkg/cri/cri.go:57) github.com/containerd/containerd/plugin.(*Registration).Init (/home/parallels/Documents/src/containerd/plugin/plugin.go:121) github.com/containerd/containerd/services/server.New (/home/parallels/Documents/src/containerd/services/server/server.go:227) github.com/containerd/containerd/cmd/containerd/command.App.func1.1 (/home/parallels/Documents/src/containerd/cmd/containerd/command/main.go:194) runtime.goexit (/snap/go/10351/src/runtime/asm_arm64.s:1197) cri插件的grcp服务端在这里已经正常起到起来了。 kubelet 和cri 插件的交互 在kubelet对Pod进行同步的时候，会调用func (m *kubeGenericRuntimeManager) SyncPod方法，同步POD的期望状态到容器运行时，流程如下图： sandbox创建 我们看到创建SandBox的处理函数是： func (c *criService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (_ *runtime.RunPodSandboxResponse, retErr error) 这个函数做了一下几件事情把pod sandbox 创建起来 ① 拉取sandbox的镜像，在containerd中配置 ② 获取创建pod要使用的runtime，可以在创建pod的yaml中指定，如果没指定使用containerd中默认的 ③ 如果pod不是hostNetwork那么添加创建新net namespace，并使用cni插件设置网络 ④ 调用containerd客户端创建一个container ⑤ 在rootDir/io.containerd.grpc.v1.cri/sandboxes下为当前pod以pod Id为名创建一个目录 （pkg/cri/cri.go） ⑥ 根据选择的runtime为sandbox容器创建task ⑦ 启动sandbox容器的task，将sandbox添加到数据库中 调用堆栈如下： github.com/containerd/containerd/api/runtime/task/v2.(*taskClient).Create (/home/parallels/Documents/src/containerd/api/runtime/task/v2/shim_ttrpc.pb.go:177) github.com/containerd/containerd/runtime/v2.(*shimTask).Create (/home/parallels/Documents/src/containerd/runtime/v2/shim.go:523) github.com/containerd/containerd/runtime/v2.(*TaskManager).Create (/home/parallels/Documents/src/containerd/runtime/v2/manager.go:420) github.com/containerd/containerd/services/tasks.(*local).Create (/home/parallels/Documents/src/containerd/services/tasks/local.go:218) github.com/containerd/containerd.(*container).NewTask (/home/parallels/Documents/src/containerd/container.go:303) github.com/containerd/containerd/pkg/cri/sbserver/podsandbox.(*Controller).Start (/home/parallels/Documents/src/containerd/pkg/cri/sbserver/podsandbox/sandbox_run.go:227) github.com/containerd/containerd/pkg/cri/sbserver.(*criService).RunPodSandbox (/home/parallels/Documents/src/containerd/pkg/cri/sbserver/sandbox_run.go:255) 这里说一下主要的抽象： type Container interface { // ID identifies the container ID() string // Info returns the underlying container record type Info(context.Context, ...InfoOpts) (containers.Container, error) // Delete removes the container Delete(context.Context, ...DeleteOpts) error // NewTask creates a new task based on the container metadata NewTask(context.Context, cio.Creator, ...NewTaskOpts) (Task, error) // Spec returns the OCI runtime specification Spec(context.Context) (*oci.Spec, error) // Task returns the current task for the container // the output from the task's fifos Task(context.Context, cio.Attach) (Task, error) // Image returns the image that the container is based on Image(context.Context) (Image, error) // Labels returns the labels set on the container Labels(context.Context) (map[string]string, error) // SetLabels sets the provided labels for the container and returns the final label set SetLabels(context.Context, map[string]string) (map[string]string, error) // Extensions returns the extensions set on the container Extensions(context.Context) (map[string]typeurl.Any, error) // Update a container Update(context.Context, ...UpdateContainerOpts) error // Checkpoint creates a checkpoint image of the current container Checkpoint(context.Context, string, ...CheckpointOpts) (Image, error) } type Task interface { Process // Pause suspends the execution of the task Pause(context.Context) error // Resume the execution of the task Resume(context.Context) error // Exec creates a new process inside the task Exec(context.Context, string, *specs.Process, cio.Creator) (Process, error) // Pids returns a list of system specific process ids inside the task Pids(context.Context) ([]ProcessInfo, error) // Checkpoint serializes the runtime and memory information of a task into an // OCI Index that can be pushed and pulled from a remote resource. // // Additional software like CRIU maybe required to checkpoint and restore tasks // NOTE: Checkpoint supports to dump task information to a directory, in this way, // an empty OCI Index will be returned. Checkpoint(context.Context, ...CheckpointTaskOpts) (Image, error) // Update modifies executing tasks with updated settings Update(context.Context, ...UpdateTaskOpts) error // LoadProcess loads a previously created exec'd process LoadProcess(context.Context, string, cio.Attach) (Process, error) // Metrics returns task metrics for runtime specific metrics // The metric types are generic to containerd and change depending on the runtime // For the built in Linux runtime, github.com/containerd/cgroups.Metrics // are returned in protobuf format Metrics(context.Context) (*types.Metric, error) // Spec returns the current OCI specification for the task Spec(context.Context) (*oci.Spec, error) } // PlatformRuntime is responsible for the creation and management of // tasks and processes for a platform. type PlatformRuntime interface { // ID of the runtime ID() string // Create creates a task with the provided id and options. Create(ctx context.Context, taskID string, opts CreateOpts) (Task, error) // Get returns a task. Get(ctx context.Context, taskID string) (Task, error) // Tasks returns all the current tasks for the runtime. // Any container runs at most one task at a time. Tasks(ctx context.Context, all bool) ([]Task, error) // Delete remove a task. Delete(ctx context.Context, taskID string) (*Exit, error) } Task 是容器的运行时对象，表示一个应用程序在容器中的运行时实例，所以可以看到在Container.Start的方法中会创建对应的Task,具体创建是调用实例了PlatformRuntime 的TaskManager来执行，PlatformRuntime是管理任务和进程的一个抽象，TaskManager.Create 调用ShimManager创建containerd-shim-runc-v2进程，把创建container的任务提交给containerd-shim-runc-v2。 containerd-shim-runc-v2以start子命令启动，创建另外一个shim子进程，然后返回这个子进程的ttrpc的监听地址，这样以start启动的进程退出后，containerd和shim进程也就脱离的关系，双方通过ttrpc相互通信，进程的状态不会互相影响。 TaskManager.Create创建好shim进程并且获取shim监听链接后，会创建shimTask，shimTask利用上面获取的shim链接，发送对应的task创建请求给shim的 task grpc服务 func (m *TaskManager) Create(ctx context.Context, taskID string, opts runtime.CreateOpts) (runtime.Task, error) { shim, err := m.manager.Start(ctx, taskID, opts) if err != nil { return nil, fmt.Errorf(&quot;failed to start shim: %w&quot;, err) } // Cast to shim task and call task service to create a new container task instance. // This will not be required once shim service / client implemented. shimTask, err := newShimTask(shim) if err != nil { return nil, err } t, err := shimTask.Create(ctx, opts) if err != nil { // NOTE: ctx contains required namespace information. m.manager.shims.Delete(ctx, taskID) dctx, cancel := timeout.WithContext(cleanup.Background(ctx), cleanupTimeout) defer cancel() sandboxed := opts.SandboxID != &quot;&quot; _, errShim := shimTask.delete(dctx, sandboxed, func(context.Context, string) {}) if errShim != nil { if errdefs.IsDeadlineExceeded(errShim) { dctx, cancel = timeout.WithContext(cleanup.Background(ctx), cleanupTimeout) defer cancel() } shimTask.Shutdown(dctx) shimTask.Close() } return nil, fmt.Errorf(&quot;failed to create shim task: %w&quot;, err) } return t, nil } 对应的TaskServer实现在：runtime/v2/runc/task/service.go func (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) { s.mu.Lock() defer s.mu.Unlock() handleStarted, cleanup := s.preStart(nil) defer cleanup() container, err := runc.NewContainer(ctx, s.platform, r) if err != nil { return nil, err } s.containers[r.ID] = container s.send(&amp;eventstypes.TaskCreate{ ContainerID: r.ID, Bundle: r.Bundle, Rootfs: r.Rootfs, IO: &amp;eventstypes.TaskIO{ Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, }, Checkpoint: r.Checkpoint, Pid: uint32(container.Pid()), }) // The following line cannot return an error as the only state in which that // could happen would also cause the container.Pid() call above to // nil-deference panic. proc, _ := container.Process(&quot;&quot;) handleStarted(container, proc) return &amp;taskAPI.CreateTaskResponse{ Pid: uint32(container.Pid()), }, nil } 调用OCI(runtime)去创建container，这样sandpox对应的容器就被创建起来了。 container的创建和运行 可以看到kubelet创建container调用的是 func (c *criService) CreateContainer(ctx context.Context, r *runtime.CreateContainerRequest) (_ *runtime.CreateContainerResponse, retErr error) 接口，可以看到这里根据传进来的sandbox ID，查询到对一个的sandBox信息，根据穿建立的其他参数信息构造出container的原信息，存储到containerStore service，这里并没有创建和运行task任务 container的运行是在kubelet调用 func (c *criService) StartContainer(ctx context.Context, r *runtime.StartContainerRequest) (retRes *runtime.StartContainerResponse, retErr error) StartContainer 根据传进来的container id 获取到对应的container 元信息，和对应的sandbox 信息，然后创建task任务并start,实质性的运行container的任务，这里和上面创建sandbox container 大致类似了。 ","link":"https://nomagic.cc/post/containerd-yuan-ma-jie-xi/"},{"title":"容器底层原理","content":"容器技术的背景 虚拟机（硬件虚拟化）在满足大规模服务部署的要求方面存在一些限制，资源占用，维护配置成本，快速扩缩容。 需要一种更轻量的虚拟化技术（操作系统虚拟化），由操作系统创建虚拟的系统环境，使应用感知不到其他应用的存在，仿佛在独自占有全部的系统资源，从而实现应用隔离的目的。 2000年，FreeBSD 4.0推出了Jail。Jail加强和改进了用于文件系统隔离的chroot环境。 2004年，Sun公司发布了Solaris 10的Containers，包括Zones和Resource management两部分。Zones实现了命名空间隔离和安全访问控制，Resource management实现了资源分配控制。 2007年，Control Groups(简称cgroups)进入Linux内核，可以限定和隔离一组进程所使用的资源(包括CPU、内存、I/O和网络等)。 2013年，Docker公司发布Docker开源项目，提供了一系列简便的工具链来使用容器，这个时候容器技术开始大爆发。 ##容器技术的实质是什么？ 容器技术是一种沙盒机制（虚拟机同样也属于一种沙盒机制），通过把应用装入一个隔离的环境中，使沙盒内的应用和外界隔离起来，达到互补影响的地步。 这里的沙盒可以形象的看做一个集装箱，把应用很多完全隔离开的集装箱内，联想Docker的图标 docker是如何创建这种隔离环境的 从上面的历史我们也可以看到，docker在容器执行这块，是对Linux提供的隔离机制进行了封装，提供出了更简单的使用工具。那这些工具封装的底层机制又是什么的，从namespace，cgroup,rootfs说开去。 namesapce 在 Linux 中，每个进程都拥有自己的命名空间，这些命名空间将进程及其相关的资源（如文件系统、网络等）隔离开来，使得不同进程可以运行在不同的环境中，互不影响。 分类 系统调用参数 相关内核版本 类别 标识符 系统版本 Mount namespaces CLONE_NEWNS Linux 2.4.19 UTS namespaces CLONE_NEWUTS Linux 2.6.19 IPC namespaces CLONE_NEWIPC Linux 2.6.19 PID namespaces CLONE_NEWPID Linux 2.6.24 Network namespaces CLONE_NEWNET 始于Linux 2.6.24 完成于 Linux 2.6.29 User namespaces CLONE_NEWUSER 始于 Linux 2.6.23 完成于 Linux 3.8) 隔离例子： #define _GNU_SOURCE #include &lt;sys/types.h&gt; #include &lt;sys/wait.h&gt; #include &lt;sys/mount.h&gt; #include &lt;stdio.h&gt; #include &lt;sched.h&gt; #include &lt;signal.h&gt; #include &lt;unistd.h&gt; #include &lt;stdlib.h&gt; #include &lt;errno.h&gt; #define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = { &quot;/bin/bash&quot;, NULL }; int container_main(void* arg) { char name[50]; sprintf(name,&quot;container_%d_&gt;&gt;&gt;&quot;,getpid()); printf(&quot;%s - inside the container !\\n&quot;,name); printf(&quot;%s - set hostname(%d) to container !\\n&quot;,name,sethostname(&quot;container&quot;,10)); ## mount(&quot;none&quot;, &quot;/proc&quot;, &quot;proc&quot;, 0, &quot;&quot;); ## mount(&quot;none&quot;, &quot;/tmp&quot;, &quot;tmpfs&quot;, 0, &quot;&quot;); execv(container_args[0], container_args); printf(&quot;Something's wrong!\\n&quot;); return 0; } int main() { printf(&quot;Host - start a container!\\n&quot;); int container_pid = clone(container_main, container_stack+STACK_SIZE, SIGCHLD|CLONE_NEWUTS|CLONE_NEWPID|CLONE_NEWNS, NULL); if (container_pid == -1 ) { perror(&quot;start a container failed\\n&quot;); return 1; } printf(&quot;child pid:%d\\n&quot;,container_pid); int status; waitpid(container_pid,&amp;status, 0); printf(&quot;Host - container stopped! %d\\n&quot;,status); return 0; } 编译完，执行可以看到输 Host - start a container! child pid:76 container_1_&gt;&gt;&gt; - inside the container ! container_1_&gt;&gt;&gt; - set hostname(0) to container ! 可以看到输出的pid 已经是1。我们知道Linux的1号进程是init进程，和我们这里看到的完全不同，所以我们的进程已经处于一个沙盒环境 这个时候我们查看/tmp目录下文件，查看到的还是父进程环境下的目录，我们把 ## mount(&quot;none&quot;, &quot;/tmp&quot;, &quot;tmpfs&quot;, 0, &quot;&quot;); 的注释去掉，编译重新执行，这是/tmp目录已经是空的，可以看到/tmp 路径已经在重新挂载。 mount(&quot;none&quot;, &quot;/proc&quot;, &quot;proc&quot;, 0, &quot;&quot;); 需要挂载这个目录是因为 top 命令还有系统的一些其他命令读取的/proc文件，如果不重新挂载读取到的还是父进程共享来的。 pid namespace 的实现原理 总结 通过namespace机制，我们可以看到进程所有看到世界，被完全隔离开了。相当于整个系统中只有自己一个进程. cgroup cgroup 完成了进程对硬件资源的使用隔离。 通过cgroup，可以限制特定进程组对CPU、内存、磁盘I/O等资源的访问。这可以帮助确保各个进程组之间不会相互干扰，同时也可以对各个进程组的资源使用进行精细控制。 cgroup的子系统可以分为以下几类： 块设备输入/输出限制（blkio）：这个子系统为块设备设定输入/输出限制，比如物理设备（磁盘，固态硬盘，USB等等）。 CPU时间（cpu）：这个子系统用于限制进程组使用的CPU时间。它不仅可以限制进程组使用的总CPU时间，还可以限制特定CPU核的使用时间。 内存（memory）：这个子系统用于管理进程组的内存使用。例如，可以限制进程组使用的总内存量，或者限制某个进程组在特定时间段的内存使用量。 网络（net）：这个子系统用于管理进程组的网络使用。例如，可以限制进程组发送和接收的网络流量。 进程组（cpuset）：这个子系统用于管理进程组的CPU和内存分配。它可以确保特定进程组不会干扰其他进程组的资源使用，也可以分配特定CPU和内存给特定的进程组。 cgroup 目前的控制端在虚拟文件系统 /sys/fs/cgroup/ 创建控制组的方式就是创建对应的文件夹就好。 # mkdir mycontainer //删除时使用（ rmdir mycontainer） # cd mycontainer/ # ls cgroup.controllers cgroup.kill cgroup.procs cgroup.threads cpu.stat memory.events memory.low memory.numa_stat memory.stat memory.swap.high pids.events cgroup.events cgroup.max.depth cgroup.stat cgroup.type io.pressure memory.events.local memory.max memory.oom.group memory.swap.current memory.swap.max pids.max cgroup.freeze cgroup.max.descendants cgroup.subtree_control cpu.pressure memory.current memory.high memory.min memory.pressure memory.swap.events pids.current 可以看到目录下已经生成很多文件，只要把控制的数据写入对应的文件，然后把进程ID写入分组任务文件，就可以限制进程的资源使用。 我们这里关注CPU的使用限制，主要是三个方面： cpu.cfs_period_us: 用来设置一个CFS调度时间周期长度，默认值是100000us(100ms)，一般cpu.cfs_period_us作为系统默认值我们不会去修改它。 cpu.cfs_quota_us: 用来设置在一个CFS调度时间周期(cfs_period_us)内，允许此控制组执行的时间。默认值为-1表示限制时间。 cpu.shares: 用来设置cpu cgroup子系统对于控制组之间的cpu分配比例。默认值是1000。 使用cfs_quota_us/cfs_period_us，例如20000us/100000us=0.2，表示允许这个控制组使用的CPU最大是0.2个CPU，即限制使用20%CPU。 如果cfs_quota_us/cfs_period_us=2，就表示允许控制组使用的CPU资源配置是2个。 对于cpu分配比例的使用，例如有两个cpu控制组foo和bar，foo的cpu.shares是1024，bar的cpu.shares是3072，它们的比例就是1:3。 在一台8个CPU的主机上，如果foo和bar设置的都是需要4个CPU的话(cfs_quota_us/cfs_period_us=4)，根据它们的CPU分配比例，控制组foo得到的是2个，而bar控制组得到的是6个。需要注意cpu.shares是在多个cpu控制组之间的分配比例，且只有到整个主机的所有CPU都打满时才会起作用。 也就是说cpu.cfs_quota_us/cpu.cfs_period_us决定cpu控制组中所有进程所能使用CPU资源的最大值，而cpu.shares决定了cpu控制组间可用CPU的相对比例，这个比例只有当主机上的CPU完全被打满时才会起作用。 用下面的例子来测试CPU的限制： int main(){ int i=0; for(;;){ i++; if (i&gt;1000000000000) break; } return 0; } 如果不做限制，那么这个程序会把cpu跑满，我们在/sys/fs/cgroup/ 下建立自己的控制组， 那么我们把这个程序的cpu限制在0.2，只要把进程ID写入task文件，cfs_quota_us =20000 cfs_period_us = 100000 就好。 cgroup v2版本的控制 参考 echo &quot;+cpu&quot; &gt;&gt; /sys/fs/cgroup/cgroup.subtree_control echo &quot;+cpuset&quot; &gt;&gt; /sys/fs/cgroup/cgroup.subtree_control echo &quot;+cpu&quot; &gt;&gt; /sys/fs/cgroup/mycontainer/cgroup.subtree_control echo &quot;+cpuset&quot; &gt;&gt; /sys/fs/cgroup/mycontainer/cgroup.subtree_control mkdir tasks cd tasks/ echo &quot;1&quot; &gt; /sys/fs/cgroup/mycontainer/tasks/cpuset.cpus echo &quot;20000 100000&quot; &gt; /sys/fs/cgroup/mycontainer/tasks/cpu.max echo $pid &gt;&gt; /sys/fs/cgroup/mycontainer/tasks/cgroup.procs 可以看到cpu的使用量被限制到了20% docker 解决了容器打包问题 kubernets 解决了容器编排和调度的问题 容器本身没有价值，有价值的是“容器编排” 总结 通过 cgroup 限制了容器资源使用。不同的容器进程的资源限额起到了隔离作用。 rootfs 我们知道进程的执行，还依赖于他存在的本地环境，也就是进程所依赖的文件系统中的所有文件。 从上面的namespace介绍来看，我们只要把容器进程的根目录/重新挂载，那么容器所依赖的文件系统也和其他进程完全隔离起来，相当于沙盒这个集装箱的底部也被封装起来。这个根目录所挂载的文件系统就是rootfs，它包含了操作系统所需的基本文件和目录结构，如/bin、/usr、/etc等。 （注意文件系统和内核的区别） 这个被挂载到/目录的rootfs就说我们所说的“容器镜像”，这个容器镜像就保证了容器进程所依赖的文件系统的一致性，可以使这个容器集装箱，随意的在不同的机器上搬来搬去（调度） 那么容器镜像的维护还有一个问题，比如我创建了一个ubuntu的系统镜像，如果被人修改了这个ubuntu镜像。 那么这两个镜像就没有任何关系了。加入A、B、C。。。。也修改了镜像，那么即使这些镜像有99%的一致性，也是不同的镜像，这就导致整个镜像系统难以维护。 比如我下载了一个nginx镜像和一个Mongo镜像，即使他们底层依赖的系统镜像是一个，但是这个系统是下载和存储两个镜像，显然不是一种好的做法， Docker在实现镜像的时候，选择了一种Union File System的文件系统，也叫UnionFS，这个文件系统功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下，可以使镜像的维护变成一个增量的过程。 整个过程由fork 变成了reference,这个增量的部分就是docker 镜像中的层。 aufs AUFS（Advanced Multi-Layered Unification File System）可以把多个文件联合挂载到一个路径，常被用于Linux系统的容器（container）中。 #!/bin/bash create(){ mkdir -p work/a work/b work/c root touch work/a/a.txt work/b/b.txt work/c/c.txt sudo mount -t aufs -o dirs=./work/a=rw:./work/b=ro:./work/c=ro none ./root echo a &gt;&gt; work/a/a.txt echo b &gt;&gt; root/b.txt # touch work/a/.wh.c.txt } delete(){ umount root rm -rf work rm -rf root } case $1 in create) create ;; delete) delete ;; esac AUFS的读写层是AUFS文件系统中的一部分，可读可写，位于最上层。在AUFS中，所有对文件的读写操作都在读写层完成。当一个文件被访问时，AUFS会根据文件路径从最上层开始逐层查找，直到找到该文件。如果文件位于最上层，则直接打开并进行读写操作；如果文件位于下层，则AUFS会先复制到最上层，然后再打开进行读写操作。这种复制操作也被称为“写复制”（CoW），它是AUFS的默认支持的技术。 AUFS的另一个重要特点是它支持“whiteout”（写隐藏）。当删除一个低层branch文件时，AUFS不会直接删除该文件，而是在顶层branch对该文件创建一个whiteout文件（.wh.&lt;origin_file_name&gt;），将该文件隐藏起来。这种方式实际上只是将文件的访问权限删除，而不会真正地删除文件数据，实现了对文件的软删除。 touch work/a/.wh.c.txt 总结： 通过联合文件系统（目前Docker使用是overlay文件系统）构建成容器镜像，给容器的文件系统提供了一致性，使容器的打包和调度更加简单。 参考： [DOCKER基础技术]https://coolshell.cn/articles/17010 深入剖析 Kubernetes(张磊) ","link":"https://nomagic.cc/post/rong-qi-di-ceng-yuan-li/"},{"title":"Kubernetes的一些优秀的设计理念","content":" 声明式API： Kubernetes的API是声明式的，这意味着用户只需通过API对象来描述他们希望得到的服务或应用，而不需要关注实现细节。这种设计使得Kubernetes具有更高的可编程性和可定制性，方便用户使用和扩展。 这个概念也可以参考函数式编程 面向对象的设计： API对象是彼此互补且可组合的，这使得API对象能够实现面向对象设计时的要求，即“高内聚、松耦合”。这种设计可以提高代码的可重用性和可维护性。 另外 Kubernetes 资源注册表实现，内核资源对象和外部资源对象，不同对象版本之间的转换 也很有参考价值 高层API与低层API的分离： API设计分为高层API和低层API两个层次。高层API以操作意图为基础设计，主要关注业务逻辑；而低层API则根据高层API的控制需求进行设计，主要关注技术实现。这种分离的设计可以降低系统的复杂性和耦合性，提高系统的可扩展性和可维护性。 分布式代理服务器： Kubernetes集群中的每个节点都运行一个分布式代理服务器，这种设计可以提高系统的伸缩性和可用性。例如，当需要访问服务的节点增多时，提供负载均衡能力的Kube-proxy也会相应增多，从而增加高可用节点的数量。 自我修复和自我扩展： Kubernetes具有自我修复和自我扩展的能力，这使得它可以自动化处理节点故障、应用崩溃等问题。同时，Kubernetes还支持自动扩展集群，使得用户可以根据需要自动增加或减少节点，以满足业务需求。 模块化设计： Kubernetes的组件和核心功能都被设计成模块化的，这使得Kubernetes具有很高的灵活性和可定制性。用户可以根据自己的需求选择和定制不同的模块，以满足特定的应用场景需求。 可观察性： Kubernetes重视可观察性设计，它通过内置的日志记录和监控工具，帮助用户实时了解集群中运行的应用状态和健康状况。这种设计可以提高系统的可靠性和稳定性。几乎每一个模块都有状态和健康监控的接口。 扩展性设计： Kubernetes的控制机制保证了分布式系统下容器运行的稳定性和高可靠性，同时Kubernetes平台开放了容器运行时接口（CRI）、容器网络接口（CNI）和容器存储接口（CSI），可以方便地进行二次开发和定制。 也是的每个模块更好的独立发展，Kubernetes 项目本身更加容易维护，CRI/CNI/CSI 可以各自独立迭代（这也是微服务思想的一种实践吧） 微服务架构： Kubernetes与微服务架构的设计理念相结合，方便微服务架构的落地，和Istio这类服务网格结合，进一步把微服务的架构从业务逻辑中抽离出来，是微服务架构成为基础设施。 ","link":"https://nomagic.cc/post/kubernetes-de-yi-xie-you-xiu-de-she-ji-li-nian/"},{"title":"iptables 详解","content":"实现原理参考： 内核实现 iptables 只是用户空间编写和传递规则的工具，真正起作用的事netfilter 表之间的优先级 iptables 表的优先级设定是为了解决不同的表之间可能存在的冲突和重叠问题。 iptables 包含了一系列的数据处理模块，它们被组织在不同的表中（例如 filter 表、nat 表、mangle 表等）。每个表都有自己的功能和用途，例如 filter 表用于过滤数据包，nat 表用于进行网络地址转换等。 然而，在某些情况下，不同的表可能会对同一个数据包进行不同的处理。例如，一个数据包可能先经过 filter 表被过滤掉，又被 nat 表进行了地址转换，那么这两个表之间的处理顺序就可能产生冲突。 为了解决这种问题，iptables 引入了表之间的优先级设定。通过为表设置优先级，可以确定处理数据包的顺序。较低的优先级意味着表的规则会在较高的优先级表之前被执行。例如，可以将 nat 表的优先级设置为比 filter 表高，这样 nat 表的规则就会在 filter 表之前被执行，是一种截断行为，也就是说nat表的规则执行后，filter表就不会被执行了，从而避免了可能的冲突。 此外，对于同一优先级的表，它们的执行顺序通常是按照定义的顺序进行的。也就是说，在定义 iptables 规则时，先定义的表通常会先被执行。 优先级是靠链实现的。 四张表的作用 filter 表：这是默认表，如果没有指定表名，就默认使用 filter 表。它负责过滤掉某些数据包，确定是否放行该数据包(过滤)。在四个表中，filter表的应用相对较少。 nat 表：网络地址转换表，用于进行网络地址转换，例如进行端口映射等操作。 mangle 表：拦截表，可以修改数据包的 TTL、TOS 等字段，用于实现一些高级功能。这个表的应用相对较少。 raw 表：原始表，主要用于设定豁免某些数据包经过 iptables 的过滤机制。 表与链的关系 根据每张表的用途，决定了和链的关系： filter表：用于过滤数据包，实施访问控制和防火墙策略。包括INPUT、FORWARD和OUTPUT三个主要链。 nat表：用于进行网络地址转换（NAT）和端口转发等操作。包括PREROUTING、POSTROUTING和OUTPUT三个主要链。 mangle表：用于修改数据包的特定字段。包括PREROUTING、INPUT、FORWARD、OUTPUT和POSTROUTING五个主要链。 raw表：用于在数据包进入连接跟踪之前进行处理。包括PREROUTING、OUTPUT两个主要链。 每个表中都包含了多个预定义的链，这些链的作用不同，可以根据需要添加规则。例如，filter表中的INPUT链用于处理目标地址为本地主机的数据包，FORWARD链用于处理经过本机转发的数据包，OUTPUT链用于处理源地址为本地主机的数据包。 此外，用户还可以创建自定义的表和链，以满足特定的需求。这些自定义的表和链可以根据具体应用场景来添加规则。 理解起来：表用于组织规则，链用于按顺序应用规则 比如为什么filter表没有PREROUTING链： filter表没有PREROUTING链是因为PREROUTING链主要用于对数据包进行网络地址转换（NAT）或端口转发等操作，而这些操作与filter表的目标不同。 filter表的主要目标是过滤数据包，实施访问控制和防火墙策略。因此，filter表中的链主要用于处理目标地址为本地主机的数据包（INPUT链）、经过本机转发的数据包（FORWARD链）和源地址为本地主机的数据包（OUTPUT链）。 而PREROUTING链主要用于在数据包进入网络协议栈之前进行处理，通常用于进行网络地址转换（NAT）或端口转发等操作。这种操作需要更早地处理数据包，以确保数据包在进入协议栈后进行正确的转发或转换。因此，PREROUTING链不适合在filter表中，而是在nat表中。 通过将不同的操作和处理过程分配给不同的表和链，可以更好地组织和管理规则，并使iptables更灵活和可扩展。这种设计使得iptables可以根据需要进行数据包过滤、地址转换、端口转发等不同类型的处理，以实现网络安全和网络功能的需求。 自定义链 参考 常用模块 multiport: 多端口匹配 可用于匹配非连续或连续端口；最多指定15个端口； iptables -A INPUT -p tcp -m multiport --dport 22,80 -j ACCEPT iptables -A OUTPUT -p tcp -m multiport --sport 22,80 -j ACCEPT iprange: 匹配指定范围内的地址 匹配一段连续的地址而非整个网络时有用 iptables -A INPUT -p tcp -m iprange --src-range 192.168.118.0-192.168.118.60 --dport 22 -j ACCEPT iptables -A OUTPUT -p tcp -m iprange --dst-range 192.168.118.0-192.168.118.60 --sport 22 -j ACCEPT string: 字符串匹配，能够检测报文应用层中的字符串 字符匹配检查高效算法：kmp, bm 能够屏蔽非法字符 # 注意该条规则需要添加到OUTPUT链，当服务端返回数据报文检查到有关键字&quot;sex&quot;时，则丢弃该报文，可用于web敏感词过滤 iptables -A OUTPUT -p tcp --dport 80 -m string --algo kmp --string &quot;sex&quot; -j DROP connlimit: 连接数限制 对每IP所能够发起并发连接数做限制； # 默认INPUT 为 DROP. 每个ip对ssh服务的访问最大为3个并发连接，超过则丢弃 iptables -A INPUT -p tcp --dport 22 -m connlimit ! --connlimit-above 3 -j ACCEPT limit: 速率限制 limit-burst: 设置默认阀值 # 默认放行10个，当到达limit-burst阀值后，平均6秒放行1个 iptables -A INPUT -p icmp -m limit --limit 10/minute --limit-burst 10 -j ACCEPT state: 状态检查 连接追踪中的状态： NEW: 新建立一个会话 ESTABLISHED：已建立的连接 RELATED: 有关联关系的连接 INVALID: 无法识别的连接 # 放行ssh的首次连接状态 iptables -A INPUT -p tcp --dport 22 -m state --state NEW -j ACCEPT addrtype: =基于地址类型的匹配。 允许您根据地址类型（例如，是否是本地主机、本地网络、广播或组播地址）来匹配数据包 # 允许本地回环地址（127.0.0.1）的所有流量： iptables -A INPUT -m addrtype --src-type LOCAL_PREF_SRC -j ACCEPT # 允许来自同一网络的所有流量： iptables -A INPUT -m addrtype --src-type LOCAL_NET -j ACCEPT # 允许来自不同网络的所有流量： iptables -A INPUT -m addrtype --src-type UNicast -j ACCEPT comment 添加注释或标记的命令。 iptables -A &lt;chain&gt; -m comment --comment &quot;Your comment here&quot; ","link":"https://nomagic.cc/post/iptables-xiang-jie/"},{"title":"数据同步","content":"目前公司OLAP使用的数据库是clickhouse,业务数据库使用的是MySQL，在做数据分析的时候需要把MySQL中的数据实时同步到clickhouse中.就采用解析binlog,然后映射到clickhouse的语句执行动作回放。 遇到的主要的几个问题是： 1 MySQL业务表的分库分表和表结构异构的情况，这个时候需要统一同步到clickhouse中一张表中。 2 由于clickhouse本身的更新和删除属于重的动作，需要变更数据的整个分区，需要提高更新或删除的的效率，不然会出现数据同步延迟。 ","link":"https://nomagic.cc/post/mysql-shi-shi-tong-bu-clickhouse-she-ji/"},{"title":"kafka如何实现消息的可靠性？","content":"持久化存储 Kafka 使用可靠的持久性存储来保存所有的消息，这样即使在消息发送后，即使消费者暂时离线或无法立即处理消息，或者broker节点down掉，消息仍然可以保留在 磁盘中不会丢失，另外Kafka的消费者使用基于偏移量的提交方式，确保已提交的消息被可靠地消费。消费者在读取消息后，会将偏移量提交给Kafka，表示该消息已被消费。如果提交失败，消费者可以手动提交偏移量。这个offset 也是持久化了的 在Kafka 0.9版本之前，offset默认保存在Zookeeper中，而在Kafka 0.9版本及以后，offset默认保存在Kafka中的一个内置主题中。这个主题为__consumer_offsets，里面采用key-value的方式存储数据，其中key是group.id+topic+分区号，value就是当前offset的值。每隔一段时间，Kafka会对这个主题进行compact操作，即每个group.id+topic+分区号会保留最新数据。 另外至于本地存储的结构：稀疏索引+顺序日志文件 主要是为了写入和读取的高效，不在这里展开。 ACK机制 为了确认producer发送的消息，已经成功的被broker接收，在broker接收成功后会相应的回复一个ACK消息，这样producer才会继续发送下一轮消息，否则就会重新发送消息。但是这个ACK消息是什么时候返回的呢？ Kafka 的 ACK 机制有以下三种可选值： ack=0：Producer 发送消息后，不等待 Broker 返回确认信息，直接发送下一条消息。这种方式的吞吐量最高，但可靠性最低。 ack=1：Producer 发送消息后，等待 Leader Broker 返回确认信息，表示消息已被 Leader Broker 接收。这种方式的可靠性较高，但吞吐量较低。 ack=all：Producer 发送消息后，等待 Leader Broker 和所有 Follower Broker 都返回确认信息，表示消息已被所有 Broker 接收。这种方式的可靠性最高，但吞吐量最低。 Kafka 的 ACK 机制可以根据应用场景的需要进行选择。例如，对于实时数据处理场景，需要保证消息的可靠性，因此可以选择 ack=all 或 ack=1。对于日志采集场景，可以选择 ack=0，以提高吞吐量。 Kafka 的 ACK 机制的主要作用如下： 保证消息的可靠性：ACK 机制可以确保消息已被 Broker 接收，即使 Broker 发生故障，消息也不会丢失。 提高吞吐量：ACK 机制可以根据应用场景的需要，灵活调整，以提高吞吐量。 副本机制 我们根据kafka的存储架构可以知道，每一个topic partition都可以选择多副本机制，通过多副本机制，可以保证在leader副本挂掉的时候，其他副本可以选举成为leader提供服务。这本身是一个日志复制机制。 leader 副本根据什么决定返回ACK给producer的呢？ 这就不得不说Kafka的ISR机制 ISR 机制 Kafka的ISR（In-Sync Replicas）机制是Kafka为确保数据可靠性和一致性而设计的重要机制。ISR是一组副本，包括分区的领导者（Leader）和追随者（Follower）副本，这些副本与领导者保持数据同步。 每个分区都有一个领导者（Leader）和零个或多个追随者（Follower）。领导者负责处理客户端的写请求，而追随者主要用于数据复制。ISR集合是分区领导者的一组追随者副本，它们与领导者保持数据同步。只有在ISR集合中的追随者副本可以参与数据的写入和读取操作。 数据复制同步过程 数据复制过程中，领导者将消息写入其本地日志，并定期将这些消息发送给ISR集合中的追随者。追随者接收消息后，将其写入本地日志，以保持数据同步。ISR集合中的每个追随者都维护了领导者的日志信息，包括领导者的Leader Epoch和Log Start Offset。这些信息用于确保数据的正确复制和同步。只有当ISR集合中的所有追随者都成功复制了一条消息后，领导者才会将该消息标记为已提交，确保数据的一致性。 如果某个追随者发生故障或者追赶进度过慢，那么该追随者可能会被从ISR集合中移除。这有助于保持数据的可靠性和避免影响性能。 副本分区的管理 分区中的所有副本统称为AR（Assigned Replicas）。所有Leader副本加上和Leader副本保持同步的Follower副本组成ISR（In-Sync Replicas）。所有没有保持同步的Follower副本组成OSR（Out-of-Sync Replicas）。AR = ISR + OSR。正常情况下，所有Follower副本都应该和Leader副本一致，即AR=ISR。 如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。 follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR(Outof-Sync Replicas)列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR 容错机制 Broker节点的容错：当Kafka集群中的一个Broker节点宕机时，由于Broker节点在启动后会向Zookeeper进行注册并保存当前节点信息，因此可以通过Zookeeper来查看节点信息。此时可以发现Isr（In-Sync Replicas）中的内容和Replicas中的不一样了，因为Isr中显示的是目前正常运行的节点。 分区的容错：Kafka通过复制机制来保证分区的可靠性。每个分区都有一个领导者（Leader）和零个或多个追随者（Follower）。当领导者发生故障时，Kafka会从ISR（In-Sync Replicas）集合中选举一个新的领导者。ISR集合中包含了与领导者保持数据同步的追随者副本。只有当ISR集合中的所有追随者都成功复制了一条消息后，领导者才会将该消息标记为已提交，确保数据的一致性。 生产者和消费者的容错：Kafka通过ack机制来保证生产者发送消息的可靠性。生产者可以设置ack参数来指定消息被成功发送到目的地的保证级别。对于消费者，Kafka通过偏移量提交来确保已提交的消息被可靠地消费。消费者在读取消息后，会将偏移量提交给Kafka，表示该消息已被消费。如果提交失败，消费者可以手动提交偏移量。 自动均衡策略：Kafka的自动均衡策略允许在集群中动态地添加或删除Broker节点，以保持集群的负载均衡和高可用性。当新增一个Broker节点时，Zookeeper会自动识别并在适当的机会选择此节点提供服务。 ","link":"https://nomagic.cc/post/kafka-ru-he-shi-xian-xiao-xi-de-ke-kao-xing/"},{"title":"Clickhouse 入门使用篇","content":"简介 ClickHouse（Click Stream,Data Warehouse）是一个用于联机分析(OLAP)的列式数据库管理系统。 其独立于Hadoop大数据体系，最核心的特点是极致压缩率和极速查询性能。同时，ClickHouse支持SQL查询，且查询性能好，特别是基于大宽表的聚合分析查询性能非常优异，比其他分析型数据库速度快一个数量级 特性 列式存储 对于分析类查询，大部分事件只需要读取特定的列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。由于数据总是打包成批量读取的，所以压缩是非常容易的。由于数据被压缩，减少IO的读取量，也使跟多的数据更容易被系统缓存。[CPU 磁盘速度对比] 极致压缩 在磁盘空间和CPU消耗之间进行不同权衡的高效通用压缩编解码器之外，针对特定类型数据的专用编解码器。 DBMS功能： 几乎覆盖了标准 SQL 的大部分语法，包括 DDL 和 DML 多样化引擎 决定了数据的存放和读取方式，从而也就决定了IO效率。 高吞吐的写入能力（合理使用的情况下） 批量异步写入，按照数据分片的方式合并数据，列式存储所以是顺序的IO操作 多节点多核并行处理 单条SQL可以完全利用整机CPU[mysql MyISAM nor InnoDB 是单SQL单线程模式 ];数据分片，化整为零，从而利用多核的优势。 支持向量化处理 限制 1.不支持事务（大数据量的事务会降低系统吞吐）。 2.不支持直接的Update/Delete操作，需要使用alter来更新或删除。 3.只有主键没有索引，并且主键使用稀疏索引的结构实现 4.不支持高并发的操作，并发低，并行高。ClickHouse的目标是保证单条查询的相应足够快，单条SQL可以跑满一大半的CPU，多shard多parts的并行操作，一条SQL有可能把CPU、内存、磁盘IO拉到很高的利用度。 集群架构与部署设计 ClickHouse不同于Elasticsearch、HDFS这类主从架构的分布式系统，它采用无中心架构模式，集群中每个节点角色对等，客户端可以访问任意一个节点发起请求。 通过分片（Shard）来水平扩展集群，将总数据水平分成M份，每个分片保存其中的一份数据避开单节点的性能瓶颈。 通过副本（Replication）来保障集群的高可用，即每个分片拥有若干个数据一样的副本。 可以设置不同的分片配置不同数量的副本。即分片0可能有1个副本，分片1可能有3个副本。 实现层面通过分布式表和本地表来完成，分布式表不存储数据，本地表存储数据，分布式表把请求转发到对应的本地表;【可以理解成常规的分表，分布式表是一个分表的访问中间件，代理转发作用】 分片机制提供的是一种数据分割的逻辑视图，具体到物理层面的存储由副本来承载，副本是分片在物理方面的呈现，一个分片下可以有多个副本，由ReplicatedMergeTree复制表系列引擎实现同一分片下不同副本之间的数据同步，借助ZooKeeper实现数据的一致性。 集群部署和节点划分，完全通过xml配置文件以声明的方式来实现。 一个副本节点的配置例子和说明 &lt;clickhouse_remote_servers&gt; // 声明一个集群 集群名称是ck_cluster &lt;ck_cluster&gt; // 数据分片1 &lt;shard&gt; //此参数设置为«true»时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。 //若此参数设置为«false»（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样. &lt;internal_replication&gt;true&lt;/internal_replication&gt; //副本1 &lt;replica&gt; &lt;host&gt;ck_node1&lt;/host&gt; &lt;port&gt;9011&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;V0JusUK7b6pHTqRX&lt;/password&gt; &lt;/replica&gt; //副本2 &lt;replica&gt; &lt;host&gt;ck_node3&lt;/host&gt; &lt;port&gt;9011&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;V0JusUK7b6pHTqRX&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; // 数据分片2 &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;ck_node2&lt;/host&gt; &lt;port&gt;9011&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;V0JusUK7b6pHTqRX&lt;/password&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;ck_node4&lt;/host&gt; &lt;port&gt;9011&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;V0JusUK7b6pHTqRX&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ck_cluster&gt; &lt;/clickhouse_remote_servers&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;10.86.0.11&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;10.86.0.12&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;10.86.0.16&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt;//影响表在zookeeper中的路径，表明当前是哪个分片哪个副本即可 &lt;shard&gt;01&lt;/shard&gt; &lt;replica&gt;01-01&lt;/replica&gt; &lt;/macros&gt; &lt;settings&gt; &lt;insert_distributed_sync&gt;1&lt;/insert_distributed_sync&gt; &lt;/settings&gt; &lt;/yandex&gt; 和zookeeper的关系 存储元数据信息 执行分布式DDL （添加 on cluster 关键字） ClickHouse对库、表的管理都是在存储节点级别独立的，集群中各节点之间的库、表元数据信息没有一致性约束. ClickHouse集群的每个节点都会把收到的分布式执行DDL请求放入到一个公共Zookeeper任务队列中然后每个节点的后台线程会依次任务队列里的DDL（ /ClickHouse/task_queue/ddl），保证了所有分布式DDL的串行执行顺序,不会引起整个集群的节点不同节点的元数据不一致。 利用Zookeeper序列自增节点（Sequence Znodes）的特性来实现同步环境下的串行提交任务。每个节点下生成 status类型节点( active Znode用来管理当前有多少节点正在执行这个DDL,finished Znode 表示已经执行成功的节点，节点执行完成有active 迁移到finished )来维护每个节点的DDL执行状态，保证DDL在集群中每个节点执行成功。【同时也会导致单个节点有问题导致DDL Task 无法执行成功，然后堵塞掉整个集群的DDL任务执行】 由上面可以看出 这里zookeeper 主要时记录任务状态，数据量较小，不会构成zookeeper的压力点。 Replicate 主从同步 Merge/Mutation 操作引发同一个sharding之间的节点需要进行数据同步。每一个数据分片变更过程和Zookeeper交互的次数不下10次，如果一次Batch数据写入数据跨10个数据分区的话那就是100次数据交互（数据写入前应该进行预聚合，争取同一批次写入的数据的分片数要少），并且会提交很多的logEntry到对应表的同步队列下，节点数量增多，如果节点数量增加到很高的状态，会使得zookeeper占用更多的内存，引发本身的full gc ,这些节点又在短时间内无法合并清理不掉的，就会不停的gc,然后引发 zookeeper的session过期，任务超时等各种问题， 可以看出节点数据同步构成了clickhouse对zookeeper的主要压力。 表引擎 常用表引擎介绍 Kafka 为了能让 ClickHouse 消费 Kafka 数据，我们需要三张表：首先需要一张存数据的表也就是 MergeTree；然后需要一张 Kafka 表，它负责描述 Topic、消费数据和解析数据；最后需要一个物化视图去把两张表关联起来，它也描述了数据的流向 ENGINE = Kafka SETTINGS kafka_cluster = 'cluster' kafka_topic_list = 'topic-xxx' kafka_group_name = 'group-xxx' kafka_format = 'JSONEachRow'; CREATE MATERIALIZED VIEW consuter_mview to data_table AS SELECT * FROM consumer_t CREATE TABLE data_table (col coltype ....) ENGINE = MergeTree PARTITION BY ... ORDER BY ...; 当无法对客户端进行写入限流的时候（以防同一个时刻写入数据过多，导致待合并分片过多拖垮系统） 业务方可以把数据写写入kafka，然后有kafka引擎去消费topic，导入到库中。 MergeTree系列 MergeTree 主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有ClickHouse SQL语法，虽然有主键，但是主键是为了快速读取数据用的，不保证唯一性。 ClickHouse中最常用也是最基础的表引擎为MergeTree，在它的功能基础上添加特定功能就构成了MergeTree系列引擎【从下面的介绍可以看到附加的操作只有在分片合并的时候才会执行】 ReplacingMergeTree 用来解决当出现主键相同的记录时，保留哪一条数据的问题。ReplacingMergeTree就是在MergeTree的基础上加入了去重的功能，根据特定的算法，决定仅保留主键相同的哪一条数据。但它仅会在合并分区时，去删除重复的数据 可以收动optimize,大数据量情况下影响系统性能。 CollapsingMergeTree 用来折叠掉不再需要的记录 ClickHouse实现了CollapsingMergeTree来消除ReplacingMergeTree的限制。该引擎要求在建表语句中指定一个标记列Sign和特定的折叠算法来达到数据消除的效果 使用的限制： 需要对业务代码进行改造 写入时： 执行删除操作需要写入取消行，而取消行中需要包含与原始状态行一样的数据（Sign列除外）。所以在应用层需要记录原始状态行的值，或者在执行删除操作前先查询数据库获取原始状态行； 查询时： 由于后台Compaction时机无法预测，在发起查询时，状态行和取消行可能尚未被折叠；另外，ClickHouse无法保证primary key相同的行落在同一个节点上，不在同一节点上的数据无法折叠。因此在进行count(* )、sum(col)等聚合计算时，可能会存在数据冗余的情况。为了获得正确结果，业务层需要改写SQL，将count()、sum(col)分别改写为sum(Sign)、sum(col * Sign)。 数据状态维护成本高： CollapsingMergeTree虽然解决了主键相同的数据即时删除的问题，但是状态持续变化且多线程并行写入情况下，状态行与取消行位置可能乱序，导致无法正常折叠。 VersionedCollapsingMergeTree CollapsingMergeTree 并发环境下的版本 VersionedCollapsingMergeTree为了解决CollapsingMergeTree乱序写入情况下无法正常折叠问题，VersionedCollapsingMergeTree表引擎在建表语句中新增了一列Version，用于在乱序情况下记录状态行与取消行的对应关系。主键相同，且Version相同、Sign相反的行，在Compaction时会被删除。与CollapsingMergeTree类似， 为了获得正确结果，业务层需要改写SQL，将count()、sum(col)分别改写为sum(Sign)、sum(col * Sign)。 SummingMergeTree ClickHouse通过SummingMergeTree来支持对主键列进行预先聚合。在后台合并数据分片时，会将主键相同的多行进行sum求和，然后使用一行数据取而代之，从而大幅度降低存储空间占用，提升聚合计算性能。 值得注意的是：ClickHouse只在分片合并时才会进行数据的预先聚合，而执行时机无法预测，所以可能存在部分数据已经被预先聚合、部分数据尚未被聚合的情况。因此，在执行聚合计算时，SQL中仍需要使用GROUP BY子句。在预先聚合时，ClickHouse会对主键列之外的其他所有列进行预聚合。如果这些列是可聚合的（比如数值类型），则直接sum；如果不可聚合（比如String类型），则随机选择一个值。通常建议将SummingMergeTree与MergeTree配合使用，使用MergeTree来存储具体明细，使用SummingMergeTree来存储预先聚合的结果加速查询。 AggregatingMergeTree AggregatingMergeTree也是预先聚合引擎的一种，用于提升聚合计算的性能。与SummingMergeTree的区别在于：SummingMergeTree对非主键列进行sum聚合，而AggregatingMergeTree则可以指定各种聚合函数。AggregatingMergeTree的语法比较复杂，需要结合物化视图或ClickHouse的特殊数据类型AggregateFunction一起使用。在insert和select时，也有独特的写法和要求：写入时需要使用-State语法，查询时使用-Merge语法。 物化视图 主要针对于OLAP场景，为业务方提供稳定高效的查询服务。在业务场景下，实时事件流上报可能会在不同的日志，以不同的格式、途径写入到clickhouse。在之前的使用中，通过查询多个日志表join实现多个指标的整合。用传统JOIN方式，我们遇到如下困难: 1.每个查询会有非常长的代码，有的甚至1500行、2000行sql，使用和理解上特别痛苦; 2.性能上无法满足业务诉求，日志量大会爆内存不足; 如何将这些数据进行整合，以ClickHouse宽表的方式呈现给上层使用，用户可以在一张表中查到所需的所有指标，避免提供多表带来的代码复杂度和性能开销问题。 主要有3个方面： 物化视图的创建、新增维度和指标，聚合函数的使用 物化视图结合字典的使用； 通过物化视图组合指标宽表 常见问题 Too many parts(xxx). Merges are processing significantly slower than inserts** 插入语句导致本地有太多的分片目录需要合并，每一条插入语句都会根据插入语句生成对应本次写入分片目录，如果文件目录生成过快，merge跟不上就会报这个错误。 检查插入语句的频次是否过高，是否是批次批次的方式写入，并且每一个批次影响的分片数应该足够小 Memory limit (for query) exceeded:would use 9.37 GiB (attempt to allocate chunk of 301989888 bytes), maximum: 9.31 GiB** 内存使用(max_memory_usage)使用的实现，查询被强制KILL 1）group by, order by , count distinct，join这样的复杂的SQL，查询的空间需要使用大量的内存。 如果是group by内存不够，推荐配置上max_bytes_before_external_group_by参数，当使用内存到达该阈值，进行磁盘group by 如果是order by内存不够，推荐配置上max_bytes_before_external_sort参数，当使用内存到达该阈值，进行磁盘order by如果是count distinct内存不够，推荐使用一些预估函数(如果业务场景允许)，这样不仅可以减少内存的使用同时还会提示查询速度 对于JOIN场景，我们需要注意的是clickhouse在进行JOIN的时候都是将&quot;右表&quot;进行多节点的传输的(右表广播) 当前查询使用内存不高的情况下，确认是否是分区合并占用掉了内存，查看system.merges/system.muations确认情况。 table readonly 1, zk 的响应超时，table 可能变成只读模式，整体的zk的压力或者当前表下znode节点结果过多 2, zk 中的元数据信息不正确，无法完成数据同步 Cannot create table from metadata file /var/lib/clickhouse/metadata/xx/xxx.sql, error: Coordination::Exception: Can’t get data for node /clickhouse/tables/xx/cluster_xxx-01/xxxx/metadata: node doesn’t exist (No node), stack trace:** zk中元数据丢失导致节点无法启动 将/var/lib/clickhouse/metadata/ 下的SQL与/var/lib/clickhouse/data/xxxx/ 下的数据备份之后删除 启动数据库； 创建与原来表数据结构的MergeTree表； 将之前分布式表的数据文件夹复制到新表的数据目录中； 重启数据库； 重新创建原结构本地表； 重新创建原结构分布式表； insert into [分布式表] select * from [MergeTree表]。 Cannot execute replicated DDL query on leader** DDL语句由于超时无法执行 通过 system.replicas 表查看当前表的leader节点，在leader节点日志中搜索DDLWorker，找到具体的query-&lt;query_number&gt;日志、 或者通过下面的SQL语句查看当前堵住的任务，哪一个节点无法完成，然后在对应的节点下查看具体日志 SELECT name, numChildren as success_nodes FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/query-0000001000/' Not executing log entry ... is greater than the current maximum 当前副本合并数量过多，检查当前是否是有小批次的写入或者写入影响到的分片过多，导致用于合并的磁盘空间被完全占用，可以提高max_bytes_to_merge_at_max_space_in_pool这个值的配置，提高用于合并的磁盘空间 内存增高直至节点OOM 在没有使用大内存SQL查询的情况下，陷入内存增高OOM节点重启的循环， 先调整max_server_memory_usage_to_ram_ratio参数现在整个节点可使用的最大内存，查看system.merges表看当前正在合并的分片的情况，有可能是由的数据分片数据过大，合并时拉入内存时撑爆内存 分片数据错误，无法完成合并，一直占用合并队列，后续数据无法写入 查看表system.replication_queue里的正在执行同步和merge的情况， 把对应无法完成的任务从队列中删除， clickhouse-client --query &quot;SELECT replica_path || '/queue/' || node_name FROM system.replication_queue JOIN system.replicas USING (database, table) WHERE create_time &lt; now() - INTERVAL 1 DAY AND type = 'MERGE_PARTS' AND last_exception LIKE '%No active replica has part%'&quot; | while read i; do zk-cli.py --host ... -n $i rm; done 然后重启同步 SYSTEM RESTART REPLICAS //这个操作会重置 ReplicatedMergeTree表的Zookeeper会话状态。以Zookeeper为参照，对比当前状态，有需要的情况下将任务添加到ZooKeeper队列。在复制表和需要同步的parts比较多的情况下，基本上无法完成。 如果不行，就只能重启节点重置同步状态。 clickhouse重启时，默认执行 chown 101 /var/lib/clickhouse,耗时很长,导致启动需要花很久服务才可用 设置环境变量 CLICKHOUSE_DO_NOT_CHOWN=1 优化 数据类型选择 1 建表时能用数值型或日期时间型表示的字段就不要用字符串，String类型为非固定长度数据类型，相对于固定长度的数据类型，处理效率更低。 2 ClickHouse底层将DateTime存储为时间戳Long类型，但不建议存储Long类型，因为DateTime不需要经过函数转换处理，执行效率高、可读性好。 3 Nullable类型几乎总是会拖累性能，因为存储Nullable列时需要创建一个额外的文件来存储NULL的标记，并且Nullable列无法被索引。因此除非极特殊情况，应直接使用字段默认值表示空，或者自行指定一个在业务中无意义的值 分区 考虑到Clickhouse的底层存储结构，写入、读取、查询条件预先筛选满足条件的分区，设立合适的分区条件，减少分区的合并操作和提高使用过程中的效率 主键排序键 主键用于生成稀疏索引（Granule中的起始值），排序建用于part内数据排序。 所以主键序列和Order By序列要排序方式保存一致的（主键序列是和Order By序列保存完全一致的，或者把主键序列定义成Order By序列的部分前缀。），这样数据的一致性较高，压缩性会好。 2 在CollapsingMergeTree 和 SummingMergeTree 引擎里进行数据合并时会提供额外的处理逻辑。 在这种情况下，需要执行主键和排序键不同 通常在使用SummingMergeTree 和 AggregatingMergeTree 引擎时引擎时，表里的列分两种：维度 和 度量 。典型的查询会通过任意的 GROUP BY 对度量列进行聚合并通过维度列进行过滤。由于 SummingMergeTree 和 AggregatingMergeTree 会对排序键相同的行进行聚合，所以把所有的维度放进排序键是很自然的做法。但这将导致排序键中包含大量的列，并且排序键会伴随着新添加的维度不断的更新。【分片变更会更多】 在这种情况下合理的做法是，只保留少量的列在主键当中用于提升扫描效率，将维度列添加到排序键中。减少分片的变更数量 Join 查询 1 当多表联查是，查询的数据仅从其中一张表出时，可考虑用 IN 操作而不是JOIN select a.* from a where a.uid in (select uid from b) #不要写成select a.* from a left join b on a.uid=b.uid 2 多表join时要满足小表在右的原则，右表关联时被加载到内存中与左表进行比较，无论是Left join 、Right join 还是 Inner join 永远都是拿着右表中的每一条记录到左表中查找该记录是否存在，所以右表必须是小表。 3 利用GLOBAL JOIN 避免查询放大带来性能损失 写入和删除 不要执行单条或小批量删除和插入操作，这样会产生小分区文件，给后台Merge任务带来巨大压力， 每次写入影响到的分片应该更小，不要一次写入太多分区，或数据写入太快，数据写入太快会导致Merge速度跟不上而报错 常用表和语句 system.clusters：当前节点关联到的集群信息 system.crash_log: 堆栈跟踪的致命错误信息。默认情况下，该表在数据库中不存在，仅在发生致命错误时才创建该表 system.dictionaries: 当前系统中所有的字典 system.distributed_ddl_queue: 分布式DDL任务队列记录 system.merges: 当前正在执行的的合并任务 system.mutations: 当前的mutation任务 system.replicas: 当前系统的同步表的相关信息 system.metrics: 当前系统状态监控。 比如，查看当前可用与合并的磁盘情况 FROM system.metrics WHERE metric = 'DiskSpaceReservedForMerge' ┌─metric────────────────────┬─────value─┬─description──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ DiskSpaceReservedForMerge │ 414118102 │ Disk space reserved for currently running background merges. It is slightly more than the total size of currently merging parts. │ └───────────────────────────┴───────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ system.processes： 查看当前正在执行的查询任务 system.events: 查看从系统启动后关键事件的发生次数 system.settings: 查看当前系统的配置信息 system.zookeeper: 当前系统在zookeeper中存储的信息，节点路径，子节点数量等信息 确认当前连接情况 SELECT * FROM system.metrics WHERE metric LIKE '%Connection 当前正在执行的查询 SELECT query_id, user, address, query FROM system.processes ORDER BY query_id; KILL Query KILL QUERY WHERE query_id='ff695827-dbf5-45ad-9858-a853946ea140'; 慢查询 user, client_hostname AS host, client_name AS client, formatDateTime(query_start_time, '%T') AS started, query_duration_ms / 1000 AS sec, round(memory_usage / 1048576) AS MEM_MB, result_rows AS RES_CNT, result_bytes / 1048576 AS RES_MB, read_rows AS R_CNT, round(read_bytes / 1048576) AS R_MB, written_rows AS W_CNT, round(written_bytes / 1048576) AS W_MB, query FROM system.query_log WHERE type = 2 ORDER BY query_duration_ms DESC LIMIT 10``` ## 表级别的空间占用 ```select database, \\ table, \\ sum(rows) AS &quot;总行数&quot;, \\ formatReadableSize(sum(data_uncompressed_bytes)) as &quot;原始大小&quot;, \\ formatReadableSize(sum(data_compressed_bytes)) AS &quot;压缩大小&quot;, \\ round((sum(data_compressed_bytes) / sum(data_uncompressed_bytes)) * 100.,2) AS &quot;压缩率/%&quot; \\ from system.parts \\ group by database,table \\ order by database``` ","link":"https://nomagic.cc/post/clickhouse-ru-men-shi-yong-pian/"},{"title":"关于微服务","content":"为什么需要微服务架构和开发模式 其实本质上是一个解耦和治理的过程，我们知道现在的应用服务越来越复杂，代码量也越来越大，如果还是按照单体模式去构造应用就会带来很多问题比如： 代码管理：所有代码都耦合在一起，一个应用的代码量惊人，没有人可以全面的了解代码的方方面面，给后期的维护变更带来比较大的困难。 功能解耦，模块隔离：比如只修改了应用的一个方面，也要对应用进行整体的测试，持续部署更是不怎么可能，现在每天都要进行多次部署更新，单体应用的停止和启动时间都比较长，即使只修改了一个很小的方面，然后整个应用都要重启，这是不可接受的。 稳定性、可用性和安全性隔离：隔离bug，不会因为单个模块的一个bug影响整个服务的稳定性和可用性，比如某块有内存泄露，或者crash不会使整个应用都不可用。 工具选择，框架迭代：每个模块都有更适合自己使用的语言，工具或者是框架，假如是单体应用就做不到灵活的选择。 数据解耦：从封装隔离和安全性的角度来说，每个模块不应该可以读取或者修改到整体的数据。 每个微服务需要有自己独立的数据库的原因主要有以下几点： 1 解耦性：微服务架构的一个核心原则是解耦，每个微服务都应该是独立的，不应该相互依赖。如果多个微服务共享一个数据库，那么这些微服务就会相互依赖，一个微服务的修改可能会影响其他微服务。 2 可扩展性：每个微服务的数据量和访问量可能不同，如果多个微服务共享一个数据库，那么可能会导致数据库的性能瓶颈。如果每个微服务都有自己独立的数据库，那么就可以根据每个微服务的实际情况进行扩展。 3 安全性：每个微服务的数据可能包含敏感信息，如果多个微服务共享一个数据库，那么可能会导致数据泄露的风险。如果每个微服务都有自己独立的数据库，那么就可以根据每个微服务的安全需求进行配置。 具体来说，以下是每个微服务独立数据库的优势： 1 提高数据库性能：每个微服务的数据量和访问量可能不同，如果多个微服务共享一个数据库，那么可能会导致数据库的性能瓶颈。如果每个微服务都有自己独立的数据库，那么就可以根据每个微服务的实际情况进行优化，提高数据库的性能。 2 提高数据库可用性：如果多个微服务共享一个数据库，那么如果数据库发生故障，可能会影响多个微服务。如果每个微服务都有自己独立的数据库，那么即使一个数据库发生故障，也不会影响其他微服务。 3 提高数据库安全性：每个微服务的数据可能包含敏感信息，如果多个微服务共享一个数据库，那么可能会导致数据泄露的风险。如果每个微服务都有自己独立的数据库，那么就可以根据每个微服务的安全需求进行配置，提高数据库的安全性。 当然，每个微服务独立数据库也有一定的缺点，比如需要增加数据库的管理成本。但是，总体而言，每个微服务独立数据库带来的优势要大于缺点。 灵活部署：应用中每个模块的特性是不同的，有些是IO密集型，有些是CPU密集型，有些只要部署一个副本就可以提供服务，有些需要多个部分才行。把单体应用切割成微服务的形式可以更好的适应每个模块的资源要求。 总体来说，微服务可以提高交付的灵活性和速度，这是当前软件研发环境所必须的能力，微服的过程本质上是一个解耦和治理的过程。 微服务的优点： 把复杂问题分治成若干简单问题。及把庞大冗杂的单体应用划分成若干隔离的，可管理与治理的服务，每个服务都是奔着高内聚低耦合的方向去的。 每个服务可以由单独的团队专注开发，使用最适合的技术，迭代更加迅速 每个服务独立部署与治理，使得持续部署成为可能 微服务的缺点 分布式架构：使得整体架构更加复杂，由于分布式架构带来的复杂性 参考分布式服务的8大谬误 数据库分：业务的事务无法使用本地事务，分布式事务和数据分区带来的数据一致性挑战 测试和排错复杂：服务间的依赖A服务依赖B服务，B服务依赖C服务，C依赖于D。。。 整个应用的服务部署复杂：每个微服务都要单独配置基础设置服务，和单独部署另外还要考虑不同服务的部署要求和服务间之间的依赖关系。 API网关 为什么要使用API网关 1 单体应用端口很少，部署的方式是多副本和负载均衡提供服务， 微服务模式每个微服务都会暴露自己的更细力度的端口，调用方是无法协调那么多端口的 2 一个接口数据，调用方不可能调用所有服务然后自己聚合数据，这个通信量网络环境也是不允许的。 3 微服务不一定使用web友好的协议，需要通过协议转换的方式对外提供服务 4 客户端不应该与微服务通信，应该是隔离开的，微服务需要可以独立变更。 5 接口也许要治理与管理：“认证、监控、负载均衡、缓存和静态，响应统一处理，限流/熔断” API网关的优点 封装了应用的内部接口，客户端只需要和网关通信，通过接口聚合，客户端减少和和服务的通信次数 把一些通用的逻辑前置（认证、监控、负载均衡、缓存和静态，响应统一处理，限流/熔断），无需散落在应用的不同地方 API网关的缺点 必须高可用 需要单独开发，部署和管理 部署完服务后，需要单独更新网关才能对外暴露服务 服务发现 为什么使用服务发现 交互中需要知道一个服务的位置，然后现在基于云的微服务应用，服务实例都是动态分配的网络特性，由于自动伸缩，故障升级，服务的网络位置都会变更，所以需要提供一种服务发现机制 服务发现的种类 客户端服务发现 客户端可以通过某种机制来获取服务的地址和端口。常见的动态服务发现机制包括： 注册中心：注册中心是一个集中式的数据库，用于存储服务的信息。客户端可以通过注册中心来获取服务的信息。 DNS：DNS可以用于解析域名，并返回域名的 IP 地址。客户端可以通过 DNS 来获取服务的信息。 Zookeeper：Zookeeper 是一个分布式协调服务，可以用于存储服务的信息。客户端可以通过 Zookeeper 来获取服务的信息。 客户端服务发现具有以下优点： 灵活性：客户端服务发现可以根据需要选择合适的机制。 可扩展性：客户端服务发现可以随着服务的数量增加而扩展。 客户端服务发现也存在以下缺点： 复杂性：动态服务发现的实现相对复杂,每个客户端都要内嵌相应实现，如果使用客户端使用的语言不通也需要多语言的去实现客户端发现机制 性能：动态服务发现可能会影响客户端的性能。 服务端服务发现 一般服务端服务发现，是指反向代理的模式，客户端通过负载均衡器向服务发出请求。负载均衡器查询服务注册中心并将每个请求路由到可用的服务实例。与客户端发现一样，服务实例由服务注册中心注册与销毁，反向代理均衡器实现了服务端发现的机制 微服务的分布式数据管理 服务所拥有的数据对当前微服务来说是私有的，只能通过其提供的 API 进行访问。封装数据可确保微服务 松耦合，独立演进 。 如 果多个服务访问相同的数据， 模式（schema）更新需要对所有服务进行耗时、协调的更新。 更糟糕的是，不同的微服务经常使用不同类型的数据库。现代应用程序存储和处理着各种数据，而关系型数据库并不总是最佳选择。在某些场景，特定的 NoSQL 数据库可能具有更方便的数据模型，提供了更好的性能和可扩展性，这就使得数据的访问和变更变得异常复杂。 分布式事务及两阶段提交在当前的服务环境下也是不可行的，根据CAP理论，我们只能选择服务的可用性，放弃数据的短期内的一致性，只能追求数据的最终一致性。 分布式事务 微服务下的数据一致性方案 事件驱动架构 在可以异步处理的模式下，并且短期内可以容忍数据不一致的情况，可以使用数据驱动的模式。 在这种架构中，服务之间通过事件进行通信和协作，而不是直接调用彼此的API。 事件驱动架构基于发布-订阅模型，其中服务可以发布事件，而其他服务可以订阅并对这些事件做出响应。当一个服务发布一个事件时，其他服务可以选择性地接收并处理该事件。 以下是微服务事件驱动架构的一些关键概念和组件： 事件：事件是系统中发生的重要事实或状态变化的表示。它可以是一个简单的数据结构，包含与该事件相关的信息。 事件发布者：事件发布者是一个服务，负责发布事件。当某个重要事件发生时，它将事件发送到事件总线或消息队列，以便其他服务可以接收到。 事件订阅者：事件订阅者是一个服务，它通过订阅特定类型的事件来表明它对该事件感兴趣。一旦订阅成功，当该类型的事件被发布时，订阅者将接收到该事件并进行相应的处理。 事件总线/消息队列：事件总线或消息队列是服务之间传递事件的中间件。它负责接收发布者发送的事件，并将其传递给订阅者。 微服务事件驱动架构的优势包括： 松耦合：服务之间通过事件进行通信，彼此之间解耦，因为发布者不需要知道哪些服务订阅了事件，订阅者也不需要知道事件来自哪个发布者。 可扩展性：由于事件驱动的架构是异步的，它可以轻松地处理高负载和大规模的系统。 弹性和松散耦合：当一个服务发生故障或不可用时，其他服务可以继续运行，并在服务恢复后重新处理事件。 可组合性：通过将服务设计为可重用和可组合的事件处理单元，可以构建复杂的业务流程和工作流。 当设计和实现微服务架构时，事件驱动架构可以提供一种灵活、可伸缩和可扩展的方式来处理服务之间的通信和协作。 如何原子性的发送消息： 建立一张event表，然后把这张表的事件插入纳入业务事务的范围，这样就可以保证消息的发送和业务数据变更是一个原子性的操作，通过另外的进程读取这张表发布到消息队列或者消息总线中。这样不止有消息的发送记录，还可以追踪业务数据的变更记录。 微服务的服务治理 ","link":"https://nomagic.cc/post/guan-yu-wei-fu-wu/"},{"title":"抽象和抽象原则","content":"#抽象的定义： 在软件工程和计算机科学中，抽象是一种隐藏计算机系统复杂性的技术。他的工作原理是在用户和计算机系统的交互之间建立一个简单的层次，隐藏更多的底部复杂细节【wikipedia】 可以看出这里的抽象就是指复杂功能的抽象出来的接口层；PS：希望谁能提出更好的理解或者定义。 #抽象的时机： Abstraction: The Rule Of Three 文章中讲到了三种抽象的时机 Don't Repeat Yourself 这个大概是指“if you need it once, build it. If you need it twice, abstract it ”。我认为即使指需要一次也需要进行某种程度的抽象，这可以让你的程序更加清晰，结构性更好。当然通过抽象的手段如何你设计的够好的话，肯定会符合Don't Repeat Yourself这个原则的。 You Aint Gonna Need It Always implement things when you actually need them, never when you just foresee that you need them 这个原则来自极限编程思想是说当你真正需要一件事物时，再去构造它。放在这里是讲，在很早的时候真的需要进行抽象吗？主要的担心是来自过早优化。那么什么时候进行优化呢？就引入了The Rule of Three. The Rule Of Three 当你第三次需要他的时候，再去抽象他。解决了You Aint Gonna Need It的疑问，但是和DRY想冲突（一次都不允许重复的话） 总结来就是：第一次需要时构造它，第二次需要时拷贝它，第三次需要时抽象它。这样是因为第一次和第二次需要时你没有足够的条件知道个组件要达到的要求。【其实这里谈的抽象更偏向于如果某个东西多次用到，然后把它拿出来独立抽象成一个组件，那么怎么进行这种抽象呢？下面引出 Abstraction Principle】 抽象的原则： The interface of a component should be independent of its implementation 。 一个组件的接口应该和它的实现分离。组件的接口是用户的视角，组件的实现是开发者的视角，如果组件的设计采用了Abstraction Principle，那么用户在使用组件的过程中，并不需要知道组件是如何工作的。并且如果开发者要修改组件的实现时也不需要通知使用者。 最典型的例子： 汽车，我们可以把接口看做仪表盘，油门，刹车，和方向盘等。而实现是引擎和传统装置，而过司机想要驾驶汽车不必要懂得引擎的知识。 另外的例子是面向接口编程，如果定义了每个模块的接口，每个模块就可以通过接口进行通信，而不必关心模块是怎么实现的，并且一个模块实现的修改也不会影响到其他的模块。 那总结怎么实践抽象原则呢？ 如果你要开发一个组件，那么这个组件就要使用相应的对外接口，和对内实现的方式进行抽象。 The Abstraction Principle Abstraction: The Rule Of Three ","link":"https://nomagic.cc/post/chou-xiang-he-chou-xiang-yuan-ze/"},{"title":"数据结构之间的逻辑","content":"这边文章只是试图理解每种数据结构之间的关系，理解每个数据结构是用来解决什么问题的，然后有什么不足，进而通过什么数据结构来解决这个不足 从分治说起 应对大数据量的方法。 分治和递归 递归是分治的一种程序性结构(碰巧解决方案一样了：非重叠子问题)。 从二分查找到二叉树 使用二分法对有序列表进行查找，如果使用二分的思想对数据进行存储，那么我们就得到了一颗二叉树。 左右指针，只是二叉树的一种存储形式 为什么是二叉树，不是3/4/5..k分叉树 函数：(k-1)log(k) 的含义和增长性 从二叉树到平衡树 二叉树退化成链表 AVL、BTree、RBTree 之间的比较 为什么有这么多种类型的平衡树 理解AVL树的选择操作 avl 树的要求：每个节点的左子树和右子树的高度差至多为1。 BTree的自平衡，以及自平衡的代价，为什么DB存储偏爱BTree 自平衡：度(dgree)的概念，每个节点key数据 dgree-1 &lt;= count(key)&lt;=2*dgree-1， 插入节点分裂,导致树的增高，删除节点合并，导致树的高度降低 自平衡的代价：分裂和合并的复杂度过高，尤其是合并的实现复杂的 为什么使用B+Tree实现索引 低高度的BTree就可以存储大量的节点（16KB/(8B+6B)≈1170），局部性更适用与磁盘的读写特性。 为什么RBTree的使用场景更广 综合效率更高 理解红黑树的性质： 性质1. 结点是红色或黑色。 性质2. 根结点是黑色。 性质3. 所有叶子都是黑色。（叶子是NIL结点） 性质4. 每个红色结点的两个子结点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色结点） 性质5. 从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点 性质的结果：最长路径 &lt;= 2*（最短路径）==&gt; 树的失衡性保持在一定范围内 最长路径：红黑交替 最短路径：全黑 保持的方法：旋转+着色 红黑树适用的场景： 数据量有限，读写频繁。 使用的场景： 文件系统 linux 进程调度，用红黑树管理进程控制块 epoll实现中，poll对象的管理。 epoll 在内核中的实现，用红黑树管理事件块 nginx 中，用红黑树管理 timer hash table 和平衡树的比较 hash 函数的性能损耗 hash 存储的离散性，无法支持range查询 hash 对于内存使用，需要先申请一段内存，后面对于扩容和缩容的需求。 但是在大内存，数据量客观的情况下，相对于平衡树。hash会更有优势，反观hash使用的场景。 为什么redis使用dict和zset为什么使用 skiplist 而不是平衡树 为什么使用disc，参数上面hash table 和平衡树的比较。 redis 是内存型数据库，寻址的时间是一样的。zset 更多的范围查询，如果使用平衡树是使用btree还是rbtree？会有什么样的影响 拓展（只做关键概念介绍） 堆，前缀树，线段树（集合） 堆 前缀树 线段树： 求 [1,2,3,4,5] 任意字区间的和 总结 可视化网站： https://visualgo.net/zh https://www.cs.usfca.edu/~galles/visualization/BTree.html ","link":"https://nomagic.cc/post/shu-ju-jie-gou-zhi-jian-de-luo-ji/"},{"title":"C/C++ 变量链接属性和存储属性","content":"备注： 文章中出现的标识符即C语言中定义的变量或者函数名称 问题 我们知道C语言的代码实现文件（.c） 经过编译后（注意这里并没有.h文件），生成对应的.o文件，然后经过链接器链接成生成可执行程序，那么： 当我们在不同的.c文件中定全局变量，这个全局变量只会被编译当前.o文件中，其他引用这个变量的文件编译时如何处理？ 定义了一个全局标识符之后，链接器又是如何把这个全局标识符链接点可执行程序的呢？ 更进一步的.c文件相当于C语言中的模块封装机制，链接器又是如何实现数据结构的封装与隔离的呢？ 这就不得不说一下C语言中标识符的链接属性和存储类型 链接属性 链接熟悉决定了链接器如何处理不同文件中同名的标识符，从而限定了一个标识符的作用域。 在C语言中，链接属性分为三类： external 对应关键字是extern internal 对应关键字 static none (无) external extern属性指定的标识符是定义在其他.c文件中，链接的是时候要从其他.o文件中链接。 例如： //hello.c #include&lt;stdio.h&gt; const char* default_msg = &quot;hello,world&quot;; void say_hey(const char* msg){ printf(&quot;%s\\n&quot;,msg); } //main.c const char* default_msg; void say_hey(const char*); int main(){ say_hey(default_msg); } 编译时报错： root@inspire:/data/cpp/test2# cc main.c hello.c /usr/bin/ld: /tmp/ccsINlvC.o:(.data.rel.local+0x0): multiple definition of `default_msg'; /tmp/cc780SKl.o:(.bss+0x0): first defined here collect2: error: ld returned 1 exit status 如果在main.c文件中不声明default_msg 变量,在单独编译.c文件的时候，会因为找不到变量声明而报错，如果普通声明了default_msg变量，链接的时候会提示全局变量default_msg分别定义在了两个.o文件中， 我们的本意是链接定义在hello.c文件中的default_msg。这个时候我们只需要添加extern属性标识 extern const char* default_msg; 标识default_msg定义在其他.o文件中，这样main.c就可以正常编译过，等链接时去其他.o文件中链接default_msg的定义。 internal 任何的语言都提供封装抽象的能，C也不例外，除了按照函数或者struct 进行抽象分装，C也提供了文件作用域的封装，类似于其他语言中的模块，例如在一个文件中定义的变量或者函数，不想被其他文件作用域的程序引用到如何处理呢，答案就是把这个标识符声明为文件内部的，已达到抽象封装的目的 举例： //fight_result_handle.h #ifndef FITHT_RESULT_HANDLE_H #define FITHT_RESULT_HANDLE_H enum result_t {win,lose}; void onWin(); void onLose(); #endif //lottery.c #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;time.h&gt; #include &quot;fight_result_handle.h&quot; static const int buf[] = {40,10}; static const int threshold = 60; static void lottery(enum result_t t){ srand(time(0)); if (rand() % 100 + buf[t] &gt; threshold){ printf(&quot;获取奖励\\n&quot;); return; } printf(&quot;再接再厉\\n&quot;); } void onWin(){ lottery(win); } void onLose(){ lottery(lose); } //main .c #include &quot;fight_result_handle.h&quot; int fight(){ return 0; } int main(){ int r = fight(); if (r &gt; 0){ onWin(); }else{ onLose(); } return 0; } lottery 中的 static const int buf[] = {40,10}; static const int threshold = 60; 和 static void lottery(enum result_t t) 函数的算法都是lottery.c内部要封装掉的，其他.o文件链接不到，达到的隔离的作用。 none 在C语言中，如果没有显式地指定链接属性，变量或函数的默认链接属性取决于其定义的位置。在函数内部定义的变量具有&quot;none&quot;链接属性，其作用范围仅限于该函数内部。而在函数外部定义的变量和函数具有&quot;external&quot;链接属性，它们可以在整个程序中访问。 存储类型 变量的存储类型(storage class)是指存储变量值的内存类型。变量的存储类型决定变量何时创建、何时销毁以及它的值将保持多久。有三个地方可以用于存储变量：普通内存、运行时堆栈、硬件寄存器。在这三个地方存储的变量具有不同的特性。 变量的缺省存储类型取决于它的声明位置。凡是在任何代码块之外声明的变量总是存储于静态内存中，也就是不属于堆栈的内存，这类变量称为静态(static)变量。对于这类变量，你无法为它们指定其他存储类型。静态变量在程序运行之前创建，在程序的整个执行期间始终存在。它始终保持原先的值，除非给它赋一个不同的值或者程序结束，位于程序镜像的常量区。 在代码块内部声明的变量的缺省存储类型是自动的(automatic)，也就是说它存储于堆栈中，称为自动(auto)变量。 在C++中由于auto存储类型声明符，不经常使用，所以C++官方把auto含义替换掉了，作为类型推导的关键字。 有一个关键字auto就是用于修饰这种存储类型的，但它极少使用，因为代码块中的变量在缺省情况下就是自动变量。在程序执行到声明自动变量的代码块时，自动变量才被创建，当程序的执行流离开该代码块时，这些自动变量便自行销毁。如果该代码块被数次执行，例如一个函数被反复调用，这些自动变量每次都将重新创建。在代码块再次执行时，这些自动变量在堆栈中所占据的内存位置有可能和原先的位置相同，也可能不同。即使它们所占据的位置相同，你也不能保证这块内存同时不会有其他的用途。因此，我们可以说自动变量在代码块执行完毕后就消失。当代码块再次执行时，它们的值一般并不是上次执行时的值。 为什么C不像其他语言自动初始化为0值呢，出于效率的考虑。插入自动初始化的指令，但这个零值不一定是程序需要的，干脆要什么都让程序员自己初始吧，减少了初始化指令的负担。 对于在代码块内部声明的变量，如果给它加上关键字static，可以使它的存储类型从自动变为静态。具有静态存储类型的变量在整个程序执行过程中一直存在，而不仅仅在声明它的代码块的执行时存在。注意，修改变量的存储类型并不表示修改该变量的作用域，它仍然只能在该代码块内部按名字访问。函数的形式参数不能声明为静态，因为实参总是在堆栈中传递给函数，用于支持递归。 最后，关键字register可以用于自动变量的声明，提示它们应该存储于机器的硬件寄存器而不是内存中，这类变量称为寄存器变量。通常，寄存器变量比存储于内存的变量访问起来效率更高。但是，编译器并不一定要理睬register关键字，如果有太多的变量被声明为register，它只选取前几个实际存储于寄存器中，其余的就按普通自动变量处理。如果一个编译器自己具有一套寄存器优化方 法，它也可能忽略register关键字，其依据是由编译器决定哪些变量存储于寄存器中比人脑的决定更为合理一些。 在典型情况下，你希望把使用频率最高的那些变量声明为寄存器变量。在有些计算机中，如果把指针声明为寄存器变量，程序的效率将能得到提高，尤其是那些频繁执行间接访问操作的指针。你可以把函数的形式参数声明为寄存器变量，编译器会在函数的起始位置生成指令，把这些值从堆栈复制到寄存器中。但是，完全有可能，这个优化措施所节省的时间和空间的开销还抵不上复制这几个值所用的开销。 寄存器变量的创建和销毁时间和自动变量相同，但它需要一些额外的工作。在一个使用寄存器变量的函数返回之前，这些寄存器先前存储的值必须恢复，确保调用者的寄存器变量未被破坏。许多机器使用运行时堆栈来完成这个任务。当函数开始执行时，它把需要使用的所有寄存器的内容都保存到堆栈中，当函数返回时，这些值再复制回寄存器中。 在许多机器的硬件实现中，并不为寄存器指定地址。同样，由于寄存器值的保存和恢复，某个特定的寄存器在不同的时刻所保存的值不一定相同。基于这些理由，机器并不向你提供寄存器变量的地址。 关于头文件的缘由 为什么有头文件这种工程文件组织形式？ 还是采用例子一，不过这个时候我们把 main 函数中的say_hey的函数声明去除。 //hello.c #include&lt;stdio.h&gt; const char* default_msg = &quot;hello,world&quot;; void say_hey(const char* msg){ printf(&quot;%s\\n&quot;,msg); } //main.c const char* default_msg; int main(){ say_hey(default_msg); } 编译执行 root@inspire:/data/cpp/sayhey# cc main.c hello.c main.c: In function ‘main’: main.c:4:9: warning: implicit declaration of function ‘say_hey’ [-Wimplicit-function-declaration] 4 | say_hey(default_msg); | ^~~~~~~ root@inspire:/data/cpp/sayhey# ./a.out hello,world root@inspire:/data/cpp/sayhey# 这时候我们看到，虽然main.c 不知道say_hey的函数声明，但是不妨碍main.c的正确编译，链接时能够链接到对应的文件即可。 那么我们就知道，函数声明只是给编译器提供调用函数的声明信息，并不是强制的。如果我们提供了函数签名，然后和调用的不一致，这时候编译器就可以提出警告。 如果函数声明写在.c文件中，每个调用地方都要写一次，显然不符合DRY(不要重复你自己)原则，如果要修改函数声明，每个地方还都要修改，所以把声明单独提取到.h文件。需要的地方用编译器#include指定导入即可。 ","link":"https://nomagic.cc/post/bian-liang-lian-jie-shu-xing-he-cun-chu-shu-xing/"},{"title":"缓存设计","content":"为什么要使用缓存 在高并发场景下，需要通过缓存来减少数据库的压力，使得大量的访问进来能够命中缓存，只有少量的需要到数据库层。由于缓存基于内存，可支持的并发量远远大于基于硬盘的数据库。所以对于高并发设计，缓存的设计时必不可少的一环。 为什么DB不可以 DB 基于硬盘 总体来看缓存的问题 穿透是怎么办 如何有效的防止穿透 穿透了之后如何防止大量大量请求压入数据库 如何更新缓存 缓存在架构中的位置 接入层缓存 nginx 应用层缓存 常见分布式缓存 Memcached Memcached适合做简单的key-value存储，内存使用率比较高，而且由于是多核处理，对于比较大的数据，性能较好。 但是缺点也比较明显，Memcached严格来讲没有集群机制，横向扩展完全靠客户端来实现。另外Memcached无法持久化，一旦挂了数据就都丢失了，如果想实现高可用，也是需要客户端进行双写才可以。 所以可以看出Memcached真的是设计出来，简简单单为了做一个缓存的。 Redis Redis提供持久化的功能，包括RDB的全量持久化，或者AOF的增量持久化，从而使得Redis挂了，数据是有机会恢复的。 Redis提供成熟的主备同步，故障切换的功能，从而保证了高可用性。 所以很多地方管Redis称为内存数据库，因为他的一些特性已经有了数据库的影子。 这也是很多人愿意用Redis的原因，集合了缓存和数据库的优势，但是往往会滥用这些优势，从而忽略了架构层面的设计，使得Redis集群有很大的风险。 很多情况下，会将Redis当做数据库使用，开启持久化和主备同步机制，以为就可以高枕无忧了，然而Redis的持久化机制，全量持久化则往往需要额外较大的内存，而在高并发场景下，内存本来就很紧张，如果造成swap【重点了解概念】，就会影响性能。增量持久化也涉及到写磁盘和fsync，也是会拖慢处理的速度，在平时还好，如果高并发场景下，仍然会影响吞吐量。 所以在架构设计角度，缓存就是缓存，要意识到数据会随时丢失的，要意识到缓存的存着的目的是拦截到数据库的请求。如果为了保证缓存的数据不丢失，从而影响了缓存的吞吐量，甚至稳定性，让缓存响应不过来，甚至挂掉，所有的请求击穿到数据库，就是更加严重的事情了。 如果非常需要进行持久化，可以考虑使用levelDB此类的，对于随机写入性能较好的key-value持久化存储，这样只有部分的确需要持久化的数据，才进行持久化，而非无论什么数据，通通往Redis里面扔，同时统一开启了持久化。 两者的区分与选择 为什么说Memcached 内存使用率高。 结构简单，key-value结构。 Redis 对于多核的情况会真么样 Redis的数据结构就丰富的多了，单线程的处理所有的请求，对于比较大的数据，性能稍微差一点。 为什么说Memcached 对于较大的数据性能会比较好 Redis 单线程的处理所有的请求，对于比较大的数据，性能稍微差一点。 Memcached 没有集群机制，客户端如何实现横向扩展 Memcached 的持久化替代方案。 缓存设计的方法 分层设计 这样某一层的缓存挂了，还有另一层可以撑着，等待缓存的修复，例如分布式缓存因为某种原因挂了，因为持久化的原因，同步机制的原因，内存过大的原因等，修复需要一段时间，在这段时间内，至少本地缓存可以抗一阵，不至于一下子就击穿数据库。而且对于特别特别热的数据，热到导致集中式的缓存处理不过来，网卡也被打满的情况，由于本地缓存不需要远程调用，也是分布在应用层的，可以缓解这种问题。 如何进程分层设计 分场景设计 具体问题，具体分析 到底要解决什么问题，可以选择不同的缓存。是要存储大的无格式的数据，还是要存储小的有格式的数据，还是要存储一定需要持久化的数据。具体的场景下一节详细谈。 分片设计 使得每一个缓存实例都不大，但是实例数目比较多，这样一方面可以实现负载均衡，防止单个实例称为瓶颈或者热点，另一方面如果一个实例挂了，影响面会小很多，高可用性大大增强。分片的机制可以在客户端实现，可以使用中间件实现，也可以使用Redis的Cluster的方式，分片的算法往往都是哈希取模，或者一致性哈希。 特定场景下缓存的设计 和数据库中的数据结构保持一致，原样缓存 这种场景是最常见的场景，也是很多架构使用缓存的适合，最先涉及到的场景。 基本就是数据库里面啥样，我缓存也啥样，数据库里面有商品信息，缓存里面也放商品信息，唯一不同的是，数据库里面是全量的商品信息，缓存里面是最热的商品信息。 每当应用要查询商品信息的时候，先查缓存，缓存没有就查数据库，查出来的结果放入缓存，从而下次就查到了。 这个是缓存最最经典的更新流程。这种方式简单，直观，很多缓存的库都默认支持这种方式。 列表排序分页场景的缓存 有时候我们需要获得一些列表数据，并对这些数据进行排序和分页。 例如我们想获取点赞最多的评论，或者最新的评论，然后列出来，一页一页的翻下去。 在这种情况下，缓存里面的数据结构和数据库里面完全不一样。 如果完全使用数据库进行实现，则按照某种条件将所有的行查询出来，然后按照某个字段进行排序，然后进行分页，一页一页的展示。 但是当数据量比较大的时候，这种方式往往成为瓶颈，首先涉及的数据库行数比较多，而且排序也是个很慢的活，尽管可能有索引，分页也是翻页到最后，越是慢。 在缓存里面，就没必要每行一个key了，而是可以使用Redis的列表方式进行存储，当然列表的长短是有限制的，肯定放不下数据库里面这么多，但是大家会发现其实对于所有的列表，用户往往没有耐心看个十页八页的，例如百度上搜个东西，也是有排序和分页的，但是你每次都往后翻了吗，每页就十条，就算是十页，或者一百页，也就一千条数据，如果保持ID的话，完全放的下。 如果已经排好序，放在Redis里面，那取出列表，翻页就非常快了。 可以后台有一个线程，异步的初始化和刷新缓存，在缓存里面保存一个时间戳，当有更新的时候，刷新时间戳，异步任务发现时间戳改变了，就刷新缓存。 计数缓存 计数对于数据库来讲，是一个非常繁重的工作，需要查询大量的行，最后得出计数的结论，当数据改变的时候，需要重新刷一遍，非常影响性能。 因此可以有一个计数服务，后端是一个缓存，将计数作为结果放在缓存里面，当数据有改变的时候，调用计数服务增加或者减少计数，而非通过异步数据库count来更新缓存。 计数服务可以使用Redis进行单个计数，或者hash表进行批量计数 重构维度缓存 重构聚合的结果 有时候数据库里面保持的数据的维度是为了写入方便，而非为了查询方便的，然而同时查询过程，也需要处理高并发，因而需要为了查询方便，将数据重新以另一个维度存储一遍，或者说将多给数据库的内容聚合一下，再存储一遍，从而不用每次查询的时候都重新聚合，如果还是放在数据库，比较难维护，放在缓存就好一些。 例如一个商品的所有的帖子和帖子的用户，以及一个用户发表过的所有的帖子就是属于两个维度。 这需要写入一个维度的时候，同时异步通知，更新缓存中的另一个维度。 在这种场景下，数据量相对比较大，因而单纯用内存缓存Memcached或者redis难以支撑，往往会选择使用levelDB进行存储，如果levelDB的性能跟不上，可以考虑在levelDB之前，再来一层Memcached。 较大的详情内容数据缓存 对于评论的详情，或者帖子的详细内容，属于非结构化的，而且内容比较大，因而使用Memcached比较好 缓存三大矛盾问题 缓存实时性和一致性问题：当有了写入后咋办？ 虽然使用了缓存，大家心里都有一个预期，就是实时性和一致性得不到完全的保证，毕竟数据保存了多份，数据库一份，缓存中一份，当数据库中因写入而产生了新的数据，往往缓存是不会和数据库操作放在一个事务里面的，如何将新的数据更新到缓存里面，什么时候更新到缓存里面，不同的策略不一样。 从用户体验角度，当然是越实时越好，用户体验越流畅，完全从这个角度出发，就应该有了写入，马上废弃缓存【当写入数据的时候，删除缓存里的内容】，触发一次数据库的读取，从而更新缓存。但是这和第三个问题，高并发就矛盾了，如果所有的都实时从数据库里面读取，高并发场景下，数据库往往受不了。 缓存的穿透问题：当没有读到咋办？ 为什么会出现缓存读取不到的情况呢？ 第一：可能读取的是冷数据，原来从来没有访问过，所以需要到数据库里面查询一下，然后放入缓存，再返回给客户。 第二：可能数据因为有了写入，被实时的从缓存中删除了，就如第一个问题中描述的那样，为了保证实时性，当数据库中的数据更新了之后，马上删除缓存中的数据，导致这个时候的读取读不到，需要到数据库里面查询后，放入缓存，再返回给客户。 第三：可能是缓存失效了，每个缓存数据都会有失效时间，过了一段时间没有被访问，就会失效，这个时候数据就访问不到了，需要访问数据库后，再放入缓存。 第四：数据被换出，由于缓存内存是有限的，当使用快满了的时候，就会使用类似LRU策略，将不经常使用的数据换出，所以也要访问数据库。 第五：后端确实也没有，应用访问缓存没有，于是查询数据库，结果数据库里面也没有，只好返回客户为空，但是尴尬的是，每次出现这种情况的时候，都会面临着一次数据库的访问，纯属浪费资源，常用的方法是，讲这个key对应的结果为空的事实也进行缓存，这样缓存可以命中，但是命中后告诉客户端没有，减少了数据库的压力。 无论哪种原因导致的读取缓存读不到的情况，该怎么办？是个策略问题。 一种是同步访问数据库后，放入缓存，再返回给客户，这样实时性最好，但是给数据库的压力也最大。 另一种方式就是异步的访问数据库，暂且返回客户一个fallback值，然后同时触发一个异步更新，这样下次就有了，这样数据库压力小很多，但是用户就访问不到实时的数据了。【防止请求压到数据库那里】 缓存对数据库高并发访问：都来访问数据库咋办？ 我们本来使用缓存，是来拦截直接访问数据库请求的，从而保证数据库大本营永远处于健康的状态。但是如果一遇到不命中，就访问数据库的话，平时没有什么问题，但是大促情况下，数据库是受不了的。 一种情况是多个客户端，并发状态下，都不命中了，于是并发的都来访问数据库，其实只需要访问一次就好，这种情况可以通过加锁，只有一个到后端来实现。 另外就是即便采取了上述的策略，依然并发量非常大，后端的数据库依然受不了，则需要通过降低实时性，将缓存拦在数据库前面，暂且撑住，来解决。 解决缓存三大矛盾的刷新策略 实时策略 为什么更新时，不是更新缓存，而是把缓存置为失效。 所谓的实时策略，是平时缓存使用的最常用的策略，也是保持实时性最好的策略。 读取的过程，应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。如果命中，应用程序从cache中取数据，取到后返回。 写入的过程，把数据存到数据库中，成功后，再让缓存失效，失效后下次读取的时候，会被写入缓存。那为什么不直接写缓存呢？【这个还是非常重要的】因为如果两个线程同时更新数据库，一个将数据库改为10，一个将数据库改为20，数据库有自己的事务机制，可以保证如果20是后提交的，数据库里面改为20，但是回过头来写入缓存的时候就没有事务了，如果改为20的线程先更新缓存，改为10的线程后更新缓存，于是就会长时间出现缓存中是10，但是数据库中是20的现象。 这种方式实时性好，用户体验好，是默认应该使用的策略。 异步策略 所谓异步策略，就是当读取的时候读不到的时候，不直接访问数据库，而是返回一个fallback数据，然后往消息队列里面放入一个数据加载的事件，在背后有一个任务，收到事件后，会异步的读取数据库，由于有队列的作用，可以实现消峰，缓冲对数据库的访问，甚至可以将多个队列中的任务合并请求，合并更新缓存，提高了效率。 当更新的时候，异步策略总是先更新数据库和缓存中的一个，然后异步的更新另一个。 一是先更新数据库，然后异步更新缓存。当数据库更新后，同样生成一个异步消息，放入消息队列中，等待背后的任务通过消息进行缓存更新，同样可以实现消峰和任务合并。缺点就是实时性比较差，估计要过一段时间才能看到更新，好处是数据持久性可以得到保证。 一是先更新缓存，然后异步更新数据库。这种方式读取和写入都用缓存，将缓存完全挡在了数据库的前面，把缓存当成了数据库在用。所以一般会使用有持久化机制和主备的redis，但是仍然不能保证缓存不丢数据，所以这种情况适用于并发量大，但是数据没有那么关键的情况，好处是实时性好。 在实时策略扛不住大促的时候，可以根据场景，切换到上面的两种模式的一个，算是降级策略。 问题 fallback 内容后面如何刷新 实例 如何进行任务合并 异步更新如何保证数据的一致性 如果能保证队列的一致性，就可以。 3、定时策略 如果并发量实在太大，数据量也大的情况，异步都难以满足，可以降级为定时刷新的策略，这种情况下，应用只访问缓存，不访问数据库，更新频率也不高，而且用户要求也不高，例如详情，评论等。 这种情况下，由于数据量比较大，建议将一整块数据拆分成几部分进行缓存，而且区分更新频繁的和不频繁的，这样不用每次更新的时候，所有的都更新，只更新一部分。并且缓存的时候，可以进行数据的预整合，因为实时性不高，读取预整合的数据更快。 有关缓存就说到这里，下一节讲分布式事务。 ","link":"https://nomagic.cc/post/huan-cun-she-ji/"},{"title":"python 装饰器","content":"{{TOC}} 简述 装饰器是Python语言的一种高级语法糖，它可以用于动态地修改函数或类的行为。装饰器的使用可以简化代码，提高代码的可读性和可维护性。 装饰器的实现原理 Python的装饰器实质上是一个语法糖，它允许我们在不修改函数或方法源代码的情况下，给函数或者方法增加新的行为和处理逻辑。装饰器的实现原理主要基于以下两点： 函数和方法在 Python 中都是一等对象：这就意味着它们可以被赋值给其他变量，可以作为参数传递给其他函数，也可以作为函数的返回值。 Python 的可调用对象：在 Python 中，任何定义了 call 方法的对象都是可调用的。这不仅包括函数和方法，还包括类和类的实例。 了解了以上两个原则，我们可以知道装饰器无非就是： 接收一个函数（或类）作为输入 返回新的函数（或类）作为输出。 从上面我们可以看出，装饰器可以实现成函数或者类都调用对象，可以用来修饰函数或者类。 以函数的形式实现装饰器 一个简单装饰器的例子 def simple_decorator(function): def wrapper(): print(&quot;在函数执行前做些什么&quot;) function() print(&quot;在函数执行后做些什么&quot;) return wrapper @simple_decorator def my_function(): print(&quot;我是被装饰的函数&quot;) &quot;&quot;&quot; 实际的过程类似如下： my_function=simple_decorator(my_function) &quot;&quot;&quot; my_function() 我们可以给函数增加多层装饰来扩展函数的功能，例如： def first_decorator(func): def wrapper(): print(&quot;First decorator before function execution.&quot;) func() print(&quot;First decorator after function execution.&quot;) return wrapper def second_decorator(func): def wrapper(): print(&quot;Second decorator before function execution.&quot;) func() print(&quot;Second decorator after function execution.&quot;) return wrapper @first_decorator @second_decorator def my_function(): print(&quot;Original Function.&quot;) my_function() 装饰器的参数 Python装饰器的参数可以分为以下两种： 被装饰函数的参数：被装饰函数的参数可以通过装饰器函数的包装函数传递给被装饰函数。 装饰器函数的参数：装饰器函数可以接受一个或多个参数，这些参数可以用于装饰器的逻辑。 被装饰函数的参数 import time def timethis(func): def wrapper(*args,**kwargs): start = time.time() result = func(*args,**kwargs) end = time.time() print(func.__name__,end-start) return result return wrapper @timethis def countdown(n:int): while n &gt;0: n -=1 countdown(100000) 装饰器函数的参数 在这里的装饰器函数我们称为装饰器工厂函数，装饰器工厂函数返回实际的装饰器函数 装饰器工厂函数可以接受一个或多个参数，这些参数可以用于装饰器的逻辑。例如，以下是一个接受两个参数的装饰器工厂函数： def decorator_with_args(arg1, arg2): def actual_decorator(function): def wrapper(*args, **kwargs): print(&quot;装饰器参数：&quot;, arg1, arg2) result = function(*args, **kwargs) return result return wrapper return actual_decorator @decorator_with_args(&quot;hello&quot;, &quot;world&quot;) def my_function(a, b): print(&quot;函数参数：&quot;, a, b) 在上述代码中，decorator_with_args()函数接受一个参数arg1,arg2 这个是装饰器的参数，内部的wrapper(*args, **kwargs)对应被装饰函数的参数。 注意观察装饰器应用的过程 @timethis的时候是没有函数调用的，在@decorator_with_args(&quot;hello&quot;, &quot;world&quot;)的时候，其实是调用了工厂函数然后返回对一个的装饰器函数进行应用 装饰器的参数的应用 装饰器的参数可以用于各种场景，例如： 控制被装饰函数的行为：装饰器函数可以根据参数来控制被装饰函数的行为。例如，以下是一个根据参数来决定是否执行被装饰函数的装饰器函数： def decorator(enable=True): def inner_decorator(func): def wrapper(*args, **kwargs): if enable: return func(*args, **kwargs) else: return None return wrapper return inner_decorator @decorator(enable=False) def target(): print(&quot;Hello, world!&quot;) target() 在上述代码中，decorator()函数接受一个参数enable，该参数用于控制是否执行被装饰函数。 元信息丢失 def my_decorator(func): def wrapper(): &quot;&quot;&quot;This is wrapper function&quot;&quot;&quot; print('Calling', func.__name__) return func() return wrapper @my_decorator def greet(): &quot;&quot;&quot;This is greet function&quot;&quot;&quot; print('Hello!') print(greet.__name__) print(greet.__doc__) 输出结果为： wrapper This is wrapper function 我们知道greet函数已经被装饰器返回的函数替代了，已经不是原来的greet函数，但是站在使用者的角度上我们只是想扩展元函数的功能，并不想丢失对应的原信息，并且greet.__name__返回wrapper 也不是我们想要的。 这种情况下，Python的functools库提供了一个wraps装饰器，可以用来解决这个问题。wraps装饰器作用在包装函数上，可以将原函数的元信息复制到包装函数上，避免元信息丢失。使用方法如下： from functools import wraps from inspect import signature def my_decorator(func): @wraps(func) def wrapper(*args,**kwargs): &quot;&quot;&quot;This is wrapper function&quot;&quot;&quot; print('Calling', func.__name__) return func(*args,**kwargs) return wrapper @my_decorator def greet(say:str): &quot;&quot;&quot;This is greet function&quot;&quot;&quot; print(say) greet('Hello') print(greet.__name__) print(greet.__doc__) greet.__wrapped__('World') print(signature(greet)) 使用@wraps(func)后，打印greet函数的__name__和__doc__属性，这些信息都是greet函数本身的信息，元信息没有丢失。 @wraps 有一个重要特征是它能让你通过属性 wrapped 直接访问被包装函数。例 如: greet.__wrapped__('hello') 我们可以通过__wrapped__来解除装饰器例如： greet = greet.__wrapped__ wrapped 属性还能让被装饰函数正确暴露底层的参数签名信息。例如: print(signature(greet)) 一个很普遍的问题是怎样让装饰器去直接复制原始函数的参数签名信息，如果想自 己手动实现的话需要做大量的工作，最好就简单的使用 wrapped 装饰器。通过底层的 wrapped 属性访问到函数签名信息。 通过装饰器给函数添加属性，来扩展函数的功能 from functools import wraps, partial import logging def attach_wrapper(obj, func=None): if func is None: return partial(attach_wrapper, obj) setattr(obj, func.__name__, func) return func def logged(level, name=None, message=None): def decorate(func): logname = name if name else func.__module__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) @attach_wrapper(wrapper) def set_level(newlevel): nonlocal level level = newlevel @attach_wrapper(wrapper) def set_message(newmsg): nonlocal logmsg logmsg = newmsg return wrapper return decorate # Example use @logged(logging.DEBUG) def add(x, y): return x + y @logged(logging.CRITICAL,'example') def spam(): print('Spam!') logging.basicConfig(level=logging.DEBUG) add(2,3) add.set_message('Add called') add.set_level(logging.WARNING) add(2,3) 输出 DEBUG:__main__:add WARNING:__main__:Add called 使用装饰器来验证参数类型 from inspect import signature from functools import wraps def typeassert(*ty_args, **ty_kwargs): def decorate(func): if not __debug__: return func sig = signature(func) bound_type = sig.bind_partial(*ty_args, **ty_kwargs).arguments @wraps(func) def wrapper(*args, **kwargs): bound_values = sig.bind(*args, **kwargs) for name,value in bound_values.arguments.items(): if name in bound_type: if not isinstance(value, bound_type[name]): raise TypeError( 'Argument {} must be {}'.format(name, bound_type[name]) ) return func(*args,**kwargs) return wrapper return decorate @typeassert(int, int) def add(x, y): return x+y add(2,'Hello') 类装饰器-使用装饰器来装饰类 类装饰器可以对一个类进行装饰。类装饰器通常会在类被实例化之前或之后执行。 def decorator(cls): # 装饰器代码 return cls @decorator class MyClass: def __init__(self): print(&quot;Hello, world!&quot;) my_class = MyClass() 在上面的代码中，decorator函数是一个类装饰器。decorator函数会在类被实例化之前执行。 我们可以看到类装饰器是把类作为参数调用函数，然后用函数返回的类替换掉原定义的类 @decorator class MyClass: 相当于 MyClass = decorator(MyClass) 所以也可以用下面的方式进行实现： def my_decorator(cls): class Wrapper(cls): def __init__(self, *args, **kwargs): print(&quot;Before init&quot;) super().__init__(*args, **kwargs) print(&quot;After init&quot;) def extra_method(self): print(&quot;This is an extra method&quot;) return Wrapper @my_decorator class MyClass: def __init__(self, x, y): self.x = x self.y = y def my_method(self): print(&quot;This is my method&quot;) obj = MyClass(1, 2) obj.my_method() # 这将会打印 &quot;This is my method&quot; obj.extra_method() # 这将会打印 &quot;This is an extra method&quot; 上面的类似返回了一个集成MyClass的子类，进行类的扩展。 以类的方法来实现装饰器 from functools import wraps class A: # Decorator as an instance method def decorator1(self, func): @wraps(func) def wrapper(*args, **kwargs): print('Decorator 1') return func(*args, **kwargs) return wrapper # Decorator as a class method @classmethod def decorator2(cls, func): @wraps(func) def wrapper(*args, **kwargs): print('Decorator 2') return func(*args, **kwargs) return wrapper a = A() @a.decorator1 def spam(): pass @A.decorator2 def grok(): pass 以类的形式来实现装饰器 import types from functools import wraps class Profiled: def __init__(self, func): wraps(func)(self) self.ncalls = 0 def __call__(self, *args, **kwargs): self.ncalls += 1 return self.__wrapped__(*args, **kwargs) def __get__(self, instance, cls): if instance is None: return self else: return types.MethodType(self, instance) @Profiled def add(x, y): return x + y class Spam: @Profiled def bar(self, x): print(self, x) s = Spam() s.bar(1) 我们知道当调用一个类的时候，会调用类的__new__/__init__方法，然后返回类的实例 在第一个例子实质是 add = Profiled(add) 返回的事Profiled的实力，当执行add的调用是，实际上是执行实例的__call__方法 在第二个例子中，bar实际上已经是Spam的一个属性（可以参考python的属性协议） 当执行s.bar的时候实际上是执行的Profiled的__get__方法，然后把s实例绑定到bar的第一个参数执行。 ","link":"https://nomagic.cc/post/python-zhuang-shi-qi/"},{"title":"go micro 源码阅读-transport","content":"作用 从tranport.go的注释和之前的博文中可以看出transport的功能是提供异步通信。 // Package is an interface for synchronous communication package transport 接口 type Socket interface { Recv(*Message) error Send(*Message) error Close() error } type Client interface { Socket } type Listener interface { Addr() string Close() error Accept(func(Socket)) error } // Transport is an interface which is used for communication between // services. It uses socket send/recv semantics and had various // implementations {HTTP, RabbitMQ, NATS, ...} type Transport interface { Dial(addr string, opts ...DialOption) (Client, error) Listen(addr string, opts ...ListenOption) (Listener, error) String() string } 从之上的代码可以看出： 当Transport作为客户端时： 提供Dial主动连接功能，连接成功返回代表一个连接的Client，Client内嵌Socket，提供Send/Recv的网络收发功能。 当Transport作为服务端时： 提供Listen监听功能，当Listener.Accept成功返回时，返回一个代表连接的Socket，提供Send/Recv的网络收发功能。 Send和Recv的数据是Message结构 type Message struct { Header map[string]string Body []byte } 由上可以看出Client是客户端语义，Socket是服务端语义。 transport在Go-micro内部的实现是httpTransport Dial 从代码中可以看出本身Dial的逻辑很简单调用 conn, err = tls.DialWithDialer 或 conn, err = net.DialTimeout 最终返回httpTransportClient对象 type httpTransportClient struct { ht *httpTransport addr string conn net.Conn dialOpts DialOptions once sync.Once sync.Mutex r chan *http.Request bl []*http.Request buff *bufio.Reader } httpTransportClient 实现Client 接口提供Send、Recv网络首发功能。 通过httpTransportClient 的Send代码可以看出，通过tpc的链接发送http请求。 func (h *httpTransportClient) Send(m *Message) error { header := make(http.Header) for k, v := range m.Header { header.Set(k, v) } reqB := bytes.NewBuffer(m.Body) defer reqB.Reset() buf := &amp;buffer{ reqB, } req := &amp;http.Request{ Method: &quot;POST&quot;, URL: &amp;url.URL{ Scheme: &quot;http&quot;, Host: h.addr, }, Header: header, Body: buf, ContentLength: int64(reqB.Len()), Host: h.addr, } ///&gt;&gt;&gt;&gt;&gt;&gt; h.Lock() h.bl = append(h.bl, req) select { case h.r &lt;- h.bl[0]: h.bl = h.bl[1:] default: } h.Unlock() ///&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; // set timeout if its greater than 0 if h.ht.opts.Timeout &gt; time.Duration(0) { h.conn.SetDeadline(time.Now().Add(h.ht.opts.Timeout)) ///设置超时 } return req.Write(h.conn) } 其他的代码都好理解，唯独///&gt;&gt;&gt;&gt;&gt;&gt; ///&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;中间的部分，为什么一直往h.bl中append,仅仅h.r可写的情况下remove. 猜想这里是不是应该这么写 if !h.dialOpts.Stream { h.Lock() h.bl = append(h.bl, req) select { case h.r &lt;- h.bl[0]: h.bl = h.bl[1:] default: } h.Unlock() } func (h *httpTransportClient) Recv(m *Message) error { if m == nil { return errors.New(&quot;message passed in is nil&quot;) } var r *http.Request if !h.dialOpts.Stream { rc, ok := &lt;-h.r if !ok { return io.EOF } r = rc } h.Lock() defer h.Unlock() if h.buff == nil { return io.EOF } // set timeout if its greater than 0 if h.ht.opts.Timeout &gt; time.Duration(0) { h.conn.SetDeadline(time.Now().Add(h.ht.opts.Timeout)) } rsp, err := http.ReadResponse(h.buff, r) if err != nil { return err } defer rsp.Body.Close() b, err := ioutil.ReadAll(rsp.Body) if err != nil { return err } if rsp.StatusCode != 200 { return errors.New(rsp.Status + &quot;: &quot; + string(b)) } m.Body = b if m.Header == nil { m.Header = make(map[string]string) } for k, v := range rsp.Header { if len(v) &gt; 0 { m.Header[k] = v[0] } else { m.Header[k] = &quot;&quot; } } return nil } 从if !h.dialOpts.Stream //这里也可以看出Stream的Send和对应的Recv直接不能插入其他的其他的Stream.Send。 Listener 根据传进来的addr，然后net/tls.Listen创建的Listener对象。 Accept 上面返回的Listener对象上进行Accept操作，如果有新的连接成功的话，创建Socket对应，用Socket调用注入的接口 fn. func (h *httpTransportListener) Accept(fn func(Socket)) error ","link":"https://nomagic.cc/post/go-micro-yuan-ma-yue-du-transport/"},{"title":"go micro 源码阅读-Broker","content":"作用 从前面的博文可以看出Broker是Service异步通信的基础功能组件。那么好奇的是Broker的代码逻辑到底是怎么样的，如何提供异步通信呢？ 整体代码逻辑 type Broker interface { Options() Options Address() string Connect() error ///启动broker服务 Disconnect() error ///关闭Broker服务 Init(...Option) error Publish(string, *Message, ...PublishOption) error ///publish topic message Subscribe(string, Handler, ...SubscribeOption) (Subscriber, error) ///注册 topic message 的 subscribe String() string } 从Broker的接口可以看出Broker基于替丁topic 的Pub/Sub的方式提供异步通信。 通过调用Connect 开启Broker 通过Subsribe 注册对某个topic的监听 通过Publish 发布某个topic的消息 通过调用Disconnect关闭Broker 代码解析 创建 func NewBroker(opts ...Option) Broker { return newHttpBroker(opts...) } 通过NewBroker调用newHTTPBroker返回时间Broker接口的httpBroker实例。[关于Option的处理，请参考] func newHttpBroker(opts ...Option) Broker { ....... h := &amp;httpBroker{ id: &quot;broker-&quot; + uuid.NewUUID().String(), address: addr, opts: options, r: reg, c: &amp;http.Client{Transport: newTransport(options.TLSConfig)}, ///用于publish时发送消息 subscribers: make(map[string][]*httpSubscriber), exit: make(chan chan error), mux: http.NewServeMux(), } h.mux.Handle(DefaultSubPath, h) ///添加默认路由handler，所有publish过来的method 到h.HTTPServer处理 return h } 启动/关闭监听 Connect 启动监听是Connnect函数，感觉这个名字起得很不好，有点迷惑，并不是去连接什么？其实这个函数的功能是创建HttpServer接受Publisher发送来的消息，并且坚定Broker的exit时间，反注册Subscriber。[Run或者Start含义会更清楚一点] func (h *httpBroker) Connect() error { .... go http.Serve(l, h.mux) ///启动HTTPServer go h.run(l) ///帧循环 .... } func (h *httpBroker) run(l net.Listener) { t := time.NewTicker(registerInterval) defer t.Stop() for { select { // heartbeat for each subscriber case &lt;-t.C: h.RLock() for _, subs := range h.subscribers { for _, sub := range subs { h.r.Register(sub.svc, registry.RegisterTTL(registerTTL))//TTL时间验证服务状态，如果服务Died,则重新注册他。 } } h.RUnlock() // received exit signal case ch := &lt;-h.exit: ch &lt;- l.Close() h.RLock() for _, subs := range h.subscribers { for _, sub := range subs { h.r.Deregister(sub.svc) } } h.RUnlock() return } } } DisConnect 那关闭监听 Disconnect做了什么呢？[同样感觉Stop之类的函数名会清楚一点] func (h *httpBroker) Disconnect() error { h.Lock() defer h.Unlock() if !h.running { return nil } // stop rcache rc, ok := h.r.(rcache.Cache) if ok { rc.Stop() } // exit and return err ch := make(chan error) h.exit &lt;- ch err := &lt;-ch // set not running h.running = false return err } 向chan h.exit发送关闭消息，帧循环中会接受到关闭消息，然后进行相应的关闭清理操作。 注意这里发送的关闭消息是err对象，这是一个应该学习的地方，可以知道是正常关闭和异常关闭，如果是异常关闭，可以知道具体错误信息是什么 订阅 func (h *httpBroker) Subscribe(topic string, handler Handler, opts ...SubscribeOption) (Subscriber, error) { ..... // register service /// 当注册一个subscriber的时候实际上注册了一个服务。然后publish通过服务的名称找到这个注册的地址，然后发送消息。 node := &amp;registry.Node{ Id: id, Address: addr, Port: port, Metadata: map[string]string{ &quot;secure&quot;: fmt.Sprintf(&quot;%t&quot;, secure), }, } // check for queue group or broadcast queue version := options.Queue if len(version) == 0 { version = broadcastVersion } service := &amp;registry.Service{ Name: &quot;topic:&quot; + topic, Version: version, Nodes: []*registry.Node{node}, } // generate subscriber subscriber := &amp;httpSubscriber{ opts: options, hb: h, id: id, topic: topic, fn: handler,///等收到publish是的回调。 svc: service, } // subscribe now ////注册服务。并且把subscribe append 到 httpBroker.subscribers中 if err := h.subscribe(subscriber); err != nil { return nil, err } // return the subscriber return subscriber, nil } func (h *httpBroker) subscribe(s *httpSubscriber) error { h.Lock() defer h.Unlock() if err := h.r.Register(s.svc, registry.RegisterTTL(registerTTL)); err != nil { return err } h.subscribers[s.topic] = append(h.subscribers[s.topic], s) return nil } 可以看到订阅服务就是注册一个Topic serivce 到 Consul，如果对应Socke的观点我在这个端口（topic）进行监听了，想发消息的就发给我吧。 发布 func (h *httpBroker) Publish(topic string, msg *Message, opts ...PublishOption) error { h.RLock() s, err := h.r.GetService(&quot;topic:&quot; + topic)///发现相关服务 if err != nil { h.RUnlock() return err } h.RUnlock() m := &amp;Message{ Header: make(map[string]string), Body: msg.Body, } for k, v := range msg.Header { m.Header[k] = v } m.Header[&quot;:topic&quot;] = topic b, err := h.opts.Codec.Marshal(m)///对消息进行编码 if err != nil { return err } pub := func(node *registry.Node, b []byte) { scheme := &quot;http&quot; // check if secure is added in metadata if node.Metadata[&quot;secure&quot;] == &quot;true&quot; { scheme = &quot;https&quot; } vals := url.Values{} vals.Add(&quot;id&quot;, node.Id) uri := fmt.Sprintf(&quot;%s://%s:%d%s?%s&quot;, scheme, node.Address, node.Port, DefaultSubPath, vals.Encode()) r, err := h.c.Post(uri, &quot;application/json&quot;, bytes.NewReader(b)) if err == nil { io.Copy(ioutil.Discard, r.Body) r.Body.Close() } } for _, service := range s { // only process if we have nodes if len(service.Nodes) == 0 { continue } switch service.Version { // broadcast version means broadcast to all nodes case broadcastVersion:///广播 for _, node := range service.Nodes { // publish async go pub(node, b) } default: // select node to publish to///随机publish一个service node := service.Nodes[rand.Int()%len(service.Nodes)] // publish async go pub(node, b) } } return nil } 从上面的代码可以肯出，整个逻辑也非常简单 获取对应topic的server 编码对应的消息 按照service的类型把消息通过http post的方式发送出去【异步发送】。 订阅收到 publisher 发送的消息handle处理 对应上面Create的时候启动的HTTPServer，收到post的消息，读取然后解码，根据对应的topic获取httpBroker.handler[string]Handler中的handle进行调用。这个逻辑也是比较简单。 ","link":"https://nomagic.cc/post/go-micro-yuan-ma-yue-du-broker/"},{"title":"公钥、私钥、数字签名","content":"目的： 保证发送方和发送的内容都没遭到篡改。其实就是说： 1 身份验证 2 数据完整性验证 加密的分类 对称加密 （私钥加密、共享密钥加密） 对称加密算法的特点是加密使用的密钥和解密使用的密钥是相同的。也就是说，加密和解密都是使用的同一个密钥。因此对称加密算法要保证安全性的话，密钥自然要做好保密，只能让使用的人知道，不能对外公开 只有一个加解密的密钥，没有公私之分，为什么叫做对称的？加解密的秘钥是对称的。 数字签名（digital signature） 微软官方给出的定义：“数字签名”是指可以添加到文件的电子安全标记。使用它可以验证文件的发行者以及帮助验证文件自被数字签名后是否发生更改。 对文件进行签名（数字的方式） 必要与联系 可以对外暴露传输的内容，只是为了保证内容不被修改。 数字签名原理 (不要搞混了，签名和加密) 要达到这个目的，一般是对信息做一个hash计算得到一个hash值，注意，这个过程是不可逆的，也就是说无法通过hash值得出原来的信息内容。在把信息发送出去时，把这个hash值加密后做为一个签名和信息一起发出去。 接收方在收到信息后，会重新计算信息的hash值，并和信息所附带的hash值(解密后)进行对比，如果一致，就说明信息的内容没有被修改过，因为这里hash计算可以保证不同的内容一定会得到不同的hash值，所以只要内容一被修改，根据信息内容计算的hash值就会变化。当然，不怀好意的人也可以修改信息内容的同时也修改hash值，从而让它们可以相匹配，为了防止这种情况，hash值一般都会加密后(也就是签名)再和信息一起发送。 数字签名使用方式 下面通过例子来说明这个过程： B给A回信时，采用了数字签名的方式 1、B先用hash函数，生成信件的摘要(digest) 2、B使用自己的私钥，对这个摘要加密，这样就生成了数字签名(signature) 3、B将这个签名附在要回复的信息后面，一起发给A 4、A收到B的信息后，取下数字签名，并通过B的公钥解密，得到信件的摘要信息 5、A在对B发送的信息本身使用B指定的hash函数，将得到的结果同上一步解密得到的摘要进行对比，如果两者一致，就说明B发过来的信息未被修改过。 数字证书（Digital Certificate） 名称挺形象的：验证某个对象是否拥有某种权利，给他颁发证书。 相当于身份证验证颁发和中心。 问题就这样结束了吗？远没有，试想，虽然A确定了B回给他的信息是未修改过的，但是怎么确定给他回信息的就是B？如果有不怀好意的C把A保存的B的公钥偷偷换成自己的，并冒用B的名义给A发信息呢？ 要解决这个问题，A只要能确定自己持有的公钥到底是不是B的就行了，这就需要用到数字证书。 数字证书是用来验证公钥所属的用户身份。在日常生活中，如果我们要验证一个人的身份，通常的做法是查看他的身份证。我们信任身份证颁发机构即政府机构的公信力，因此只要验证一个人的身份证不是伪造的，我们就相信这个人的身份和身份证上所描述的是一致的。 数字证书就是一个人或者组织在网络世界中的身份证，其发证机关是证书管理机构(certificate authority,CA)。CA用自己的私钥对用户的身份信息(主要是用户名和该用户的公钥)进行签名，该签名和用户的身份信息一起就形成了证书。 A 要验证他所拥有的证书是否是自己想要访问的目标的证书。公钥和证书是绑定在一起的，证书正确，那么公钥就是正确的。 数字证书的组成 —————— 用户公钥 —————— 用户身份 —————— 证书机构名称 —————— 有效期 —————— + —————— 证书机构签名 ---&gt; 只要保证证书本身没有被篡改 —————— 非对称加密 单向散列函数 私钥加密 私钥加密，说的很明白不能被公开的秘钥。 又称 对称加密算法，因为这种算法解密密钥和加密密钥是相同的。也正因为同一密钥既用于加密又用于解密，所以这个密钥是不能公开的。常见的有： DES、 IDEA、 RC2、 RC4、 SKIPJACK、 RC5、 AES 公钥加密： 单向函数的数学原理，用公钥加密的文件只能用私钥解密，而私钥加密的文件只能用公钥解密,这样就保证了信息不被篡改，没能验证身份，因为得到公钥的任何一方都可以进行加密，所以，身份的验证通过其他的方式，比如身份信息包含在被加密的消息中传输。 常见非对称加密算法： RSA 如何确保内容不被篡改 HASH(明文) ------&gt; 摘要 A私钥(摘要) ------&gt; 数字签名 明文+签名 ------&gt; 签名明文 生成加密秘钥DES 对签名明文进行加密 --&gt;密文 B公钥（加密秘钥DES）+ 密文--&gt;发送 B私钥（B公钥（加密秘钥））-----&gt;加密秘钥DES DES(密文) ---&gt;（明文） + 签名 A公钥(签名)---&gt; 摘要 HASH(内容)---&gt; 比较---&lt;&gt;摘要、 SSL/TSL 为什么会有两种名称 为了解决 HTTP 协议的以上缺点，在上世纪90年代中期，由网景（NetScape）公司设计了 SSL 协议。SSL 是“Secure Sockets Layer”的缩写，中文叫做“安全套接层”。（顺便插一句，网景公司不光发明了 SSL，还发明了很多 Web 的基础设施——比如“CSS 样式表”和“JS 脚本”）。 到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为 TLS（是“Transport Layer Security”的缩写），中文叫做“传输层安全协议”。 很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。 HTTPS 和 SSL/TSL 的关系是什么？ HTTPS 其实是“HTTP over SSL”或“HTTP over TLS HTTPS 利用 SSL/TSL 进行加密 和服务端的通信是加了密的，刚上来要根据ssl 协议进行握手。 HTTP HTTPS ———————— ———————— HTTP HTTP ———————— ———————— TCP SSL/TSL ———————— ———————— TCP ———————— URI 和 URL 的关系 统一资源标识符： 只是标识了这是一个资源，但资源的访问则是另一种步骤了。 统一资源定位符 URI 属于 URL 更高层次的抽象，一种字符串文本标准。就是说，URI 属于父类，而 URL 属于 URI 的子类。URL 是 URI 的一个子集。二者的区别在于，URI 表示请求服务器的路径，定义这么一个资源。而 URL 同时说明要如何访问这个资源（http://）。 ","link":"https://nomagic.cc/post/gong-yao-si-yao-shu-zi-qian-ming/"},{"title":"Go micro 源码阅读-结构","content":"介绍 micro 是一个用来开发cloud native application的程序工具集，采用的是微服务架构，使用go语言编写。 micro主要包括： toolkits api - api 网关，服务的唯一入口点 bot - Slack and hipchat bot. CLI and ChatOps via messaging. cli 命令行工具用于和整个系统交互 new - 一个微服务代码模板生成工具 5 . web - web的控制面板，通过它可以和相应的service进行交互、查询 framework 即go-micro的部分，一个插件式的基于grpc的分布式开发框架 plugin - micro的插件 几个部分。 本次的代码阅读主要研究micro的toolkits和framework部分 go-micro go-micro ​ ├── broker ├── client // service的对外接口 ├── cmd ├── codec //编解码器 ├── errors //Error接口的实现，提供返回error信息 ├── function.go // https://micro.mu/blog/2017/06/06/functions.html ├── function_test.go ├── go-micro.go //go-micro 对外服务接口（service，和相关度 Pub/Sub接口） ├── go-micro.png ├── LICENSE ├── metadata ├── options.go //service的配置Option实现 ├── publisher.go //依赖注入Client ├── README.md ├── registry //服务注册器 ├── selector //服务选择器（服务发现的负载均衡） ├── server //service 的内部server ├── service.go //封装service的实现 ├── service_test.go ├── transport //用于实现点对点通信 ├── wrapper.go └── wrapper_test.go 从上面可以看出go-micro 是一个编写微服务的框架，一个service包括接收请求的server和对外发送请求的client。通信的方式包括点对点的同步通信（transport）和异步通信（Broker）通过事件的pub/sub的方式提供。 设计上把一个类要聚合的内容通过Options的方式替换出去，每一个聚合的对象都是一个Option，然后每个Option都是可以设置的，只要满足了特定的接口即可。 例如：Service 的Options定义 type Options struct { Broker broker.Broker Cmd cmd.Cmd Client client.Client Server server.Server Registry registry.Registry Transport transport.Transport .... } 具体的每一个项目都是一个Option例如Broker、Cmd、Client、而且每一个Option都是接口形式的定义，例如： type Broker interface { Options() Options Address() string Connect() error Disconnect() error Init(...Option) error Publish(string, *Message, ...PublishOption) error Subscribe(string, Handler, ...SubscribeOption) (Subscriber, error) String() string } 这样就实现了每一个模块都是可以替换的实现。也就对应了go-micro’s pluggable的设计理念。通过面向接口编程的方式，搭建了整个库的骨架。 组件 一个大概的micro图样。 一个Service 包括Server和Client。Server包括Register、Transport、Broker。Client包括Broker、Transport、Register、Selector。 Register：服务的注册于反注册 Transport ： 点对点的同步通信支持 Broker ： 异步通信支持（通过事件的 Publicate 和 Subscribe) Selector : 发现服务 和 选取服务时的负载均衡。 靠这些组件的组合来对外提供Services. ","link":"https://nomagic.cc/post/go-micro-yuan-ma-yue-du-jie-gou/"},{"title":"理解分布式系统的 8 个谬误（译）","content":"您正在开发分布式系统吗？微服务、Web API、SOA、Web 服务器、应用程序服务器、数据库服务器、缓存服务器、负载均衡器 – 如果这些描述了系统设计中的组件，那么答案是肯定的。分布式系统由许多计算机组成，这些计算机通过协调来实现共同的目标。 20 多年前，Peter Deutsch 和 James Gosling 定义了分布式计算的 8 个谬误。这些是许多开发人员对分布式系统做出的错误假设。从长远来看，这些通常被证明是错误的，导致难以修复错误。 这8个谬误是： 网络是可靠 延迟为零 带宽是无限的 网络是安全 拓扑是不变 有1名管理员 运输成本为零 网络是同构的 让我们回顾一下每个谬误，讨论问题和潜在的解决方案。 网络是可靠 如今，大多数系统都会调用其他系统。您是否与第三方系统（支付网关、会计系统、CRM）集成？您正在进行网络服务调用吗？如果呼叫失败会发生什么情况？如果您正在查询数据，只需重试一次即可。但是如果您发送命令会发生什么？让我们举一个简单的例子： var creditCardProcessor = new CreditCardPaymentService(); creditCardProcessor.Charge(chargeRequest); 如果我们收到 HTTP 超时异常会发生什么？如果服务器没有处理该请求，那么我们可以重试。但是，如果它确实处​​理了请求，我们需要确保不会向客户收取双重费用。您可以通过使服务器幂等来做到这一点。这意味着，如果您针对同一收费请求调用 10 次，则客户只需支付一次费用。如果您没有正确处理这些错误，则您的系统是不确定的。处理所有这些情况可能很快就会变得非常复杂。 解决方案 那么，如果网络呼叫失败，我们该怎么办？好吧，我们可以自动重试。排队系统在这方面非常擅长。他们通常使用一种称为存储和转发的模式。他们在将消息转发给收件人之前将其存储在本地。如果收件人离线，排队系统将重试发送消息。MSMQ就是此类排队系统的一个示例。 但这个变化会对你的系统设计产生很大的影响。您正在从请求/响应模型转向即发即忘模型。由于您不再等待响应，因此您需要更改系统中的用户旅程。您不能仅用队列发送来替换每个 Web 服务调用。 结论 您可能会说现在的网络更加可靠——事实确实如此。但事情发生了。硬件和软件可能会出现故障——电源、路由器、更新或补丁失败、无线信号弱、网络拥塞、啮齿动物或鲨鱼。是的，鲨鱼： 在发生一系列鲨鱼咬伤事件后，谷歌正在用 Kevlar 加固海底数据电缆。 还有人方面。人们可以发起 DDOS 攻击，也可以破坏物理设备。 这是否意味着您需要放弃当前的技术堆栈并使用消息传递系统？可能不会！您需要权衡失败的风险和需要进行的投资。您可以通过投资基础设施和软件来最大限度地减少失败的可能性。在许多情况下，失败是一种选择。但在设计分布式系统时确实需要考虑失败。 延迟为零 内存调用和互联网调用之间存在七个数量级的差异。您的应用程序应该具有网络感知能力。这意味着您应该明确区分本地调用和远程调用。让我们看一下我在代码库中看到的一个示例： var viewModel = new ViewModel(); var documents = new DocumentsCollection(); foreach (var document in documents) { var snapshot = document.GetSnapshot(); viewModel.Add(snapshot); } 无需进一步检查，这看起来很好。但是，有两个远程调用。第 2 行进行一次调用以获取文档摘要列表。第 5 行有另一个调用，用于检索有关每个文档的更多信息。这是一个经典的Select n+1 问题。为了考虑网络延迟，您应该在一次调用中返回所有必需的数据。一般的建议是本地调用可以细粒度，但远程调用应该更粗粒度。这就是分布式对象和“网络透明性”的想法消亡的原因。但是，尽管每个人都同意分布式对象是一个坏主意，但有些人仍然认为延迟加载始终是一个好主意： var employee = EmployeeRepository.GetBy(someCriteria) var department = employee.Department; var manager = department.Manager; foreach (var peer in manager.Employees;) { // do something } 您不会期望属性获取器执行网络调用。但是，每个“。” 上面代码中的调用实际上可以触发对数据库的访问。 解决方案 带回您可能需要的所有数据 如果您进行远程调用，请确保带回您可能需要的所有数据。网络交流不应该是闲聊的。 让数据更接近客户 另一种可能的解决方案是将数据移至更靠近客户端的位置。如果您使用云，请根据客户的位置仔细选择可用区域。缓存还可以帮助减少网络调用的数量。对于静态内容，内容交付网络 (CDN) 是另一个不错的选择。 反转数据流 删除远程调用的另一个选择是反转数据流。我们可以使用 Pub/Sub 并将数据存储在本地，而不是查询其他服务。这样，我们就可以在需要时获得数据。当然，这会带来一些复杂性，但它可以成为工具箱中的一个好工具。 结论 尽管延迟在 LAN 中可能不是问题，但当您转移到 WAN 或 Internet 时，您会注意到延迟。这就是为什么将网络调用与内存调用明确分开很重要。采用微服务架构模式时应该牢记这一点。您不应该仅仅用远程调用来代替本地调用。这很可能会将您的系统变成一个分布式的大泥球。 带宽是无限的 带宽是网络在一段时间内发送数据的能力。到目前为止，我还没有发现它是一个问题，但我可以理解为什么它在某些情况下可能是一个问题。尽管带宽随着时间的推移而提高，但我们发送的数据量也增加了。视频流或 VoIP 比通过网络传递简单 DTO 的应用程序需要更多的带宽。带宽对于移动应用程序来说更为重要，因此开发人员在设计后端 API 时需要考虑这一点。 错误地使用 ORM 也会造成伤害。我见过开发人员在查询中过早调用 .ToList() 的示例，从而将整个表加载到内存中。 解决方案 领域驱动设计模式 那么我们如何确保不会带来太多数据呢？领域驱动设计模式可以帮助： 首先，您不应该追求单一的、企业范围的域模型。您应该将域划分为有界上下文。 为了避免有界上下文中出现大型且复杂的对象图，您可以使用聚合模式。聚合确保一致性并定义事务边界。 命令和查询职责分离 我们有时会加载复杂的对象图，因为我们需要在屏幕上显示它的一部分。如果我们在很多地方这样做，我们最终会得到一个庞大而复杂的模型，对于写作和阅读来说都是次优的。另一种方法是使用命令和查询职责分离——CQRS。这意味着将领域模型一分为二： 写入模型将确保不变性成立并且数据一致。由于写入模型不关心视图问题，因此它可以保持较小且集中。 读取模型针对视图问题进行了优化，因此我们可以检索特定视图（例如应用程序中的屏幕）所需的所有数据。 结论 第二个谬论（延迟不为 0）和第三个谬论（带宽无限）之间存在着矛盾。您应该传输更多数据以最大程度地减少网络往返次数。您应该传输较少的数据以最大限度地减少带宽使用。您需要平衡这两种力量并找到通过线路发送的适量数据。 尽管您可能不会经常遇到带宽限制，但考虑传输的数据很重要。数据越少越容易理解。更少的数据意味着更少的耦合。因此，仅传输您可能需要的数据。 网络是安全 这一假设比其他假设得到了更多的媒体报道。您的系统的安全程度取决于您最薄弱的环节。坏消息是分布式系统中有很多链接。您正在使用 HTTPS，除非与不支持它的第三方旧系统进行通信。您正在检查自己的代码，寻找安全问题，但正在使用可能存在风险的开源库。OpenSSL 漏洞允许人们窃取受 SSL/TLS 保护的数据。Apache Struts 中的一个错误允许攻击者在服务器上执行代码。即使您正在防范所有这些，仍然存在人为因素。恶意 DBA 可能会“放错”数据库备份。今天的攻击者手中拥有大量的计算能力和足够的耐心。因此，问题不在于他们是否会攻击您的系统，而在于何时攻击。 解决方案 纵深防御 您应该使用分层方法来保护您的系统。您需要在网络、基础设施和应用程序级别进行不同的安全检查。 安全心态 设计系统时请牢记安全性。 过去 5 年来，十大漏洞列表没有太大变化。您应该遵循安全软件设计的最佳实践，并检查代码是否存在常见的安全缺陷。您应该定期搜索第三方库以查找新漏洞。常见漏洞和暴露列表可以提供帮助。 威胁建模 威胁建模是识别系统中可能的安全威胁的系统方法。您首先要确定系统中的所有资产（数据库中的用户数据、文件等）以及它们的访问方式。之后，您确定可能的攻击并开始执行它们。我建议阅读高级 API 安全性中的第 2 章 ，以更好地概述威胁建模。 结论 唯一安全的系统是断电的、不连接到任何网络的系统（理想情况下浇铸在混凝土块中）。这是一个多么有用的系统呀！事实是，安全既困难又昂贵。分布式系统中有很多组件和链接，其中每一个都可能成为恶意用户的目标。企业需要平衡攻击的风险和概率与实施预防机制的成本。 攻击者拥有大量的耐心和计算能力。我们可以通过使用威胁建模来防止某些类型的攻击，但我们不能保证 100% 的安全。因此，最好向企业明确这一点，共同决定在安全方面投资多少，并制定安全漏洞发生时的计划。 拓扑不变 网络拓扑一直在变化。有时它会因意外原因而发生变化——当您的应用程序服务器出现故障而您需要更换它时。很多时候这是故意的——在新服务器上添加新进程。如今，随着云和容器的兴起，这一点更加明显。弹性扩展（根据工作负载添加或删除服务器的能力）需要一定程度的网络敏捷性。 解决方案 抽象网络的物理结构 您需要做的第一件事是抽象网络的物理结构。您可以通过多种方式来做到这一点： 1 停止对 IP 进行硬编码 – 您应该更喜欢使用主机名。通过使用 URI，我们依靠 DNS 将主机名解析为 IP。 2 当 DNS 不够用时（例如，当您需要映射 IP 和端口时），请使用发现服务。 3 服务总线框架还可以提供位置透明性。 无价值的，而非重要的 通过将您的服务器视为没有价值的，而不是重要的，您可以确保没有服务器是不可替代的。这一点智慧可以帮助您进入正确的心态：任何服务器都可能发生故障（从而改变拓扑），因此您应该尽可能地实现自动化。 测试 最后一条建议是测试你的假设。停止服务或关闭服务器并查看您的系统是否仍在运行。Netflix 的Chaos Monkey等工具通过随机关闭生产环境中的虚拟机或容器，将这一点提升了一个档次。通过解决这个问题，您就会更有动力去构建一个更具弹性的系统来处理拓扑变化。 结论 十年前，大多数拓扑变化并不那么频繁。但当它发生时，它可能发生在生产中并导致一些停机时间。如今，随着云和容器的兴起，这种谬论很难被忽视。您需要为失败做好准备并对其进行测试。不要等待它在生产中发生！ 有一名管理员 嗯，这一点看起来很明显。当然，没有一个人知道一切。这是一个问题吗？只要应用程序运行顺利，就不是这样。但是，当出现问题时，您需要修复它。因为有太多人接触过该应用程序，所以知道如何解决问题的人可能并不在那里。 有很多事情可能会出错。一个例子是配置。当今的应用程序将配置存储在多个存储中：配置文件、环境变量、数据库、命令行参数。没有人知道每个可能的配置值的效果是什么。 另一件可能出错的事情是系统升级。分布式应用程序有许多移动部件，您需要确保它们同步。例如，您需要确保当前版本的代码适用于当前版本的数据库。如今，人们关注的是 DevOps 和持续交付。但支持零停机部署并非易事。 但是，至少这些事情都在你的掌控之中。许多应用程序与第三方系统交互。这意味着，如果它们下降，您无能为力。因此，即使您的系统有一名管理员，您仍然无法控制第 3 方系统。 解决方案 每个人都应该对发布过程负责 这意味着从一开始就让运维人员或系统管理员参与其中。理想情况下，他们应该成为团队的一部分。尽早让系统管理员了解您的进度可以帮助您发现限制。例如，生产环境可能具有与开发环境不同的配置、安全限制、防火墙规则或可用端口。 记录和监控 系统管理员应该拥有正确的工具来报告错误和管理问题。您应该从一开始就考虑监控。分布式系统应该有集中式日志记录。访问十个不同服务器上的日志来调查问题并不是一种可接受的方法。 解耦 您应该努力在系统升级期间尽量减少停机时间。这意味着您应该能够独立升级系统的不同部分。通过使组件向后兼容，您可以在不同时间更新服务器和客户端。 通过在组件之间放置队列，您可以暂时解耦它们。这意味着，例如，即使后端关闭，Web 服务器仍然可以接受请求。 隔离第 3 方依赖项 您应该以不同于您拥有的组件的方式对待您无法控制的系统。这意味着让您的系统对第三方故障更具弹性。您可以通过引入抽象层来减少外部依赖项的影响。这意味着当第 3 方系统出现故障时，您查找错误的地方就会减少。 结论 要解决这个谬论，您需要使您的系统易于管理。DevOps、日志记录和监控可以提供帮助。您还需要考虑系统的升级过程。如果升级需要数小时的停机时间，则您无法部署每个冲刺。没有一个管理员，因此每个人都应该对发布过程负责。 运输成本为零 这个谬论与第二个谬论有关——延迟为零。通过网络传输东西是有代价的，包括时间和资源。如果第二个谬论讨论的是时间方面，那么谬论#7 则解决资源消耗问题。 这个谬论有两个不同的方面： 网络基础设施的成本 网络基础设施是有成本的。服务器、SAN、网络交换机、负载平衡器以及维护这些设备的人员——所有这些都需要花钱。如果您的系统是在本地部署的，那么您需要预先支付此价格。如果您使用云，那么您只需为使用的内容付费 – 但您仍然需要为其付费。 序列化/反序列化的成本 这个谬论的第二个方面是在传输层和应用层之间传输数据的成本。序列化和反序列化会消耗CPU时间，因此需要花钱。如果您的应用程序部署在本地，并且您不主动监控资源消耗，则此成本在某种程度上是隐藏的。但如果您的应用程序部署在云中，这种成本就非常明显，因为您要为使用的内容付费。 解决方案 对于基础设施的成本，您无能为力。您只能确保尽可能有效地使用它。SOAP 或 XML 比 JSON 更昂贵。JSON 比 Google 的 Protocol Buffers 等二进制协议更昂贵。根据系统的类型，这可能或多或少重要。例如，对于与视频流或 VoIP 相关的应用程序来说，传输成本更为重要。 结论 您应该注意传输成本以及应用程序正在执行的序列化和反序列化量。这并不意味着您应该进行优化，除非有必要。您应该对资源消耗进行基准测试和监控，并确定运输成本是否对您来说是一个问题。 网络是同构的 同质网络是使用相似配置和相同通信协议的计算机网络。拥有具有相似配置的计算机是一项很难完成的任务。例如，您几乎无法控制哪些移动设备可以连接到您的应用程序。这就是为什么关注标准协议很重要。 解决方案 您应该选择标准格式以避免供应商锁定。这可能意味着 XML、JSON 或 Protocol Buffers。有很多选项可供选择。 结论 您需要确保系统的组件可以相互通信。使用专有协议会损害应用程序的互操作性。 设计分布式系统很困难 这些谬论早在20多年前就已发表。但它们在今天仍然适用，其中一些比其他更重要。我认为当今许多开发人员都知道它们，但我们编写的代码并没有显示这一点。 我们必须接受这些事实：网络不可靠、不安全并且需要花钱。带宽有限。网络的拓扑将会改变。其组件的配置方式不同。意识到这些限制将有助于我们设计更好的分布式系统。 [原文地址]（https://www.simpleorientedarchitecture.com/8-fallacies-of-distributed-systems/） ","link":"https://nomagic.cc/post/li-jie-fen-bu-shi-xi-tong-de-8-ge-miu-wu/"},{"title":"go micro 源码阅读-Options [Functional Options Pattern]","content":"解决的问题 如果想提供一个具有可选参数的方法，那么在其他语言中很简单使用默认值就好了。然而在Go中不支持默认值的做法，退而求其次那么使用函数重载好了，接着就会发现Go中同样不支持函数重载，那么我使用不同的函数名好了。。。。（代码估计很丑，并且不容易辨析） ** Go 不支持默认值参数和函数重载的原因 ** ​ Method dispatch is simplified if it doesn't need to do type matching as well. Experience with other languages told us that having a variety of methods with the same name but different signatures was occasionally useful but that it could also be confusing and fragile in practice. Matching only by name and requiring consistency in the types was a major simplifying decision in Go's type system. Regarding operator overloading, it seems more a convenience than an absolute requirement. Again, things are simpler without it 如果不需要做类型匹配的话那么函数分发的处理就会简单。并且具有同样函数名的函数却有不同的类型（增加了代码的理解难度，比如你在读一份代码的时候，看到一个函数调用，然而这个函数调用却有不同的重载或者不同数量的调用参数的话，那么头脑里就要维护这个函数的多个版本，始终不如只存在一个版本理解上来的容易），并且保持函数名和函数类型的一致，可以是Go的类型系统的处理更加简单。 #解决方案 以上对Go不支持可选参数和函数重载的原因进行了说明，可是实际的编程中确实有这种模型的需要，如何解决这种问题呢。及如何使用不变的接口，实行可变的构造行为（可变----》 不变的转换） 构造者模式 本身这个一个对象的够着的问题，可以从构造模式的方向进行考虑，显然通过Builder（构造者模式）可以解决这个问题。 可变参数 func Fun(arg ...int)的方式，可是这种方式可变参数的类型只能是统一中类型，那么如果用interface，当然可以，只是函数体内充满了类型断言，对函数本身的逻辑造成了很大的干扰。（职能单一原则） Functional Options Pattern 与Builder模式类似，变---》不变的转换，找到变动的元素，然后固定他。把变的东西通过特定的设计归一它，把变动的参数类型，归约为不变的函数类型，类似于变换到同类型的可变参数（函数类型）。（及变动的逻辑，只以一种方式存在） 比如说我想制作一杯咖啡，（糖，牛奶，咖啡粉，盐，酒，冰淇淋，蜂蜜。。。。。）这些原料应该是可以选择添加的。那么：一杯 \\二/，就可以随意添加自己喜欢的原料了。 type func CoffeeOption (Opts *CoffeeOptions) type CoffeeOptions struct{ sugar int milk int cofferPowder int ... } type Coffee struct{ opts *CoffeeOptions } func CoffeeSugar（sugar int)CoffeeOption{ return func(opts *ConffeeOptions){ opts.sugar = sugar } } func CoffeeMilk（milk int)CoffeeOption{ return func(opts *ConffeeOptions){ opts.milk = milk } } func newDefaultCoffeeOptions()*CoffeeOptions{ return &amp;CoffeeOptions{ sugar:2, milk:5, coffeePowder:100, ..... } } func NewCoffee(opts ...CoffeeOption)*Coffer{ defaultOptions = newDefaultCoffeeOptions() for _,opt := range opts{ opt(defaultOptions) } return &amp;Coffee{ opts:defaultOptions, } } 应用 在go-micro中你可以看到很多functional options pattern的应用，基本每个micro组件都使用了这种方式进行构造。 在grpc中同样很多地方也使用了这种方式。 ","link":"https://nomagic.cc/post/go-micro-yuan-ma-yue-du-options-functional-options-pattern/"},{"title":"python 魔法函数","content":"魔法函数 1. 前言 1.1 什么是魔法函数？ 所谓魔法函数（Magic Methods），是Python的一种高级语法，允许你在类中自定义函数（函数名格式一般为__xx__），并绑定到类的特殊方法中。比如在类A中自定义__str__()函数，则在调用str(A())时，会自动调用__str__()函数，并返回相应的结果。在我们平时的使用中，可能经常使用**__init__函数（构造函数）和__del__函数（析构函数）**，其实这也是魔法函数的一种。 Python中以双下划线(__xx__)开始和结束的函数（不可自己定义）为魔法函数。 调用类实例化的对象的方法时自动调用魔法函数。 在自己定义的类中，可以实现之前的内置函数。 1.2 魔法函数有什么作用？ 魔法函数可以为你写的类增加一些额外功能，方便使用者理解。举个简单的例子，我们定义一个“人”的类People，当中有属性姓名name、年龄age。让你需要利用sorted函数对一个People的数组进行排序，排序规则是按照name和age同时排序，即name不同时比较name，相同时比较age。由于People类本身不具有比较功能，所以需要自定义，你可以这么定义People类： class People(object): def __init__(self, name, age): self.name = name self.age = age return def __str__(self): return self.name + &quot;:&quot; + str(self.age) def __lt__(self, other): return self.name &lt; other.name if self.name != other.name else self.age &lt; other.age if __name__==&quot;__main__&quot;: print(&quot;\\t&quot;.join([str(item) for item in sorted([People(&quot;abc&quot;, 18), People(&quot;abe&quot;, 19), People(&quot;abe&quot;, 12), People(&quot;abc&quot;, 17)])])) 输出结果： abc:17 abc:18 abe:12 abe:19 上个例子中的__lt__函数即less than函数，即当比较两个People实例时自动调用。 2. 常见的魔法函数 我们将魔法方法分为：非数学运算和数学运算两大类。 2.1 非数学运算 2.1.1 字符串表示 __repr__函数和__str__函数： Python 中，每个对象都有两个特殊方法，用于将对象转换为字符串：repr__和__str。 repr 方法返回一个对象的“官方”字符串表示形式，它的输出通常可以直接用作代码片段来创建该对象的副本。它的主要目的是提供一种明确且详细的表示形式，以便开发者可以了解对象的内部结构和状态。 str 方法返回一个对象的“用户友好”字符串表示形式，适合显示给终端用户。 区别 方法 功能 输出 repr 对象的“官方”字符串表示形式 适合开发者 str 对象的“用户友好”字符串表示形式 适合终端用户 示例 Python class Point: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return f&quot;Point({self.x}, {self.y})&quot; def __str__(self): return f&quot;({self.x}, {self.y})&quot; p1 = Point(1, 2) print(repr(p1)) # Point(1, 2) print(str(p1)) # (1, 2) 输出： Point(1, 2) (1, 2) 注意 如果对象没有定义__repr__方法，Python 会使用__str__方法的返回值作为__repr__方法的返回值。 如果对象定义了__repr__方法，但没有定义__str__方法，Python 会使用__repr__方法的返回值作为__str__方法的返回值。 使用建议 建议在类中定义__repr__方法，以便开发者可以使用它来创建对象的副本。 建议在类中定义__str__方法，以便终端用户可以使用它来显示对象的信息。 2.1.2 集合、序列相关 __len__函数、__getitem__函数、__setitem__函数、__delitem__函数和__contains__函数： 在 Python 中，len、getitem、setitem、delitem 和 contains 是五个特殊方法，用于实现序列的行为。 序列 序列是 Python 中的一种数据类型，可以存储多个元素。序列的常见类型包括列表、元组、字符串和 range。 len 方法 len 方法用于返回序列的长度。 Python class MySequence: def __len__(self): return 10 seq = MySequence() print(len(seq)) # 10 在上述示例中，MySequence 类定义了 __len__ 方法。该方法返回 10，因此 seq 对象的长度为 10。 getitem 方法 getitem 方法用于获取序列中指定位置的元素。 Python class MySequence: def __init__(self, data): self.data = data def __getitem__(self, index): return self.data[index] seq = MySequence([1, 2, 3]) print(seq[0]) # 1 print(seq[1]) # 2 print(seq[2]) # 3 在上述示例中，MySequence 类定义了 __getitem__ 方法。该方法根据索引 index 返回序列中对应的元素。 setitem 方法 setitem 方法用于设置序列中指定位置的元素。 Python class MySequence: def __init__(self, data): self.data = data def __setitem__(self, index, value): self.data[index] = value seq = MySequence([1, 2, 3]) seq[0] = 100 print(seq) # [100, 2, 3] 在上述示例中，MySequence 类定义了 __setitem__ 方法。该方法根据索引 index 和值 value 设置序列中对应的元素。 delitem 方法 delitem 方法用于删除序列中指定位置的元素。 Python class MySequence: def __init__(self, data): self.data = data def __delitem__(self, index): del self.data[index] seq = MySequence([1, 2, 3]) del seq[0] print(seq) # [2, 3] 在上述示例中，MySequence 类定义了 __delitem__ 方法。该方法根据索引 index 删除序列中对应的元素。 contains 方法 contains 方法用于判断序列中是否包含指定元素。 class MySequence: def __init__(self, data): self.data = data def __contains__(self, value): return value in self.data seq = MySequence([1, 2, 3]) print(1 in seq) # True print(4 in seq) # False 在上述示例中，MySequence 类定义了 __contains__ 方法。该方法根据值 value 判断序列中是否包含指定元素。 使用建议 建议在实现自定义序列时，使用上述五个特殊方法。 建议在使用上述特殊方法时，尽量使用 super().__len__()、super().__getitem__()、super().__setitem__()、super().__delitem__() 和 super().__contains__() 方法来调用父类的序列方法。 2.1.3 迭代相关 __iter__函数和__next__函数： Python 中，迭代器可以通过两种方式实现： 使用类 使用生成器 使用类实现迭代器 要使用类实现迭代器，需要在类中实现两个方法：iter 和 next。 class MyIterator: def __init__(self, start, end): self.start = start self.end = end def __iter__(self): return self def __next__(self): if self.start &gt; self.end: raise StopIteration else: value = self.start self.start += 1 return value iterator = MyIterator(1, 10) for item in iterator: print(item) 输出： 1 2 3 4 5 6 7 8 9 10 使用生成器实现迭代器 要使用生成器实现迭代器，可以使用 yield 关键字。 def my_iterator(start, end): for i in range(start, end): yield i iterator = my_iterator(1, 10) for item in iterator: print(item) 输出： 1 2 3 4 5 6 7 8 9 10 迭代器的使用场景 迭代器可以用于以下场景： 遍历可迭代对象 实现惰性计算 实现生成器 遍历可迭代对象 迭代器可以用于遍历可迭代对象，例如列表、元组、字典和集合。 list1 = [1, 2, 3] iterator = iter(list1) for item in iterator: print(item) 输出： 1 2 3 实现惰性计算 迭代器可以用于实现惰性计算，即只有在需要时才会计算下一个元素。 def fibonacci(n): a, b = 0, 1 for i in range(n): yield a a, b = b, a + b for i in fibonacci(10): print(i) 输出： 0 1 1 2 3 5 8 13 21 34 实现生成器 迭代器可以用于实现生成器，生成器是一种特殊的迭代器，可以使用 yield 关键字生成值。 def fibonacci(n): a, b = 0, 1 for i in range(n): yield a a, b = b, a + b for i in fibonacci(10): print(i) 输出： 0 1 1 2 3 5 8 13 21 34 2.1.4 可调用 __call__函数： 2.1.5 with上下文管理器 __enter__函数和__exit__函数： 在 Python 中，enter 和 exit 是两个特殊方法，用于实现上下文管理器。上下文管理器是一种对象，可以用于管理资源的访问。 enter 方法 enter 方法在 with 语句中执行时被调用一次。该方法的返回值会被赋值给 with 语句的目标，即 as 语句后面的名称。 exit 方法 exit 方法在 with 语句执行完毕时被调用。该方法的参数为三个： exc_type：异常类型，如果没有异常，则为 None。 exc_value：异常值，如果没有异常，则为 None。 exc_traceback：异常堆栈跟踪，如果没有异常，则为 None。 exit 方法可以返回 True 或 False。如果返回 True，则表示 with 语句正常执行完毕。如果返回 False，则表示 with 语句在执行过程中发生了异常。 示例 class MyContextManager: def __init__(self): self.resource = open(&quot;my_file.txt&quot;, &quot;w&quot;) def __enter__(self): return self.resource def __exit__(self, exc_type, exc_value, exc_traceback): self.resource.close() with MyContextManager() as f: f.write(&quot;Hello, world!&quot;) 在上述示例中，MyContextManager 类实现了上下文管理器。__enter__ 方法返回 f 对象，该对象是 open() 函数的返回值，表示文件对象。__exit__ 方法在 with 语句执行完毕时调用，关闭文件对象。 使用建议 建议在实现上下文管理器时，在 __enter__ 方法中初始化资源，在 __exit__ 方法中释放资源。 建议在 __exit__ 方法中捕获异常，并根据异常类型进行处理。 2.1.6 数值转换 __abs__函数、__bool__函数、__int__函数、__float__函数、__hash__函数和__index__函数： 2.1.7 元类相关 __new__函数和__init__函数： 2.1.8 属性相关 __getattr__函数、__setattr__函数、__getattribute__函数、__setattribute__函数和__dir__函数： __getattr_/__setattr__ 不存在时调用。 在 Python 中，getattr、setattr、getattribute、setattribute 和 dir 是五个特殊方法，用于自定义对象的属性访问行为。 getattr 方法 getattr 方法用于在对象没有指定属性时返回属性值。 class MyClass: def __init__(self): self.name = &quot;Bard&quot; def __getattr__(self, name): if name == &quot;age&quot;: return 100 else: raise AttributeError(f&quot;No attribute named {name}.&quot;) obj = MyClass() print(obj.name) # Bard print(obj.age) # 100 print(obj.not_exists) # AttributeError: No attribute named not_exists. setattr 方法 setattr 方法用于在设置对象属性时执行自定义操作 class MyClass: def __init__(self): self.name = &quot;Bard&quot; def __setattr__(self, name, value): if name == &quot;age&quot;: print(f&quot;Setting age to {value}.&quot;) else: super().__setattr__(name, value) obj = MyClass() obj.name = &quot;Bard&quot; obj.age = 100 输出： Setting age to 100. getattribute 方法 getattribute 方法用于获取对象属性的值。 class MyClass: def __init__(self): self.name = &quot;Bard&quot; def __getattribute__(self, name): print(f&quot;Getting attribute {name}.&quot;) return super().__getattribute__(name) obj = MyClass() print(obj.name) # Getting attribute name. print(obj.age) # Getting attribute age. 输出： Getting attribute name. Bard Getting attribute age. setattribute 方法 setattribute 方法用于设置对象属性的值。 Python class MyClass: def __init__(self): self.name = &quot;Bard&quot; def __setattribute__(self, name, value): print(f&quot;Setting attribute {name} to {value}.&quot;) super().__setattribute__(name, value) obj = MyClass() obj.name = &quot;Bard&quot; obj.age = 100 输出： Setting attribute name to Bard. Setting attribute age to 100. dir 方法 dir 方法用于返回对象的属性列表。 class MyClass: def __init__(self): self.name = &quot;Bard&quot; obj = MyClass() print(dir(obj)) # ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'name'] 使用建议 建议在实现自定义属性访问行为时，使用 __getattribute__ 和 __setattr__ 方法。 建议在使用 __getattr__ 方法时，尽量使用 super().__getattr__() 方法来调用父类的 __getattr__() 方法。 建议在使用 __setattribute__ 方法时，尽量使用 super().__setattribute__() 方法来调用父类的 __setattribute__() 方法。 2.1.9 属性描述符 __get__函数、__set__函数和__delete_函数： 在 Python 中，get、set 和 delete 是三个特殊方法，用于实现描述符的行为。 描述符 描述符是一种特殊的类，可以用于控制属性的访问。描述符可以用于以下场景： 验证属性值 在访问属性时执行自定义操作 在删除属性时执行自定义操作 get 方法 get 方法用于获取属性值。 class MyDescriptor: def __get__(self, instance, owner): print(f&quot;Getting attribute {self.name}.&quot;) return getattr(instance, self.name) class MyClass: name = MyDescriptor() obj = MyClass() print(obj.name) # Getting attribute name. 在上述示例中，MyDescriptor 类定义了 __get__ 方法。当访问 obj.name 时，会调用 __get__ 方法。在 __get__ 方法中，会打印一条日志，然后调用父类的 __getattribute__ 方法来获取属性值。 set 方法 set 方法用于设置属性值。 class MyDescriptor: def __set__(self, instance, value): print(f&quot;Setting attribute {self.name} to {value}.&quot;) setattr(instance, self.name, value) class MyClass: name = MyDescriptor() obj = MyClass() obj.name = &quot;Bard&quot; print(obj.name) # Getting attribute name. 在上述示例中，MyDescriptor 类定义了 __set__ 方法。当设置 obj.name 为 &quot;Bard&quot; 时，会调用 __set__ 方法。在 __set__ 方法中，会打印一条日志，然后调用父类的 __setattr__ 方法来设置属性值。 delete 方法 delete 方法用于删除属性。 class MyDescriptor: def __delete__(self, instance): print(f&quot;Deleting attribute {self.name}.&quot;) delattr(instance, self.name) class MyClass: name = MyDescriptor() obj = MyClass() del obj.name print(obj.name) # AttributeError: 'MyClass' object has no attribute 'name' 在上述示例中，MyDescriptor 类定义了 __delete__ 方法。当删除 obj.name 时，会调用 __delete__ 方法。在 __delete__ 方法中，会打印一条日志，然后调用父类的 __delattr__ 方法来删除属性。 使用建议 建议在实现自定义属性访问行为时，使用描述符。 建议在使用描述符时，尽量使用 super().__get__()、super().__set__() 和 super().__delete__() 方法来调用父类的描述符方法。 2.1.10 协程 __await__函数、__aiter__函数、__anext__函数、__aenter__函数和__aexit__函数 2.2 数学运算 2.2.1 一元运算符 __neg__ (-)、__pos__ (+)和__abs__函数。 2.2.2 二元运算符 __lt__ (&lt;)、__le__ (&lt;=)、__eq__ (==)、__ne__ (!=)、__gt__ (&gt;)和__ge__ (&gt;=)。 2.2.3 算术运算符 __add__ (+)、__sub__ (-)、__mul__ (*)、__truediv__ (/)、__floordiv__ (//)、__mod__ (%)、__divmod__ 或divmod()、__pow__ 或pow() (**)和__round__ 或round()。 2.2.4 反向算术运算符 __radd__、__rsub__、__rmul__、__rtruediv__、__rfloordiv__、__rmod__、__rdivmod__和__rpow__。 2.2.5 增量赋值算术运算符 __iadd__、__isub__、__imul__、__ifloordiv__和__ipow__。 2.2.6 位运算符 __invert__ (~)、__lshift__ (&lt;&lt;)、__rshift__ (&gt;&gt;)、__and__ (&amp;)、__or__ (|)和__xor__ (^)。 2.2.7 反向位运算符 __rlshift__、__rrshift__、__iand__、__ixor__和__ior__。 2.2.8 增量赋值运算符 __ilshift__、__irshift__、__iand__、__ixor__和__ior__。 2.3 其他魔法函数 __ unicode__()函数，__ delattr__()函数， __ del__()函数， __dict__()函数，__all__()函数： 参考 ^ ","link":"https://nomagic.cc/post/python-mo-fa-han-shu/"},{"title":"有趣的unowned(unsafe)","content":"逛git时，看到一段代码：源地址：https://gist.github.com/hoshi-takanori/07b2982c9942ebe186c530fa611921bb class Person { let name: String init(name: String) { self.name = name } func printName() { print(\"name = \\(name)\") } deinit { print(\"\\(name) is being deinited\") } } var p: Person? = Person(name: \"Hoshi\") p?.printName() // name = Hoshi unowned(unsafe) let q = p! q.printName() // name = Hoshi p = nil //q.printName() // incorrect checksum for freed object - object was probably modified after being freed. let pp = Person(name: \"Takanori\") q.printName() // name = Takanori 当q.printName（）打印:name = Takanori ,我想多少人会WTF。“计算机是不会干错事的，只是我们没有理解计算机的诚实。” 但是到底为什么会有这样的结果？ Swift内存管理有这么几个部分： strong weak unowned 其中unowned分为unowned（safe）unowned(unsafe),默认情况下unowned是unowned（safe） 当unowned（safe）引用的对象被dealloc之后，如果再次访问这个对象，将会抛出一个异常来终止程序(访问之前会进行检查，如果所引用的对象已经被release，就抛出个异常，这也行就是safe的意思吧) 但unowned（unsafe）不同，当unowned（unsafe）引用的对象被dealloc之后，如果再次访问这个内存，会出现三种情况：（不会检查，直接访问，这也许就是unsafe的意思吧） 1 内存没有被改变点--&gt;输出原来的值 2 内存被改变掉--&gt;crash 3 内存被同类型对象覆盖--&gt;不会crash，到使用的内存模型确实新对象的。 更深入的了解：http://stackoverflow.com/questions/26553924/what-is-the-difference-in-swift-between-unownedsafe-and-unownedunsafe ","link":"https://nomagic.cc/post/you-qu-de-unownedunsafe/"},{"title":"Swift:Deep in immutable","content":"这篇文章不探讨let.或者const的含义与用法，而是要探讨一个概念 value types is immutable #理念上的不可变 值类型是无法改变的。这个如何理解,比如下面的例子 struct Person{ var name:String var age:Int mutating func setAge(age:Int){ self.age = age } } var person_Lilei = Person(name:\"lilei\",age:18) print(person_Lilei.name) //print out lilei person_Lilei.name = \"hanmeimei\" print(person_Lilei.name)//print out hanmeimei what???不是值类型不可改变么？是的值类型不可改变，但是为什么出现这种输出呢。 来看一个例子 var a = 4; a = 10 那么是a改变了，还是4改变了？ 现在再回头来看看person，是person改变了，还是person所代表的对象改变了。 a/person 均为变量，看看维基百科关于变量的定义 在程序设计中，变数（英语：Variable，scalar）是指一个包含部分已知或未知数值或资讯（即一个值）之储存位址，以及相对应之符号名称（识别字） 可见a没有改变，只是a地址上放的东西变了，以前放的4，现在放的是10了，但是4的概念没有变成10，“4是4，10是10，40是40，14是14....,value types is immutable&quot;。 再来看person,是否可以理解为person里面放的对象变掉了？由person[lilei]的对象，被换成了person[hanmeimei]对象。 name赋值操作之下到底怎么实现了，我不知道。我只是给予一定的猜测： 观察可以知道，每一个struct对象都有一个self属性【这个和C++,OC,Java等有所区别】 setAge method中的self，是person的属性，而不是C++ setAge(Perons* this,int age) 不是OC 的 id objc_msgSend ( id self, SEL op, ... ); 可以猜测为 func setName(name:String){ var copyself =self.copy() copyself.name = name self = copyself } 是的，person里所放的对象变掉了。【注意：这只是我个人的猜测，目前为止我还没有找到任何的依据，但这明显符合value types is immutable的概念】 这就看来，person 由person[lilei]变成person[hanmeimei]和a 由 4 变成10 的性质是一样的。 我想了很久，为什么Struct的实例方法默认情况下不可以改变实例的属性，必须要加上关键字mutating才可以改变，而对应的属性声明是var。从如果不加mutaing,引发的错误来看。 不可变的是self，也就是Person 实例本身，更进一步的佐证了我的猜测，可以看出本身Struct的设计上符合了value types is immutable，毕竟Struct 是 value types。 #比较直观的不可改变， 当 value types 类型的值在被：赋值，初始化，参数传递的时候，不可改变。 例如： var person_Lili = person_Lilei perons_Lili.name = \"lili\" print(person_Lilei.name) //print out lilei 这种情况下不管person_Lili，做任何改变，对于person_Lilei来说都是不可改变的。这也说明了，为什么在多线程环境下，immutable是比较安全的。 ","link":"https://nomagic.cc/post/swiftdeep-in-immutable/"}]}